{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce544786-7477-49fb-a436-3a1e472d29ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/14/24 15:26:35] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> To use the Graphein submodule                                         <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">embeddings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py#45\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         graphein.protein.features.sequence.embeddings, you need to install:   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: pip install biovec               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Alternatively, you can install graphein with the extras:              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pip install graphein<span style=\"font-weight: bold\">[</span>extras<span style=\"font-weight: bold\">]</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/14/24 15:26:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m To use the Graphein submodule                                         \u001b]8;id=261998;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py\u001b\\\u001b[2membeddings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=955044;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py#45\u001b\\\u001b[2m45\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         graphein.protein.features.sequence.embeddings, you need to install:   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec                                                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: pip install biovec               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Alternatively, you can install graphein with the extras:              \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pip install graphein\u001b[1m[\u001b[0mextras\u001b[1m]\u001b[0m                                          \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/14/24 15:26:36] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> To use the Graphein submodule graphein.protein.visualisation, you  <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/visualisation.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">visualisation.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/visualisation.py#36\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         need to install: pytorch3d                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pytorch3d cannot be installed via pip                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/14/24 15:26:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m To use the Graphein submodule graphein.protein.visualisation, you  \u001b]8;id=791235;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/visualisation.py\u001b\\\u001b[2mvisualisation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=258349;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/visualisation.py#36\u001b\\\u001b[2m36\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         need to install: pytorch3d                                         \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pytorch3d cannot be installed via pip                              \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> To use the Graphein submodule graphein.protein.meshes, you need to        <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/meshes.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">meshes.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/meshes.py#30\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         install: pytorch3d                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: pip install pytorch3d                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m To use the Graphein submodule graphein.protein.meshes, you need to        \u001b]8;id=143375;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/meshes.py\u001b\\\u001b[2mmeshes.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=393950;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/meshes.py#30\u001b\\\u001b[2m30\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         install: pytorch3d                                                        \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: pip install pytorch3d                \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from graphein.protein.config import ProteinGraphConfig\n",
    "from graphein.protein.graphs import construct_graph\n",
    "from graphein.protein.edges.distance import (add_peptide_bonds,\n",
    "                                             add_hydrogen_bond_interactions,\n",
    "                                             add_disulfide_interactions,\n",
    "                                             add_ionic_interactions,\n",
    "                                             add_aromatic_interactions,\n",
    "                                             add_aromatic_sulphur_interactions,\n",
    "                                             add_cation_pi_interactions\n",
    "                                            )\n",
    "from graphein.protein.features.nodes import (\n",
    "    amino_acid_one_hot,\n",
    "    expasy_protein_scale,\n",
    "    hydrogen_bond_acceptor,\n",
    "    hydrogen_bond_donor,\n",
    "    meiler_embedding\n",
    ")\n",
    "from graphein.protein.features.nodes.geometry import add_sidechain_vector, add_beta_carbon_vector, add_sequence_neighbour_vector\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "import h5py\n",
    "import g2papi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336d7703-50ba-49a2-ab6c-1fba8143da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038466b3-2344-4e45-a66c-a7597f8992e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_db_path = \"/work/gr-fe/databases/alpha_fold/human/\"\n",
    "prefix = \"AF-\"\n",
    "suffix = \"-F1-model_v4.pdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3efca5-479a-4d44-b26b-a340bef70b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define edge and annotation functions\n",
    "\n",
    "edge_funcs = {\"edge_construction_functions\": [add_peptide_bonds,\n",
    "                                              add_aromatic_interactions,\n",
    "                                              add_hydrogen_bond_interactions,\n",
    "                                              add_disulfide_interactions,\n",
    "                                              add_ionic_interactions,\n",
    "                                              add_aromatic_sulphur_interactions,\n",
    "                                              add_cation_pi_interactions]}\n",
    "\n",
    "\n",
    "all_node_metadata = {\"node_metadata_functions\" : [amino_acid_one_hot,\n",
    "                                                 expasy_protein_scale,\n",
    "                                                 hydrogen_bond_acceptor,\n",
    "                                                 hydrogen_bond_donor,\n",
    "                                                 meiler_embedding]}\n",
    "\n",
    "#all_graph_metadata = {\"graph_metadata_functions\": [esm_residue_embedding]}  \n",
    "\n",
    "#config = ProteinGraphConfig(**{**edge_funcs, **all_node_metadata, **all_graph_metadata}) \n",
    "config = ProteinGraphConfig(**{**edge_funcs, **all_node_metadata}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48cf6d15-d039-42d4-9039-d683e1d98ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HGNC</th>\n",
       "      <th>DN</th>\n",
       "      <th>LOF</th>\n",
       "      <th>GOF</th>\n",
       "      <th>UniprotEntry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARS1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P49588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABCA1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O95477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACD</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Q96AP0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACTA1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P68133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACTB</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>P60709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>ZFPM2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Q8WW38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>ZIC2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>O95409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>ZMYM2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Q9UBW7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>ZMYND11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Q15326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>ZNF462</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Q96JM2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1276 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HGNC  DN  LOF  GOF UniprotEntry\n",
       "0       AARS1   1    0    0       P49588\n",
       "1       ABCA1   1    0    0       O95477\n",
       "2         ACD   1    0    0       Q96AP0\n",
       "3       ACTA1   1    0    0       P68133\n",
       "4        ACTB   1    1    1       P60709\n",
       "...       ...  ..  ...  ...          ...\n",
       "1271    ZFPM2   0    1    0       Q8WW38\n",
       "1272     ZIC2   0    1    0       O95409\n",
       "1273    ZMYM2   0    1    0       Q9UBW7\n",
       "1274  ZMYND11   0    1    0       Q15326\n",
       "1275   ZNF462   0    1    0       Q96JM2\n",
       "\n",
       "[1276 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read ground_truth dataframe\n",
    "\n",
    "df = pd.read_csv(\"../data/DN_LOF_GOF_truth.tsv\", sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4a8379-d3e4-4591-b3ec-de48313d03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels and uniprot ids\n",
    "\n",
    "all_uniprot_ids = [] \n",
    "all_hgnc = []\n",
    "all_labels = [] \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    all_uniprot_ids.append(row[\"UniprotEntry\"])\n",
    "    all_hgnc.append(row[\"HGNC\"])\n",
    "    all_labels.append([row[\"DN\"], row[\"LOF\"], row[\"GOF\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313c7fdd-8240-4375-81d6-b6c85b21b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniprotEntry</th>\n",
       "      <th>ASAquick_normscore</th>\n",
       "      <th>DFLpredScore</th>\n",
       "      <th>DRNApredDNAscore</th>\n",
       "      <th>DRNApredRNAscore</th>\n",
       "      <th>DisoDNAscore</th>\n",
       "      <th>DisoPROscore</th>\n",
       "      <th>DisoRNAscore</th>\n",
       "      <th>MMseq2_conservation_score</th>\n",
       "      <th>MoRFchibiScore</th>\n",
       "      <th>PSIPRED_helix</th>\n",
       "      <th>PSIPRED_strand</th>\n",
       "      <th>SCRIBERscore</th>\n",
       "      <th>SignalP_score</th>\n",
       "      <th>flDPnn_score</th>\n",
       "      <th>seqlength</th>\n",
       "      <th>PTMbinary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A024R1R8</td>\n",
       "      <td>0.747,0.578,0.611,0.572,0.581,0.494,0.366,0.52...</td>\n",
       "      <td>0.026,0.021,0.022,0.020,0.018,0.019,0.023,0.02...</td>\n",
       "      <td>0.036,0.092,0.094,0.123,0.070,0.081,0.080,0.09...</td>\n",
       "      <td>0.441,0.247,0.219,0.259,0.200,0.280,0.291,0.30...</td>\n",
       "      <td>0.478,0.444,0.516,0.472,0.430,0.489,0.541,0.50...</td>\n",
       "      <td>0.582,0.621,0.632,0.638,0.652,0.622,0.609,0.61...</td>\n",
       "      <td>0.034,0.029,0.024,0.024,0.024,0.024,0.023,0.01...</td>\n",
       "      <td>3.69,2.85,2.55,2.69,2.60,2.57,2.57,2.76,2.52,2...</td>\n",
       "      <td>0.835,0.839,0.848,0.862,0.859,0.859,0.871,0.89...</td>\n",
       "      <td>0,8,28,32,41,46,59,78,73,92,78,50,60,20,61,93,...</td>\n",
       "      <td>0,1,1,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>0.949,0.884,0.764,0.823,0.763,0.447,0.292,0.25...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.96,0.94,0.93,0.92,0.89,0.89,0.93,0.90,0.84,0...</td>\n",
       "      <td>64</td>\n",
       "      <td>0,1,1,0,0,0,0,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A024RBG1</td>\n",
       "      <td>0.695,0.392,0.521,0.209,0.444,0.421,0.455,0.40...</td>\n",
       "      <td>0.024,0.032,0.040,0.038,0.043,0.055,0.059,0.07...</td>\n",
       "      <td>0.099,0.090,0.134,0.082,0.236,0.132,0.199,0.36...</td>\n",
       "      <td>0.081,0.078,0.067,0.048,0.060,0.059,0.057,0.06...</td>\n",
       "      <td>0.172,0.142,0.135,0.145,0.110,0.116,0.094,0.12...</td>\n",
       "      <td>0.505,0.495,0.473,0.483,0.483,0.486,0.484,0.49...</td>\n",
       "      <td>0.060,0.057,0.055,0.053,0.056,0.048,0.046,0.05...</td>\n",
       "      <td>3.69,3.68,2.81,2.84,2.81,3.23,3.09,3.37,2.97,2...</td>\n",
       "      <td>0.653,0.663,0.677,0.673,0.675,0.673,0.669,0.66...</td>\n",
       "      <td>0,3,2,2,1,3,7,5,4,10,4,2,2,22,49,57,78,80,67,3...</td>\n",
       "      <td>0,4,7,8,3,3,3,7,19,24,39,27,9,4,1,1,2,4,11,32,...</td>\n",
       "      <td>0.278,0.234,0.214,0.239,0.288,0.276,0.251,0.29...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.35,0.38,0.42,0.46,0.45,0.45,0.45,0.42,0.38,0...</td>\n",
       "      <td>181</td>\n",
       "      <td>0,0,6,0,6,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A075B6H5</td>\n",
       "      <td>0.694,0.501,0.365,0.286,0.237,0.290,0.280,0.20...</td>\n",
       "      <td>0.068,0.082,0.084,0.107,0.138,0.158,0.191,0.22...</td>\n",
       "      <td>0.181,0.109,0.309,0.136,0.190,0.342,0.386,0.13...</td>\n",
       "      <td>0.052,0.044,0.032,0.030,0.031,0.034,0.034,0.03...</td>\n",
       "      <td>0.060,0.069,0.076,0.081,0.088,0.086,0.095,0.10...</td>\n",
       "      <td>0.481,0.501,0.490,0.473,0.473,0.476,0.468,0.42...</td>\n",
       "      <td>0.038,0.030,0.037,0.030,0.031,0.035,0.030,0.03...</td>\n",
       "      <td>3.68,2.88,2.96,2.74,2.61,2.96,2.96,2.19,3.23,2...</td>\n",
       "      <td>0.500,0.485,0.511,0.541,0.541,0.546,0.550,0.55...</td>\n",
       "      <td>0,1,1,2,3,3,2,3,4,10,16,7,1,2,3,70,77,95,94,94...</td>\n",
       "      <td>0,30,59,73,82,77,55,16,10,7,3,2,2,3,3,1,1,1,1,...</td>\n",
       "      <td>0.377,0.330,0.265,0.216,0.230,0.296,0.278,0.22...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.33,0.32,0.33,0.28,0.33,0.30,0.35,0.38,0.40,0...</td>\n",
       "      <td>130</td>\n",
       "      <td>0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A075B6H7</td>\n",
       "      <td>0.721,0.579,0.358,0.389,0.238,0.276,0.085,0.08...</td>\n",
       "      <td>0.047,0.046,0.043,0.040,0.038,0.043,0.040,0.04...</td>\n",
       "      <td>0.152,0.203,0.157,0.115,0.130,0.130,0.099,0.09...</td>\n",
       "      <td>0.066,0.065,0.062,0.062,0.063,0.063,0.060,0.06...</td>\n",
       "      <td>0.072,0.077,0.081,0.048,0.052,0.053,0.054,0.05...</td>\n",
       "      <td>0.430,0.428,0.410,0.434,0.466,0.439,0.434,0.43...</td>\n",
       "      <td>0.042,0.027,0.018,0.022,0.020,0.024,0.023,0.01...</td>\n",
       "      <td>3.69,1.81,1.94,3.06,2.10,3.33,1.73,2.16,2.10,2...</td>\n",
       "      <td>0.546,0.539,0.545,0.549,0.559,0.571,0.587,0.58...</td>\n",
       "      <td>0,24,67,83,94,99,99,100,100,100,99,96,83,62,3,...</td>\n",
       "      <td>0,2,1,2,1,0,0,0,0,0,0,0,1,1,0,2,3,7,11,24,44,4...</td>\n",
       "      <td>0.146,0.148,0.107,0.170,0.141,0.178,0.136,0.17...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.14,0.13,0.12,0.07,0.10,0.07,0.08,0.08,0.11,0...</td>\n",
       "      <td>116</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A075B6H8</td>\n",
       "      <td>0.649,0.510,0.364,0.481,0.218,0.241,0.259,0.37...</td>\n",
       "      <td>0.060,0.054,0.067,0.061,0.056,0.052,0.048,0.05...</td>\n",
       "      <td>0.431,0.393,0.548,0.570,0.354,0.274,0.292,0.26...</td>\n",
       "      <td>0.038,0.038,0.040,0.037,0.036,0.036,0.036,0.03...</td>\n",
       "      <td>0.174,0.172,0.171,0.169,0.168,0.105,0.107,0.10...</td>\n",
       "      <td>0.252,0.282,0.286,0.287,0.311,0.357,0.381,0.35...</td>\n",
       "      <td>0.105,0.088,0.059,0.047,0.048,0.056,0.047,0.05...</td>\n",
       "      <td>3.68,2.92,3.67,2.93,1.84,3.23,2.11,3.35,2.04,2...</td>\n",
       "      <td>0.459,0.468,0.512,0.522,0.529,0.530,0.536,0.53...</td>\n",
       "      <td>0,10,29,48,82,91,97,99,99,99,99,100,99,99,95,8...</td>\n",
       "      <td>0,1,3,3,1,1,0,0,0,0,0,0,0,0,0,1,1,4,5,12,36,71...</td>\n",
       "      <td>0.395,0.278,0.208,0.353,0.186,0.211,0.164,0.25...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.26,0.17,0.18,0.13,0.12,0.07,0.09,0.09,0.09,0...</td>\n",
       "      <td>117</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20449</th>\n",
       "      <td>V9GZ13</td>\n",
       "      <td>0.752,0.627,0.587,0.419,0.434,0.395,0.326,0.47...</td>\n",
       "      <td>0.070,0.089,0.095,0.084,0.076,0.080,0.081,0.09...</td>\n",
       "      <td>0.818,0.894,0.837,0.867,0.869,0.882,0.437,0.88...</td>\n",
       "      <td>0.020,0.020,0.020,0.020,0.020,0.020,0.020,0.02...</td>\n",
       "      <td>0.054,0.059,0.064,0.069,0.074,0.080,0.081,0.09...</td>\n",
       "      <td>0.147,0.147,0.172,0.176,0.182,0.171,0.170,0.17...</td>\n",
       "      <td>0.052,0.057,0.056,0.061,0.047,0.044,0.041,0.04...</td>\n",
       "      <td>3.68,2.83,3.16,3.36,2.84,4.34,1.60,2.95,2.30,3...</td>\n",
       "      <td>0.860,0.890,0.901,0.982,0.981,0.981,0.981,0.98...</td>\n",
       "      <td>0,4,15,31,63,80,83,83,87,75,70,65,80,80,64,70,...</td>\n",
       "      <td>0,2,3,4,2,2,2,2,1,2,9,25,13,8,12,10,8,13,37,24...</td>\n",
       "      <td>0.873,0.716,0.745,0.722,0.672,0.855,0.643,0.76...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.41,0.44,0.45,0.41,0.32,0.19,0.12,0.13,0.13,0...</td>\n",
       "      <td>50</td>\n",
       "      <td>0,6,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20450</th>\n",
       "      <td>W5XKT8</td>\n",
       "      <td>0.590,0.263,0.221,0.247,0.248,0.137,0.144,0.32...</td>\n",
       "      <td>0.045,0.050,0.062,0.074,0.075,0.068,0.062,0.06...</td>\n",
       "      <td>0.119,0.088,0.078,0.082,0.103,0.097,0.119,0.32...</td>\n",
       "      <td>0.066,0.051,0.049,0.049,0.051,0.050,0.050,0.05...</td>\n",
       "      <td>0.185,0.199,0.198,0.195,0.192,0.191,0.189,0.18...</td>\n",
       "      <td>0.259,0.224,0.224,0.269,0.271,0.258,0.261,0.25...</td>\n",
       "      <td>0.302,0.303,0.292,0.298,0.214,0.161,0.166,0.14...</td>\n",
       "      <td>3.68,2.56,2.34,1.76,2.37,2.60,2.24,2.60,1.59,2...</td>\n",
       "      <td>0.530,0.534,0.533,0.525,0.515,0.505,0.454,0.42...</td>\n",
       "      <td>0,83,91,95,98,96,81,54,74,64,87,98,99,99,100,9...</td>\n",
       "      <td>0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>0.193,0.137,0.173,0.192,0.117,0.175,0.151,0.16...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.15,0.16,0.14,0.14,0.14,0.13,0.12,0.11,0.11,0...</td>\n",
       "      <td>324</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20451</th>\n",
       "      <td>W6CW81</td>\n",
       "      <td>0.703,0.524,0.455,0.454,0.260,0.369,0.272,0.16...</td>\n",
       "      <td>0.025,0.027,0.027,0.034,0.043,0.041,0.044,0.05...</td>\n",
       "      <td>0.257,0.144,0.442,0.246,0.349,0.418,0.124,0.11...</td>\n",
       "      <td>0.058,0.044,0.050,0.046,0.049,0.050,0.043,0.04...</td>\n",
       "      <td>0.156,0.144,0.158,0.158,0.150,0.122,0.121,0.11...</td>\n",
       "      <td>0.315,0.311,0.310,0.331,0.356,0.355,0.340,0.34...</td>\n",
       "      <td>0.022,0.026,0.017,0.018,0.019,0.021,0.018,0.01...</td>\n",
       "      <td>3.69,2.83,2.82,2.57,3.44,2.26,2.86,2.41,2.16,2...</td>\n",
       "      <td>0.548,0.555,0.586,0.602,0.602,0.611,0.618,0.62...</td>\n",
       "      <td>0,37,57,79,83,88,90,91,79,78,76,59,39,51,31,25...</td>\n",
       "      <td>0,0,1,2,2,2,3,7,14,7,13,17,12,5,3,2,1,0,0,0,0,...</td>\n",
       "      <td>0.247,0.219,0.167,0.181,0.205,0.161,0.161,0.15...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.41,0.36,0.32,0.20,0.26,0.26,0.19,0.19,0.15,0...</td>\n",
       "      <td>113</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20452</th>\n",
       "      <td>X5D2U9</td>\n",
       "      <td>0.570,0.295,0.150,0.142,0.357,0.109,0.425,0.51...</td>\n",
       "      <td>0.077,0.092,0.100,0.075,0.077,0.086,0.081,0.09...</td>\n",
       "      <td>0.077,0.072,0.071,0.066,0.143,0.080,0.097,0.12...</td>\n",
       "      <td>0.147,0.116,0.110,0.073,0.093,0.077,0.114,0.09...</td>\n",
       "      <td>0.301,0.305,0.295,0.286,0.275,0.253,0.247,0.23...</td>\n",
       "      <td>0.386,0.373,0.355,0.304,0.291,0.305,0.308,0.31...</td>\n",
       "      <td>0.048,0.050,0.058,0.068,0.048,0.055,0.042,0.05...</td>\n",
       "      <td>3.67,2.52,3.68,2.26,3.79,2.37,3.17,2.13,2.58,2...</td>\n",
       "      <td>0.535,0.551,0.561,0.566,0.566,0.560,0.523,0.44...</td>\n",
       "      <td>0,1,1,2,1,1,2,4,19,25,31,21,29,20,18,28,24,52,...</td>\n",
       "      <td>0,52,75,84,83,42,8,2,4,9,27,41,49,67,65,65,69,...</td>\n",
       "      <td>0.119,0.120,0.112,0.128,0.118,0.142,0.214,0.15...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.34,0.33,0.33,0.25,0.24,0.21,0.22,0.17,0.11,0...</td>\n",
       "      <td>266</td>\n",
       "      <td>0,0,8,0,6,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20453</th>\n",
       "      <td>X6R8D5</td>\n",
       "      <td>0.669,0.453,0.485,0.515,0.492,0.433,0.503,0.31...</td>\n",
       "      <td>0.053,0.062,0.062,0.070,0.058,0.045,0.048,0.05...</td>\n",
       "      <td>0.119,0.110,0.172,0.193,0.092,0.196,0.094,0.18...</td>\n",
       "      <td>0.068,0.064,0.063,0.065,0.058,0.064,0.058,0.06...</td>\n",
       "      <td>0.168,0.156,0.174,0.184,0.190,0.193,0.124,0.09...</td>\n",
       "      <td>0.739,0.758,0.766,0.752,0.783,0.777,0.776,0.77...</td>\n",
       "      <td>0.010,0.012,0.010,0.012,0.009,0.008,0.007,0.00...</td>\n",
       "      <td>2.36,2.11,2.95,2.26,2.70,3.53,2.70,1.87,3.22,1...</td>\n",
       "      <td>0.655,0.660,0.673,0.676,0.681,0.683,0.682,0.68...</td>\n",
       "      <td>0,2,9,13,20,14,6,1,2,1,1,2,4,9,9,3,3,23,23,28,...</td>\n",
       "      <td>0,2,3,6,6,4,3,1,1,1,1,2,6,9,5,3,4,3,3,2,2,1,1,...</td>\n",
       "      <td>0.824,0.652,0.828,0.768,0.767,0.817,0.766,0.73...</td>\n",
       "      <td>0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...</td>\n",
       "      <td>0.62,0.67,0.64,0.60,0.58,0.64,0.63,0.69,0.64,0...</td>\n",
       "      <td>127</td>\n",
       "      <td>0,0,0,6,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20454 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UniprotEntry                                 ASAquick_normscore  \\\n",
       "0       A0A024R1R8  0.747,0.578,0.611,0.572,0.581,0.494,0.366,0.52...   \n",
       "1       A0A024RBG1  0.695,0.392,0.521,0.209,0.444,0.421,0.455,0.40...   \n",
       "2       A0A075B6H5  0.694,0.501,0.365,0.286,0.237,0.290,0.280,0.20...   \n",
       "3       A0A075B6H7  0.721,0.579,0.358,0.389,0.238,0.276,0.085,0.08...   \n",
       "4       A0A075B6H8  0.649,0.510,0.364,0.481,0.218,0.241,0.259,0.37...   \n",
       "...            ...                                                ...   \n",
       "20449       V9GZ13  0.752,0.627,0.587,0.419,0.434,0.395,0.326,0.47...   \n",
       "20450       W5XKT8  0.590,0.263,0.221,0.247,0.248,0.137,0.144,0.32...   \n",
       "20451       W6CW81  0.703,0.524,0.455,0.454,0.260,0.369,0.272,0.16...   \n",
       "20452       X5D2U9  0.570,0.295,0.150,0.142,0.357,0.109,0.425,0.51...   \n",
       "20453       X6R8D5  0.669,0.453,0.485,0.515,0.492,0.433,0.503,0.31...   \n",
       "\n",
       "                                            DFLpredScore  \\\n",
       "0      0.026,0.021,0.022,0.020,0.018,0.019,0.023,0.02...   \n",
       "1      0.024,0.032,0.040,0.038,0.043,0.055,0.059,0.07...   \n",
       "2      0.068,0.082,0.084,0.107,0.138,0.158,0.191,0.22...   \n",
       "3      0.047,0.046,0.043,0.040,0.038,0.043,0.040,0.04...   \n",
       "4      0.060,0.054,0.067,0.061,0.056,0.052,0.048,0.05...   \n",
       "...                                                  ...   \n",
       "20449  0.070,0.089,0.095,0.084,0.076,0.080,0.081,0.09...   \n",
       "20450  0.045,0.050,0.062,0.074,0.075,0.068,0.062,0.06...   \n",
       "20451  0.025,0.027,0.027,0.034,0.043,0.041,0.044,0.05...   \n",
       "20452  0.077,0.092,0.100,0.075,0.077,0.086,0.081,0.09...   \n",
       "20453  0.053,0.062,0.062,0.070,0.058,0.045,0.048,0.05...   \n",
       "\n",
       "                                        DRNApredDNAscore  \\\n",
       "0      0.036,0.092,0.094,0.123,0.070,0.081,0.080,0.09...   \n",
       "1      0.099,0.090,0.134,0.082,0.236,0.132,0.199,0.36...   \n",
       "2      0.181,0.109,0.309,0.136,0.190,0.342,0.386,0.13...   \n",
       "3      0.152,0.203,0.157,0.115,0.130,0.130,0.099,0.09...   \n",
       "4      0.431,0.393,0.548,0.570,0.354,0.274,0.292,0.26...   \n",
       "...                                                  ...   \n",
       "20449  0.818,0.894,0.837,0.867,0.869,0.882,0.437,0.88...   \n",
       "20450  0.119,0.088,0.078,0.082,0.103,0.097,0.119,0.32...   \n",
       "20451  0.257,0.144,0.442,0.246,0.349,0.418,0.124,0.11...   \n",
       "20452  0.077,0.072,0.071,0.066,0.143,0.080,0.097,0.12...   \n",
       "20453  0.119,0.110,0.172,0.193,0.092,0.196,0.094,0.18...   \n",
       "\n",
       "                                        DRNApredRNAscore  \\\n",
       "0      0.441,0.247,0.219,0.259,0.200,0.280,0.291,0.30...   \n",
       "1      0.081,0.078,0.067,0.048,0.060,0.059,0.057,0.06...   \n",
       "2      0.052,0.044,0.032,0.030,0.031,0.034,0.034,0.03...   \n",
       "3      0.066,0.065,0.062,0.062,0.063,0.063,0.060,0.06...   \n",
       "4      0.038,0.038,0.040,0.037,0.036,0.036,0.036,0.03...   \n",
       "...                                                  ...   \n",
       "20449  0.020,0.020,0.020,0.020,0.020,0.020,0.020,0.02...   \n",
       "20450  0.066,0.051,0.049,0.049,0.051,0.050,0.050,0.05...   \n",
       "20451  0.058,0.044,0.050,0.046,0.049,0.050,0.043,0.04...   \n",
       "20452  0.147,0.116,0.110,0.073,0.093,0.077,0.114,0.09...   \n",
       "20453  0.068,0.064,0.063,0.065,0.058,0.064,0.058,0.06...   \n",
       "\n",
       "                                            DisoDNAscore  \\\n",
       "0      0.478,0.444,0.516,0.472,0.430,0.489,0.541,0.50...   \n",
       "1      0.172,0.142,0.135,0.145,0.110,0.116,0.094,0.12...   \n",
       "2      0.060,0.069,0.076,0.081,0.088,0.086,0.095,0.10...   \n",
       "3      0.072,0.077,0.081,0.048,0.052,0.053,0.054,0.05...   \n",
       "4      0.174,0.172,0.171,0.169,0.168,0.105,0.107,0.10...   \n",
       "...                                                  ...   \n",
       "20449  0.054,0.059,0.064,0.069,0.074,0.080,0.081,0.09...   \n",
       "20450  0.185,0.199,0.198,0.195,0.192,0.191,0.189,0.18...   \n",
       "20451  0.156,0.144,0.158,0.158,0.150,0.122,0.121,0.11...   \n",
       "20452  0.301,0.305,0.295,0.286,0.275,0.253,0.247,0.23...   \n",
       "20453  0.168,0.156,0.174,0.184,0.190,0.193,0.124,0.09...   \n",
       "\n",
       "                                            DisoPROscore  \\\n",
       "0      0.582,0.621,0.632,0.638,0.652,0.622,0.609,0.61...   \n",
       "1      0.505,0.495,0.473,0.483,0.483,0.486,0.484,0.49...   \n",
       "2      0.481,0.501,0.490,0.473,0.473,0.476,0.468,0.42...   \n",
       "3      0.430,0.428,0.410,0.434,0.466,0.439,0.434,0.43...   \n",
       "4      0.252,0.282,0.286,0.287,0.311,0.357,0.381,0.35...   \n",
       "...                                                  ...   \n",
       "20449  0.147,0.147,0.172,0.176,0.182,0.171,0.170,0.17...   \n",
       "20450  0.259,0.224,0.224,0.269,0.271,0.258,0.261,0.25...   \n",
       "20451  0.315,0.311,0.310,0.331,0.356,0.355,0.340,0.34...   \n",
       "20452  0.386,0.373,0.355,0.304,0.291,0.305,0.308,0.31...   \n",
       "20453  0.739,0.758,0.766,0.752,0.783,0.777,0.776,0.77...   \n",
       "\n",
       "                                            DisoRNAscore  \\\n",
       "0      0.034,0.029,0.024,0.024,0.024,0.024,0.023,0.01...   \n",
       "1      0.060,0.057,0.055,0.053,0.056,0.048,0.046,0.05...   \n",
       "2      0.038,0.030,0.037,0.030,0.031,0.035,0.030,0.03...   \n",
       "3      0.042,0.027,0.018,0.022,0.020,0.024,0.023,0.01...   \n",
       "4      0.105,0.088,0.059,0.047,0.048,0.056,0.047,0.05...   \n",
       "...                                                  ...   \n",
       "20449  0.052,0.057,0.056,0.061,0.047,0.044,0.041,0.04...   \n",
       "20450  0.302,0.303,0.292,0.298,0.214,0.161,0.166,0.14...   \n",
       "20451  0.022,0.026,0.017,0.018,0.019,0.021,0.018,0.01...   \n",
       "20452  0.048,0.050,0.058,0.068,0.048,0.055,0.042,0.05...   \n",
       "20453  0.010,0.012,0.010,0.012,0.009,0.008,0.007,0.00...   \n",
       "\n",
       "                               MMseq2_conservation_score  \\\n",
       "0      3.69,2.85,2.55,2.69,2.60,2.57,2.57,2.76,2.52,2...   \n",
       "1      3.69,3.68,2.81,2.84,2.81,3.23,3.09,3.37,2.97,2...   \n",
       "2      3.68,2.88,2.96,2.74,2.61,2.96,2.96,2.19,3.23,2...   \n",
       "3      3.69,1.81,1.94,3.06,2.10,3.33,1.73,2.16,2.10,2...   \n",
       "4      3.68,2.92,3.67,2.93,1.84,3.23,2.11,3.35,2.04,2...   \n",
       "...                                                  ...   \n",
       "20449  3.68,2.83,3.16,3.36,2.84,4.34,1.60,2.95,2.30,3...   \n",
       "20450  3.68,2.56,2.34,1.76,2.37,2.60,2.24,2.60,1.59,2...   \n",
       "20451  3.69,2.83,2.82,2.57,3.44,2.26,2.86,2.41,2.16,2...   \n",
       "20452  3.67,2.52,3.68,2.26,3.79,2.37,3.17,2.13,2.58,2...   \n",
       "20453  2.36,2.11,2.95,2.26,2.70,3.53,2.70,1.87,3.22,1...   \n",
       "\n",
       "                                          MoRFchibiScore  \\\n",
       "0      0.835,0.839,0.848,0.862,0.859,0.859,0.871,0.89...   \n",
       "1      0.653,0.663,0.677,0.673,0.675,0.673,0.669,0.66...   \n",
       "2      0.500,0.485,0.511,0.541,0.541,0.546,0.550,0.55...   \n",
       "3      0.546,0.539,0.545,0.549,0.559,0.571,0.587,0.58...   \n",
       "4      0.459,0.468,0.512,0.522,0.529,0.530,0.536,0.53...   \n",
       "...                                                  ...   \n",
       "20449  0.860,0.890,0.901,0.982,0.981,0.981,0.981,0.98...   \n",
       "20450  0.530,0.534,0.533,0.525,0.515,0.505,0.454,0.42...   \n",
       "20451  0.548,0.555,0.586,0.602,0.602,0.611,0.618,0.62...   \n",
       "20452  0.535,0.551,0.561,0.566,0.566,0.560,0.523,0.44...   \n",
       "20453  0.655,0.660,0.673,0.676,0.681,0.683,0.682,0.68...   \n",
       "\n",
       "                                           PSIPRED_helix  \\\n",
       "0      0,8,28,32,41,46,59,78,73,92,78,50,60,20,61,93,...   \n",
       "1      0,3,2,2,1,3,7,5,4,10,4,2,2,22,49,57,78,80,67,3...   \n",
       "2      0,1,1,2,3,3,2,3,4,10,16,7,1,2,3,70,77,95,94,94...   \n",
       "3      0,24,67,83,94,99,99,100,100,100,99,96,83,62,3,...   \n",
       "4      0,10,29,48,82,91,97,99,99,99,99,100,99,99,95,8...   \n",
       "...                                                  ...   \n",
       "20449  0,4,15,31,63,80,83,83,87,75,70,65,80,80,64,70,...   \n",
       "20450  0,83,91,95,98,96,81,54,74,64,87,98,99,99,100,9...   \n",
       "20451  0,37,57,79,83,88,90,91,79,78,76,59,39,51,31,25...   \n",
       "20452  0,1,1,2,1,1,2,4,19,25,31,21,29,20,18,28,24,52,...   \n",
       "20453  0,2,9,13,20,14,6,1,2,1,1,2,4,9,9,3,3,23,23,28,...   \n",
       "\n",
       "                                          PSIPRED_strand  \\\n",
       "0      0,1,1,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "1      0,4,7,8,3,3,3,7,19,24,39,27,9,4,1,1,2,4,11,32,...   \n",
       "2      0,30,59,73,82,77,55,16,10,7,3,2,2,3,3,1,1,1,1,...   \n",
       "3      0,2,1,2,1,0,0,0,0,0,0,0,1,1,0,2,3,7,11,24,44,4...   \n",
       "4      0,1,3,3,1,1,0,0,0,0,0,0,0,0,0,1,1,4,5,12,36,71...   \n",
       "...                                                  ...   \n",
       "20449  0,2,3,4,2,2,2,2,1,2,9,25,13,8,12,10,8,13,37,24...   \n",
       "20450  0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "20451  0,0,1,2,2,2,3,7,14,7,13,17,12,5,3,2,1,0,0,0,0,...   \n",
       "20452  0,52,75,84,83,42,8,2,4,9,27,41,49,67,65,65,69,...   \n",
       "20453  0,2,3,6,6,4,3,1,1,1,1,2,6,9,5,3,4,3,3,2,2,1,1,...   \n",
       "\n",
       "                                            SCRIBERscore  \\\n",
       "0      0.949,0.884,0.764,0.823,0.763,0.447,0.292,0.25...   \n",
       "1      0.278,0.234,0.214,0.239,0.288,0.276,0.251,0.29...   \n",
       "2      0.377,0.330,0.265,0.216,0.230,0.296,0.278,0.22...   \n",
       "3      0.146,0.148,0.107,0.170,0.141,0.178,0.136,0.17...   \n",
       "4      0.395,0.278,0.208,0.353,0.186,0.211,0.164,0.25...   \n",
       "...                                                  ...   \n",
       "20449  0.873,0.716,0.745,0.722,0.672,0.855,0.643,0.76...   \n",
       "20450  0.193,0.137,0.173,0.192,0.117,0.175,0.151,0.16...   \n",
       "20451  0.247,0.219,0.167,0.181,0.205,0.161,0.161,0.15...   \n",
       "20452  0.119,0.120,0.112,0.128,0.118,0.142,0.214,0.15...   \n",
       "20453  0.824,0.652,0.828,0.768,0.767,0.817,0.766,0.73...   \n",
       "\n",
       "                                           SignalP_score  \\\n",
       "0      0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "1      0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "2      0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "3      0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "4      0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "...                                                  ...   \n",
       "20449  0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "20450  0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "20451  0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "20452  0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "20453  0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.00...   \n",
       "\n",
       "                                            flDPnn_score  seqlength  \\\n",
       "0      0.96,0.94,0.93,0.92,0.89,0.89,0.93,0.90,0.84,0...         64   \n",
       "1      0.35,0.38,0.42,0.46,0.45,0.45,0.45,0.42,0.38,0...        181   \n",
       "2      0.33,0.32,0.33,0.28,0.33,0.30,0.35,0.38,0.40,0...        130   \n",
       "3      0.14,0.13,0.12,0.07,0.10,0.07,0.08,0.08,0.11,0...        116   \n",
       "4      0.26,0.17,0.18,0.13,0.12,0.07,0.09,0.09,0.09,0...        117   \n",
       "...                                                  ...        ...   \n",
       "20449  0.41,0.44,0.45,0.41,0.32,0.19,0.12,0.13,0.13,0...         50   \n",
       "20450  0.15,0.16,0.14,0.14,0.14,0.13,0.12,0.11,0.11,0...        324   \n",
       "20451  0.41,0.36,0.32,0.20,0.26,0.26,0.19,0.19,0.15,0...        113   \n",
       "20452  0.34,0.33,0.33,0.25,0.24,0.21,0.22,0.17,0.11,0...        266   \n",
       "20453  0.62,0.67,0.64,0.60,0.58,0.64,0.63,0.69,0.64,0...        127   \n",
       "\n",
       "                                               PTMbinary  \n",
       "0      0,1,1,0,0,0,0,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "1      0,0,6,0,6,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,...  \n",
       "2      0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "3      0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "4      0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "...                                                  ...  \n",
       "20449  0,6,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "20450  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "20451  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,...  \n",
       "20452  0,0,8,0,6,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "20453  0,0,0,6,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,...  \n",
       "\n",
       "[20454 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read extra features from describeProt\n",
    "\n",
    "describe_prot = pd.read_csv(\"../data/describePROT_clean.tsv\", sep=\"\\t\")\n",
    "describe_prot_featnames = describe_prot.columns[(describe_prot.columns != \"seqlength\") & (describe_prot.columns != \"UniprotEntry\") & (describe_prot.columns != \"SignalP_score\")] #SignalP_score doesnt match the protein length\n",
    "describe_prot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6401c9-9e68-4116-b374-9be21923b52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Length</th>\n",
       "      <th>ACT_SITE</th>\n",
       "      <th>BINDING</th>\n",
       "      <th>DNA_BIND</th>\n",
       "      <th>TOPO_DOM</th>\n",
       "      <th>TRANSMEM</th>\n",
       "      <th>DISULFID</th>\n",
       "      <th>PROPEP</th>\n",
       "      <th>SIGNAL</th>\n",
       "      <th>TRANSIT</th>\n",
       "      <th>STRAND</th>\n",
       "      <th>HELIX</th>\n",
       "      <th>COILED</th>\n",
       "      <th>COMPBIAS</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>REGION</th>\n",
       "      <th>REPEAT</th>\n",
       "      <th>ZN_FING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087X1C5</td>\n",
       "      <td>515</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1100000000000000000000011111111111111111111111...</td>\n",
       "      <td>0011111111111111111111100000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000001...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000011...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A0B4J2F0</td>\n",
       "      <td>54</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111000000000000000000000111111111111111111111...</td>\n",
       "      <td>0000111111111111111111111000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111110000000000000000000000000...</td>\n",
       "      <td>1100000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000001111111111111111111111111111111111111000...</td>\n",
       "      <td>0000000000000000000000000011111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000011...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000011111110000000000...</td>\n",
       "      <td>0000000000000001000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0B4J2F2</td>\n",
       "      <td>783</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000011111111100000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111111111111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000011111111100011111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000011111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A0C5B5G6</td>\n",
       "      <td>16</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>1111111111111111</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000011100000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>0000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>201</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000001111111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000001000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111110000000000000000000000...</td>\n",
       "      <td>1111111111111110000010000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000111111000011111110...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000001111111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20429</th>\n",
       "      <td>Q9UI54</td>\n",
       "      <td>55</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000001111111111...</td>\n",
       "      <td>0000000000000011111111111111111111100000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111111111111111100000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>Q9UI72</td>\n",
       "      <td>69</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111111111111111111111111111...</td>\n",
       "      <td>1111111111000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111100000000000000000000000...</td>\n",
       "      <td>1000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000111111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000111111111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20431</th>\n",
       "      <td>Q9Y3F1</td>\n",
       "      <td>56</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111000000000000000000000001111111111111111111...</td>\n",
       "      <td>0011001111111111111111111110000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000100010000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111100000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20432</th>\n",
       "      <td>Q9Y6C7</td>\n",
       "      <td>94</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0110000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000110011110011111111100111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111111110000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000010111111111111111111000000111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20433</th>\n",
       "      <td>Q9Y6Z2</td>\n",
       "      <td>57</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111111111111111111111111111111111111111111...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000010000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111111100000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000011111100000111111111110000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>1111110001000101010000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20434 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry  Length                                           ACT_SITE  \\\n",
       "0      A0A087X1C5     515  0000000000000000000000000000000000000000000000...   \n",
       "1      A0A0B4J2F0      54  0000000000000000000000000000000000000000000000...   \n",
       "2      A0A0B4J2F2     783  0000000000000000000000000000000000000000000000...   \n",
       "3      A0A0C5B5G6      16                                   0000000000000000   \n",
       "4      A0A0K2S4Q6     201  0000000000000000000000000000000000000000000000...   \n",
       "...           ...     ...                                                ...   \n",
       "20429      Q9UI54      55  0000000000000000000000000000000000000000000000...   \n",
       "20430      Q9UI72      69  0000000000000000000000000000000000000000000000...   \n",
       "20431      Q9Y3F1      56  0000000000000000000000000000000000000000000000...   \n",
       "20432      Q9Y6C7      94  0000000000000000000000000000000000000000000000...   \n",
       "20433      Q9Y6Z2      57  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                 BINDING  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000000000011111111100000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                DNA_BIND  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                TOPO_DOM  \\\n",
       "0      1100000000000000000000011111111111111111111111...   \n",
       "1      1111000000000000000000000111111111111111111111...   \n",
       "2      1111111111111111111111111111111111111111111111...   \n",
       "3                                       1111111111111111   \n",
       "4      0000000000000000000000001111111111111111111111...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000001111111111...   \n",
       "20430  1111111111111111111111111111111111111111111111...   \n",
       "20431  1111000000000000000000000001111111111111111111...   \n",
       "20432  0110000000000000000000000000000000000000000000...   \n",
       "20433  1111111111111111111111111111111111111111111111...   \n",
       "\n",
       "                                                TRANSMEM  \\\n",
       "0      0011111111111111111111100000000000000000000000...   \n",
       "1      0000111111111111111111111000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000011111111111111111111100000000000...   \n",
       "20430  1111111111000000000000000000000000000000000000...   \n",
       "20431  0011001111111111111111111110000000000000000000...   \n",
       "20432  0000000000110011110011111111100111111111111111...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                DISULFID  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000001000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000100010000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000010000000000000...   \n",
       "\n",
       "                                                  PROPEP  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                  SIGNAL  \\\n",
       "0      1111111111111111111111000000000000000000000000...   \n",
       "1      1111111111111111111110000000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      1111111111111111111111110000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  1111111111111111111111111111111111100000000000...   \n",
       "20430  1111111111111111111111100000000000000000000000...   \n",
       "20431  1111111111111111111100000000000000000000000000...   \n",
       "20432  1111111111111111111111111110000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                 TRANSIT  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      1100000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      1111111111111110000010000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  1000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  1111111100000000000000000000000000000000000000...   \n",
       "\n",
       "                                                  STRAND  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000011111111100011111111...   \n",
       "3                                       0000000011100000   \n",
       "4      0000000000000000000000000000111111000011111110...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000011111100000111111111110000...   \n",
       "\n",
       "                                                   HELIX  \\\n",
       "0      0000000000000000000000000000000000000000000001...   \n",
       "1      0000001111111111111111111111111111111111111000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000010111111111111111111000000111111111111...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                  COILED  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000011111111111111111111...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                COMPBIAS  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000000000000000000011...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000111111111111111111111...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                  DOMAIN  \\\n",
       "0      0000000000000000000000000000000000000000000011...   \n",
       "1      0000000000000000000000000000000000000000000000...   \n",
       "2      0000000000000000000000000011111111111111111111...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000001111111111111111111111...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                  REGION  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000000000000000000011111110000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000111111111111111111111111...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  1111110001000101010000000000000000000000000000...   \n",
       "\n",
       "                                                  REPEAT  \\\n",
       "0      0000000000000000000000000000000000000000000000...   \n",
       "1      0000000000000001000000000000000000000000000000...   \n",
       "2      0000000000000000000000000000000000000000000000...   \n",
       "3                                       0000000000000000   \n",
       "4      0000000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "20429  0000000000000000000000000000000000000000000000...   \n",
       "20430  0000000000000000000000000000000000000000000000...   \n",
       "20431  0000000000000000000000000000000000000000000000...   \n",
       "20432  0000000000000000000000000000000000000000000000...   \n",
       "20433  0000000000000000000000000000000000000000000000...   \n",
       "\n",
       "                                                 ZN_FING  \n",
       "0      0000000000000000000000000000000000000000000000...  \n",
       "1      0000000000000000000000000000000000000000000000...  \n",
       "2      0000000000000000000000000000000000000000000000...  \n",
       "3                                       0000000000000000  \n",
       "4      0000000000000000000000000000000000000000000000...  \n",
       "...                                                  ...  \n",
       "20429  0000000000000000000000000000000000000000000000...  \n",
       "20430  0000000000000000000000000000000000000000000000...  \n",
       "20431  0000000000000000000000000000000000000000000000...  \n",
       "20432  0000000000000000000000000000000000000000000000...  \n",
       "20433  0000000000000000000000000000000000000000000000...  \n",
       "\n",
       "[20434 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniprot = pd.read_csv(\"../data/uniprot_all_human_proteins_annotated.txt\", sep=\"\\t\")\n",
    "uniprot_features = ['ACT_SITE', 'BINDING', 'DNA_BIND',\n",
    "                   'TOPO_DOM', 'TRANSMEM',\n",
    "                   'DISULFID',  'PROPEP', 'SIGNAL', 'TRANSIT',\n",
    "                   'STRAND', 'HELIX',\n",
    "                   'COILED', 'COMPBIAS', 'DOMAIN', 'REGION', 'REPEAT', 'ZN_FING']\n",
    "\n",
    "# Initialize a new DataFrame to store the output\n",
    "uniprot_clean = pd.DataFrame()\n",
    "uniprot_clean['Entry'] = uniprot['Entry']  # Copy 'Entry' column Length\n",
    "uniprot_clean['Length'] = uniprot['Length'] \n",
    "\n",
    "# Iterate over each feature and assign the correct value to the output\n",
    "for feature in uniprot_features:\n",
    "    actual_or_pred_col = f'actual_or_pred_{feature}'\n",
    "    actual_annotation_col = f'annotation_actual_{feature}'\n",
    "    pred_annotation_col = f'annotation_pred_{feature}'\n",
    "    \n",
    "    # Use `np.where` to conditionally select the 'actual' or 'pred' annotation\n",
    "    uniprot_clean[feature] = uniprot.apply(\n",
    "        lambda row: row[actual_annotation_col] if row[actual_or_pred_col] == 'actual' else row[pred_annotation_col], \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "uniprot_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67f2037-a708-4ac9-899f-dd812f2de85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>phyloP100way_vertebrate</th>\n",
       "      <th>phyloP30way_mammalian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A075B759</td>\n",
       "      <td>1.6,0.1,1.2,1.4,-0.1,3.1,1.5,2.3,0.7,-0.4,-1.7...</td>\n",
       "      <td>0.3,0.3,-0.5,-1.5,-0.8,-0.3,-0.1,0,-0.1,-0.1,-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A075B767</td>\n",
       "      <td>1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1....</td>\n",
       "      <td>0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A087WTH1</td>\n",
       "      <td>3.1,2.4,2.8,1.8,4.1,2.7,2.2,1.8,2.3,0.3,1.9,1....</td>\n",
       "      <td>1.2,0.6,1.2,0.9,1.2,1.3,1.1,0.7,0.9,-0.1,1.1,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A087WUV0</td>\n",
       "      <td>0.6,0.8,-0.4,-0.9,0.5,0.6,2.5,-0.1,0.5,0.9,1.3...</td>\n",
       "      <td>0.9,0.7,0.8,-0.2,0.5,1,0.6,0,0.4,0.1,0.1,0.9,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A087WV53</td>\n",
       "      <td>3,2.3,3.1,0.9,1.7,0.6,0.1,2.1,2.1,0.2,-0.4,1,0...</td>\n",
       "      <td>1,0.9,0.7,0.9,0.9,0.9,0.1,0.8,0.7,0.5,0.3,0.5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18174</th>\n",
       "      <td>Q9Y6Z7</td>\n",
       "      <td>4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4....</td>\n",
       "      <td>1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18175</th>\n",
       "      <td>U3KPV4</td>\n",
       "      <td>3.7,2.2,2.9,0.6,2.7,2.7,0.1,-0.4,7.4,2.8,0.4,2...</td>\n",
       "      <td>0.3,0.8,1,0.7,1.1,1.1,-0.1,0.4,1.2,1.1,-0.1,0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18176</th>\n",
       "      <td>W5XKT8</td>\n",
       "      <td>1.1,1.2,-0.2,-0.2,-0.6,-0.9,-1.9,-0.9,-0.2,0.3...</td>\n",
       "      <td>1.2,0.6,-0.2,-0.7,-0.3,-0.7,0.2,-0.4,-0.7,-0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18177</th>\n",
       "      <td>X6R8D5</td>\n",
       "      <td>-0.9,0.5,0.5,-0.3,1.2,-1.6,0.3,0.2,0.2,0.2,0,0...</td>\n",
       "      <td>0.3,-0.4,0.3,-0.2,0.8,-0.1,1.1,0.1,0.3,0.1,0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>X6R8R1</td>\n",
       "      <td>0,1.6,0.2,0.5,-1.8,1.6,4.9,1.9,3.6,6.9,0.7,2.3...</td>\n",
       "      <td>0.5,1.1,-0.5,-0.9,-0.9,0.4,1,0.3,0.4,1,0.1,0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18179 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                            phyloP100way_vertebrate  \\\n",
       "0      A0A075B759  1.6,0.1,1.2,1.4,-0.1,3.1,1.5,2.3,0.7,-0.4,-1.7...   \n",
       "1      A0A075B767  1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1.8,1....   \n",
       "2      A0A087WTH1  3.1,2.4,2.8,1.8,4.1,2.7,2.2,1.8,2.3,0.3,1.9,1....   \n",
       "3      A0A087WUV0  0.6,0.8,-0.4,-0.9,0.5,0.6,2.5,-0.1,0.5,0.9,1.3...   \n",
       "4      A0A087WV53  3,2.3,3.1,0.9,1.7,0.6,0.1,2.1,2.1,0.2,-0.4,1,0...   \n",
       "...           ...                                                ...   \n",
       "18174      Q9Y6Z7  4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4.4,4....   \n",
       "18175      U3KPV4  3.7,2.2,2.9,0.6,2.7,2.7,0.1,-0.4,7.4,2.8,0.4,2...   \n",
       "18176      W5XKT8  1.1,1.2,-0.2,-0.2,-0.6,-0.9,-1.9,-0.9,-0.2,0.3...   \n",
       "18177      X6R8D5  -0.9,0.5,0.5,-0.3,1.2,-1.6,0.3,0.2,0.2,0.2,0,0...   \n",
       "18178      X6R8R1  0,1.6,0.2,0.5,-1.8,1.6,4.9,1.9,3.6,6.9,0.7,2.3...   \n",
       "\n",
       "                                   phyloP30way_mammalian  \n",
       "0      0.3,0.3,-0.5,-1.5,-0.8,-0.3,-0.1,0,-0.1,-0.1,-...  \n",
       "1      0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0....  \n",
       "2      1.2,0.6,1.2,0.9,1.2,1.3,1.1,0.7,0.9,-0.1,1.1,0...  \n",
       "3      0.9,0.7,0.8,-0.2,0.5,1,0.6,0,0.4,0.1,0.1,0.9,0...  \n",
       "4      1,0.9,0.7,0.9,0.9,0.9,0.1,0.8,0.7,0.5,0.3,0.5,...  \n",
       "...                                                  ...  \n",
       "18174  1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1.2,1....  \n",
       "18175  0.3,0.8,1,0.7,1.1,1.1,-0.1,0.4,1.2,1.1,-0.1,0....  \n",
       "18176  1.2,0.6,-0.2,-0.7,-0.3,-0.7,0.2,-0.4,-0.7,-0.8...  \n",
       "18177  0.3,-0.4,0.3,-0.2,0.8,-0.1,1.1,0.1,0.3,0.1,0.2...  \n",
       "18178  0.5,1.1,-0.5,-0.9,-0.9,0.4,1,0.3,0.4,1,0.1,0.8...  \n",
       "\n",
       "[18179 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conservation\n",
    "phylop = pd.read_csv(\"../data/proteins_phylop_perresidue.tsv.gz\", sep=\"\\t\")\n",
    "phylop_features = ['phyloP100way_vertebrate', 'phyloP30way_mammalian']\n",
    "phylop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800dbcb-3bd4-4be6-8e42-075fd6c84d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read protein (graph) features\n",
    "archs_df = pd.read_csv(\"../data/archs_protein_embeds.tsv.gz\", sep=\"\\t\")\n",
    "go_df = pd.read_csv(\"../data/go_protein_embeds.tsv.gz\", sep=\"\\t\")\n",
    "string_df = pd.read_csv(\"../data/string_protein_embeds.tsv.gz\", sep=\"\\t\")\n",
    "gnomad_df = pd.read_csv(\"../data/gnomadv4_constraints.tsv.gz\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648d4e3-c122-4508-b7c2-480e6d850675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# creat PyG objects\n",
    "all_pyg_graphs = []\n",
    "all_graphein_features = [\"amino_acid_one_hot\", \"expasy\", 'hbond_acceptors', 'hbond_donors', 'meiler', 'sidechain_vector', 'c_beta_vector', 'sequence_neighbour_vector_n_to_c']  # AAonehot 20, expasy = 61, hbond_acceptors=1, hbond_donors=1, meiler=7, sidechain_vector=3, c_beta_vector=3, sequence_neighbour_vector_n_to_c=3\n",
    "#all_g2p_features = [\"Accessible surface area (Å²)*\", \"Phi angle (degrees)*\", \"Psi angle (degrees)*\", \"ss_B\", \"ss_C\", \"ss_H\"]\n",
    "\n",
    "counter = 1\n",
    "for uniprot_id, hgnc, label in zip(all_uniprot_ids, all_hgnc, all_labels):\n",
    "\n",
    "    # remove those that are not in the describe_prot database\n",
    "    if uniprot_id not in describe_prot[\"UniprotEntry\"].values:\n",
    "        continue\n",
    "        \n",
    "    # construct a networkx graph from AlphaFold predictions\n",
    "    g = construct_graph(config=config, path=(af_db_path + prefix + uniprot_id + suffix))\n",
    "\n",
    "    add_sidechain_vector(g)\n",
    "    add_beta_carbon_vector(g)\n",
    "    add_sequence_neighbour_vector(g)\n",
    "    \n",
    "    # convert to PyG object\n",
    "    g2 = from_networkx(g)\n",
    "\n",
    "    '''\n",
    "    # ignore proteins with problematic length\n",
    "    if len(g2.residue_name) != describe_prot[describe_prot[\"UniprotEntry\"] == uniprot_id][\"seqlength\"].values[0]:\n",
    "        continue\n",
    "\n",
    "    if len(g2.residue_name) != uniprot_clean[uniprot_clean[\"Entry\"] == uniprot_id][\"Length\"].values[0]:\n",
    "        continue\n",
    "    '''\n",
    "    \n",
    "    # add graphein features\n",
    "    g2.x = g2[all_graphein_features[0]] \n",
    "    for feature in all_graphein_features[1:]:\n",
    "        g2.x = torch.cat((g2.x, g2[feature]), dim=1)\n",
    "\n",
    "    # add describe_prot features\n",
    "    for feature in describe_prot_featnames:\n",
    "        try:\n",
    "            temp_feature = describe_prot[describe_prot[\"UniprotEntry\"] == uniprot_id][feature].values[0]\n",
    "            temp_num_list = [float(num) for num in temp_feature.split(',')]\n",
    "            temp_num_tensor = torch.tensor(temp_num_list).reshape(-1, 1)\n",
    "            g2.x = torch.cat((g2.x, temp_num_tensor), dim=1)\n",
    "        except:\n",
    "            g2.x = torch.cat((g2.x, torch.full((len(g2.residue_name), 1), float('nan'))), dim=1)\n",
    "\n",
    "    # add uniprot features\n",
    "    for feature in uniprot_features:\n",
    "        try:\n",
    "            temp_feature = uniprot_clean[uniprot_clean[\"Entry\"] == uniprot_id][feature].values[0]\n",
    "            temp_num_list = [int(char) for char in temp_feature]\n",
    "            temp_num_tensor = torch.tensor(temp_num_list).reshape(-1, 1)\n",
    "            g2.x = torch.cat((g2.x, temp_num_tensor), dim=1)\n",
    "        except:\n",
    "            g2.x = torch.cat((g2.x, torch.full((len(g2.residue_name), 1), float('nan'))), dim=1)\n",
    "\n",
    "    # add phylop features\n",
    "    for feature in phylop_features:\n",
    "        try:\n",
    "            temp_feature = phylop[phylop[\"Entry\"] == uniprot_id][feature].values[0]\n",
    "            temp_num_list = [float(num) for num in temp_feature.split(',')]\n",
    "            temp_num_tensor = torch.tensor(temp_num_list).reshape(-1, 1)\n",
    "            g2.x = torch.cat((g2.x, temp_num_tensor), dim=1)\n",
    "        except:\n",
    "            g2.x = torch.cat((g2.x, torch.full((len(g2.residue_name), 1), float('nan'))), dim=1)\n",
    "    \n",
    "    # add protT5 embeddings\n",
    "    with h5py.File('../data/protT5_per_residue.h5', 'r') as f:\n",
    "        embeds = f[uniprot_id][:]  \n",
    "\n",
    "    '''\n",
    "    if embeds.shape[0] != g2.x.shape[0]:\n",
    "        continue\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        g2.x = torch.cat((g2.x, torch.from_numpy(embeds)), dim=1)\n",
    "    except:\n",
    "        g2.x = torch.cat((g2.x, torch.full((len(g2.residue_name), 1024), float('nan'))), dim=1)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # add G2P features\n",
    "    g2p_features = g2papi.get_protein_features(geneName=hgnc, uniprotId=uniprot_id)\n",
    "    g2p_features['ss_B'] = g2p_features['Secondary structure (DSSP 3-state)*'].apply(lambda x: 1 if x == 'B (strand)' else 0)\n",
    "    g2p_features['ss_C'] = g2p_features['Secondary structure (DSSP 3-state)*'].apply(lambda x: 1 if x == 'C (loop/coil)' else 0)\n",
    "    g2p_features['ss_H'] = g2p_features['Secondary structure (DSSP 3-state)*'].apply(lambda x: 1 if x == 'H (helix)' else 0)\n",
    "    g2p_features2 = g2p_features[all_g2p_features]\n",
    "    g2.x = torch.cat((g2.x, torch.tensor(g2p_features2.values, dtype=torch.float32)), dim=1) \n",
    "    '''\n",
    "\n",
    "    # add protein (graph) features\n",
    "    try:\n",
    "        archs_embed = torch.tensor(archs_df[archs_df[\"uniprot_ids\"] == uniprot_id].values[0][1:].tolist())\n",
    "    except:\n",
    "        archs_embed = torch.full((256,), float('nan'))\n",
    "\n",
    "    try:\n",
    "        go_embed = torch.tensor(go_df[go_df[\"uniprot_ids\"] == uniprot_id].values[0][1:].tolist())\n",
    "    except:\n",
    "        go_embed = torch.full((256,), float('nan'))\n",
    "\n",
    "    try:\n",
    "        string_embed = torch.tensor(string_df[string_df[\"uniprot_ids\"] == uniprot_id].values[0][1:].tolist())\n",
    "    except:\n",
    "        string_embed = torch.full((256,), float('nan'))\n",
    "\n",
    "    try:\n",
    "        gnomad_constraints = torch.tensor(gnomad_df[gnomad_df[\"uniprot_ids\"] == uniprot_id].values[0][1:].tolist())\n",
    "    except:\n",
    "        gnomad_constraints = torch.full((6,), float('nan'))\n",
    "\n",
    "    g2.u = torch.tensor([])\n",
    "    g2.u = torch.cat((g2.u, archs_embed, go_embed, string_embed, gnomad_constraints))\n",
    "\n",
    "    # add label\n",
    "    g2.y = label\n",
    "\n",
    "    all_pyg_graphs.append(g2)\n",
    "    print(f\"finished {counter} proteins\")\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70287c76-9fdf-41c8-af4c-59122f4971eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple version of all pyg graphs\n",
    "\n",
    "all_pyg_graphs_simple = []\n",
    "\n",
    "for pyg in all_pyg_graphs:\n",
    "    \n",
    "    simple_pyg = Data(\n",
    "        x=pyg.x,               # Node features\n",
    "        y=pyg.y,               # Labels\n",
    "        batch=pyg.batch,       # Batch information\n",
    "        edge_index=pyg.edge_index,  # Edge connections\n",
    "        name = pyg.name,\n",
    "        u=pyg.u,\n",
    "        coords=pyg.coords\n",
    "    )\n",
    "\n",
    "    all_pyg_graphs_simple.append(simple_pyg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56eb0b-8775-4339-8c61-3106ee987764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_pyg_graphs and all_pyg_graphs_simple\n",
    "'''\n",
    "with open('../res/pyg_graphs/all_pyg_graphs_simple.pkl', 'wb') as f:\n",
    "    pickle.dump(all_pyg_graphs_simple, f)\n",
    "\n",
    "with open('../res/pyg_graphs/all_pyg_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_pyg_graphs, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d445cc5d-b6e3-47c6-ac5f-752d24841ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all_pyg_graphs_simple\n",
    "\n",
    "with open('../res/pyg_graphs/all_pyg_graphs_simple.pkl', 'rb') as file:\n",
    "    all_pyg_graphs_simple = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec228d9-1410-4b64-aa3b-99204ac532f1",
   "metadata": {},
   "source": [
    "### Train test split + Normalization + Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724d5ad6-6472-4841-8997-ac6e65b10224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze graph freatures from 1*1*L to L\n",
    "for graph in all_pyg_graphs_simple:\n",
    "    graph.u = torch.squeeze(graph.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92296dcc-8137-4fa5-8c21-c640a3d92145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def stratified_split(graphs, train_size=0.8, val_size=0.1, test_size=0.1, random_state=42):\n",
    "    # Extract the labels (y) as tuples for stratification\n",
    "    labels = [tuple(graph.y) for graph in graphs]\n",
    "\n",
    "    # Split the data into training and temp (to split further into val and test)\n",
    "    train_graphs, temp_graphs, train_labels, temp_labels = train_test_split(\n",
    "        graphs, labels, test_size=(1 - train_size), stratify=labels, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Determine the proportion of the temp set to split into validation and test\n",
    "    val_test_ratio = val_size / (val_size + test_size)\n",
    "\n",
    "    # Split the remaining data (temp) into validation and test sets\n",
    "    val_graphs, test_graphs, val_labels, test_labels = train_test_split(\n",
    "        temp_graphs, temp_labels, test_size=(1 - val_test_ratio), stratify=temp_labels, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_graphs, val_graphs, test_graphs\n",
    "\n",
    "# Example usage:\n",
    "train_list, val_list, test_list = stratified_split(all_pyg_graphs_simple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd42e1e0-b47f-4d10-b139-ac57c1732c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      " {(0, 0, 1): 200, (0, 1, 0): 297, (1, 0, 1): 92, (1, 0, 0): 198, (1, 1, 1): 63, (1, 1, 0): 90, (0, 1, 1): 71}\n",
      "\n",
      "test:\n",
      " {(1, 1, 1): 7, (0, 1, 0): 37, (0, 0, 1): 25, (1, 0, 0): 25, (1, 1, 0): 12, (1, 0, 1): 12, (0, 1, 1): 9}\n",
      "\n",
      "val:\n",
      " {(1, 1, 0): 11, (1, 1, 1): 8, (1, 0, 0): 25, (0, 0, 1): 25, (0, 1, 0): 37, (1, 0, 1): 11, (0, 1, 1): 9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_label_occurrences(graphs):\n",
    "    # Create a dictionary to store the count of each unique label tuple\n",
    "    label_count = defaultdict(int)\n",
    "    \n",
    "    # Iterate over the list of graphs\n",
    "    for graph in graphs:\n",
    "        # Convert the label list to a tuple (as tuples are hashable and can be used as dict keys)\n",
    "        label_tuple = tuple(graph.y)\n",
    "        # Increment the count of this label\n",
    "        label_count[label_tuple] += 1\n",
    "    \n",
    "    return dict(label_count)\n",
    "\n",
    "print(f\"train:\\n {count_label_occurrences(train_list)}\\n\")\n",
    "print(f\"test:\\n {count_label_occurrences(test_list)}\\n\")\n",
    "print(f\"val:\\n {count_label_occurrences(val_list)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e71839-0b5c-47f4-8c04-4aca35642f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_and_normalize(graph_list):\n",
    "    for graph in graph_list:\n",
    "        # Get node features (x)\n",
    "        x = graph.x\n",
    "\n",
    "        # Step 1: Imputation (replace NaNs in non-one-hot columns using median)\n",
    "        col_median = torch.nanmedian(x, dim=0).values  # Calculate column median, ignoring NaNs\n",
    "\n",
    "        # Identify one-hot encoded columns (assuming values are exactly 0 or 1)\n",
    "        onehot_mask = torch.all((x == 0) | (x == 1), dim=0)\n",
    "\n",
    "        # Impute only non-one-hot columns\n",
    "        non_onehot_mask = ~onehot_mask\n",
    "        x[:, non_onehot_mask] = torch.where(torch.isnan(x[:, non_onehot_mask]), col_median[non_onehot_mask], x[:, non_onehot_mask])\n",
    "\n",
    "        # Step 2: Normalization (only normalize non-one-hot columns)\n",
    "        max_values = torch.max(x[:, non_onehot_mask], dim=0).values  # Find max of each non-one-hot column\n",
    "        max_values[max_values == 0] = 1  # Avoid division by zero\n",
    "        x[:, non_onehot_mask] = x[:, non_onehot_mask] / max_values  # Normalize non-one-hot columns\n",
    "\n",
    "        # Assign the updated node features back to the graph\n",
    "        graph.x = x\n",
    "    \n",
    "    return graph_list\n",
    "\n",
    "\n",
    "train_list2= impute_and_normalize(train_list)\n",
    "val_list2 = impute_and_normalize(val_list)\n",
    "test_list2 = impute_and_normalize(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd36061-7423-40e6-be55-a86fc008a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "def stratified_split(dataset, train_ratio=0.9, val_ratio=0.05, test_ratio=0.05):\n",
    "\n",
    "    # Calculate label frequencies\n",
    "    label_counts = {}\n",
    "    for data in dataset:\n",
    "        label = tuple(data.y)\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "    # Determine split sizes\n",
    "    train_size = int(len(dataset) * train_ratio)\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    # Create stratified splits\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    for label, count in label_counts.items():\n",
    "        indices = torch.where(dataset.data.y == label)[0]\n",
    "        torch.manual_seed(42)  # Set a seed for reproducibility\n",
    "        indices = indices[torch.randperm(count)]\n",
    "        train_idx.extend(indices[:train_size * count // len(dataset)])\n",
    "        val_idx.extend(indices[train_size * count // len(dataset):train_size * count // len(dataset) + val_size * count // len(dataset)])\n",
    "        test_idx.extend(indices[train_size * count // len(dataset) + val_size * count // len(dataset):])\n",
    "\n",
    "    return dataset[train_idx], dataset[val_idx], dataset[test_idx]\n",
    "\n",
    "train_list, val_list, test_list = stratified_split(all_pyg_graphs_simple)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e414247-a497-4d42-a55a-28853f224ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# normalize\\ntrain_list_norm = min_max_normalize_features_global(train_list_impute, global_min_node, global_max_node, global_min_graph, global_max_graph)\\ntest_list_norm = min_max_normalize_features_global(test_list_impute, global_min_node, global_max_node, global_min_graph, global_max_graph)\\nval_list_norm = min_max_normalize_features_global(val_list_impute, global_min_node, global_max_node, global_min_graph, global_max_graph)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_global_min_max_median(graphs):\n",
    "    ### Node features\n",
    "    # Concatenate features of all graphs along the node dimension\n",
    "    all_features = torch.cat([data.x for data in graphs], dim=0)\n",
    "    \n",
    "    # Mask NaN values by creating a boolean mask\n",
    "    nan_mask = torch.isnan(all_features)\n",
    "    \n",
    "    # Replace NaN values with a very large number for min and very small for max\n",
    "    all_features_min = all_features.clone()\n",
    "    all_features_max = all_features.clone()\n",
    "    \n",
    "    all_features_min[nan_mask] = float('inf')  # Use infinity for min\n",
    "    all_features_max[nan_mask] = -float('inf')  # Use negative infinity for max\n",
    "    \n",
    "    # Compute global min, max and median ignoring NaNs\n",
    "    global_min_node = torch.min(all_features_min, dim=0, keepdim=True)[0]\n",
    "    global_max_node = torch.max(all_features_max, dim=0, keepdim=True)[0]\n",
    "    \n",
    "    # For median, use nan-to-num to replace NaNs with a large number temporarily, then calculate the median\n",
    "    all_features_median = torch.nan_to_num(all_features, nan=0)\n",
    "    global_median_node = torch.median(all_features_median, dim=0, keepdim=True)[0]\n",
    "\n",
    "    ### graph features\n",
    "    # Concatenate features of all graphs along the node dimension\n",
    "    all_features = torch.stack([torch.squeeze(data.u) for data in graphs])\n",
    "    \n",
    "    # Mask NaN values by creating a boolean mask\n",
    "    nan_mask = torch.isnan(all_features)\n",
    "    \n",
    "    # Replace NaN values with a very large number for min and very small for max\n",
    "    all_features_min = all_features.clone()\n",
    "    all_features_max = all_features.clone()\n",
    "    \n",
    "    all_features_min[nan_mask] = float('inf')  # Use infinity for min\n",
    "    all_features_max[nan_mask] = -float('inf')  # Use negative infinity for max\n",
    "    \n",
    "    # Compute global min, max and median ignoring NaNs\n",
    "    global_min_graph = torch.min(all_features_min, dim=0, keepdim=True)[0]\n",
    "    global_max_graph = torch.max(all_features_max, dim=0, keepdim=True)[0]\n",
    "    \n",
    "    # For median, use nan-to-num to replace NaNs with a large number temporarily, then calculate the median\n",
    "    all_features_median = torch.nan_to_num(all_features, nan=0)\n",
    "    global_median_graph = torch.median(all_features_median, dim=0, keepdim=True)[0]\n",
    "    return global_min_node, global_max_node, global_median_node, global_min_graph, global_max_graph, global_median_graph \n",
    "\n",
    "\n",
    "def replace_nan_with_median(graphs, global_median_node, global_mdeian_graph):\n",
    "    # Replace NaN values with the median for each graph\n",
    "    for data in graphs:\n",
    "        # Replace NaN values in node feature matrix with the global median\n",
    "        data.x = torch.where(torch.isnan(data.x), global_median_node, data.x)\n",
    "\n",
    "        data.u = torch.where(torch.isnan(data.u), global_median_graph, data.u)\n",
    "        \n",
    "    return graphs\n",
    "\n",
    "\n",
    "def min_max_normalize_features_global(graphs, global_min_node, global_max_node, global_min_graph, global_max_graph):\n",
    "\n",
    "    global_range_node = global_max_node - global_min_node + 1e-9  # To avoid division by zero\n",
    "    global_range_graph = global_max_graph - global_min_graph + 1e-9  # To avoid division by zero\n",
    "    \n",
    "    # Normalize each graph's features using the global min and max\n",
    "    for data in graphs:\n",
    "        data.x = (data.x - global_min_node) / global_range_node\n",
    "        data.u = (data.u - global_min_graph) / global_range_graph\n",
    "        \n",
    "    return graphs\n",
    "\n",
    "# Compute global min and max based on train list\n",
    "global_min_node, global_max_node, global_median_node, global_min_graph, global_max_graph, global_median_graph = compute_global_min_max_median(train_list2)\n",
    "\n",
    "# replace NaN\n",
    "train_list_norm = replace_nan_with_median(train_list2, global_median_node, global_median_graph)\n",
    "test_list_norm = replace_nan_with_median(test_list2, global_median_node, global_median_graph)\n",
    "val_list_norm = replace_nan_with_median(val_list2, global_median_node, global_median_graph)\n",
    "\n",
    "'''\n",
    "# normalize\n",
    "train_list_norm = min_max_normalize_features_global(train_list_impute, global_min_node, global_max_node, global_min_graph, global_max_graph)\n",
    "test_list_norm = min_max_normalize_features_global(test_list_impute, global_min_node, global_max_node, global_min_graph, global_max_graph)\n",
    "val_list_norm = min_max_normalize_features_global(val_list_impute, global_min_node, global_max_node, global_min_graph, global_max_graph)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8062a-c33e-4e4b-8ce0-7e87e81c3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_global_mean_std_median(graphs):\n",
    "    ### Node features\n",
    "    # Concatenate features of all graphs along the node dimension\n",
    "    all_features = torch.cat([data.x for data in graphs], dim=0)\n",
    "    \n",
    "    # Mask NaN values by creating a boolean mask\n",
    "    nan_mask = torch.isnan(all_features)\n",
    "    \n",
    "    # Replace NaN values with zeros temporarily for mean and std computation\n",
    "    all_features_for_mean_std = torch.nan_to_num(all_features, nan=0)\n",
    "    \n",
    "    # Compute the global mean and standard deviation ignoring NaNs\n",
    "    global_mean_node = torch.sum(all_features_for_mean_std, dim=0, keepdim=True) / (~nan_mask).sum(dim=0, keepdim=True).float()\n",
    "    global_std_node = torch.sqrt(torch.sum((all_features_for_mean_std - global_mean_node)**2, dim=0, keepdim=True) / (~nan_mask).sum(dim=0, keepdim=True).float())\n",
    "    \n",
    "    # For median, use nan-to-num to replace NaNs with a large number temporarily, then calculate the median\n",
    "    global_median_node = torch.median(torch.nan_to_num(all_features, nan=0), dim=0, keepdim=True)[0]\n",
    "\n",
    "    ### Graph features (data.u)\n",
    "    # Stack all graph-level features (u)\n",
    "    all_graph_features = torch.stack([torch.squeeze(data.u) for data in graphs])\n",
    "    \n",
    "    # Mask NaN values by creating a boolean mask\n",
    "    nan_mask_graph = torch.isnan(all_graph_features)\n",
    "    \n",
    "    # Replace NaN values with zeros temporarily for mean and std computation\n",
    "    all_graph_features_for_mean_std = torch.nan_to_num(all_graph_features, nan=0)\n",
    "    \n",
    "    # Compute the global mean and standard deviation ignoring NaNs\n",
    "    global_mean_graph = torch.sum(all_graph_features_for_mean_std, dim=0, keepdim=True) / (~nan_mask_graph).sum(dim=0, keepdim=True).float()\n",
    "    global_std_graph = torch.sqrt(torch.sum((all_graph_features_for_mean_std - global_mean_graph)**2, dim=0, keepdim=True) / (~nan_mask_graph).sum(dim=0, keepdim=True).float())\n",
    "    \n",
    "    # For median, calculate it similarly\n",
    "    global_median_graph = torch.median(torch.nan_to_num(all_graph_features, nan=0), dim=0, keepdim=True)[0]\n",
    "    \n",
    "    return global_mean_node, global_std_node, global_median_node, global_mean_graph, global_std_graph, global_median_graph\n",
    "\n",
    "\n",
    "def zscore_normalize_features_global(graphs, global_mean_node, global_std_node, global_mean_graph, global_std_graph):\n",
    "    \n",
    "    # Normalize each graph's node and graph features using the global mean and std\n",
    "    for data in graphs:\n",
    "        data.x = (data.x - global_mean_node) / (global_std_node + 1e-9)  # Add epsilon to avoid division by zero\n",
    "        data.u = (data.u - global_mean_graph) / (global_std_graph + 1e-9)\n",
    "        \n",
    "    return graphs\n",
    "\n",
    "\n",
    "# Compute global mean, std, and median based on the train list\n",
    "global_mean_node, global_std_node, global_median_node, global_mean_graph, global_std_graph, global_median_graph = compute_global_mean_std_median(train_list2)\n",
    "\n",
    "# Replace NaN values with median (same as before)\n",
    "train_list_norm = replace_nan_with_median(train_list2, global_median_node, global_median_graph)\n",
    "test_list_norm = replace_nan_with_median(test_list2, global_median_node, global_median_graph)\n",
    "val_list_norm = replace_nan_with_median(val_list2, global_median_node, global_median_graph)\n",
    "\n",
    "\n",
    "# Normalize using Z-score normalization\n",
    "train_list_norm = zscore_normalize_features_global(train_list_norm, global_mean_node, global_std_node, global_mean_graph, global_std_graph)\n",
    "test_list_norm = zscore_normalize_features_global(test_list_norm, global_mean_node, global_std_node, global_mean_graph, global_std_graph)\n",
    "val_list_norm = zscore_normalize_features_global(val_list_norm, global_mean_node, global_std_node, global_mean_graph, global_std_graph)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a12ede-f032-4d3a-a6d9-f5cf8c68a344",
   "metadata": {},
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbab53df-9e0f-4237-9dc3-42bf49462bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/14/24 15:31:01] </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Deprotonating protein. This removes H atoms from the pdb_df dataframe    <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">graphs.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py#188\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/14/24 15:31:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Deprotonating protein. This removes H atoms from the pdb_df dataframe    \u001b]8;id=988319;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py\u001b\\\u001b[2mgraphs.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=984894;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py#188\u001b\\\u001b[2m188\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Detected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">968</span> total nodes                                                 <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">graphs.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py#435\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Detected \u001b[1;36m968\u001b[0m total nodes                                                 \u001b]8;id=88015;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py\u001b\\\u001b[2mgraphs.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=103202;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/graphs.py#435\u001b\\\u001b[2m435\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Reading Expasy protein scales from:                                   <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">amino_acid.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py#40\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/graphein/protein/features/nodes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">amino_acid_properties.csv</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Reading Expasy protein scales from:                                   \u001b]8;id=16609;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py\u001b\\\u001b[2mamino_acid.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=362900;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py#40\u001b\\\u001b[2m40\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages\u001b[0m \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/graphein/protein/features/nodes/\u001b[0m\u001b[95mamino_acid_properties.csv\u001b[0m            \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span> aromatic-aromatic interactions                               <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">distance.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#467\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">467</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found: \u001b[1;36m84\u001b[0m aromatic-aromatic interactions                               \u001b]8;id=510395;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\u001b\\\u001b[2mdistance.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=923240;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#467\u001b\\\u001b[2m467\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">404</span> hbond interactions.                                         <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">distance.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1344</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found \u001b[1;36m404\u001b[0m hbond interactions.                                         \u001b]8;id=488530;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\u001b\\\u001b[2mdistance.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=71900;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\u001b\\\u001b[2m1344\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41</span> hbond interactions.                                          <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">distance.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1344</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found \u001b[1;36m41\u001b[0m hbond interactions.                                          \u001b]8;id=607128;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\u001b\\\u001b[2mdistance.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=48198;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\u001b\\\u001b[2m1344\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> disulfide interactions.                                      <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">distance.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1344</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found \u001b[1;36m13\u001b[0m disulfide interactions.                                      \u001b]8;id=675947;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\u001b\\\u001b[2mdistance.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=317119;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\u001b\\\u001b[2m1344\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7862</span> ionic interactions.                                        <a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">distance.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1344</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found \u001b[1;36m7862\u001b[0m ionic interactions.                                        \u001b]8;id=261254;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py\u001b\\\u001b[2mdistance.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=687322;file:///work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/graphein/protein/edges/distance.py#1344\u001b\\\u001b[2m1344\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['aa0',\n",
       " 'aa1',\n",
       " 'aa2',\n",
       " 'aa3',\n",
       " 'aa4',\n",
       " 'aa5',\n",
       " 'aa6',\n",
       " 'aa7',\n",
       " 'aa8',\n",
       " 'aa9',\n",
       " 'aa10',\n",
       " 'aa11',\n",
       " 'aa12',\n",
       " 'aa13',\n",
       " 'aa14',\n",
       " 'aa15',\n",
       " 'aa16',\n",
       " 'aa17',\n",
       " 'aa18',\n",
       " 'aa19',\n",
       " 'pka_cooh_alpha',\n",
       " 'pka_nh3',\n",
       " 'pka_rgroup',\n",
       " 'isoelectric_points',\n",
       " 'molecularweight',\n",
       " 'numbercodons',\n",
       " 'bulkiness',\n",
       " 'polarityzimmerman',\n",
       " 'polaritygrantham',\n",
       " 'refractivity',\n",
       " 'recognitionfactors',\n",
       " 'hphob_eisenberg',\n",
       " 'hphob_sweet',\n",
       " 'hphob_woods',\n",
       " 'hphob_doolittle',\n",
       " 'hphob_manavalan',\n",
       " 'hphob_leo',\n",
       " 'hphob_black',\n",
       " 'hphob_breese',\n",
       " 'hphob_fauchere',\n",
       " 'hphob_guy',\n",
       " 'hphob_janin',\n",
       " 'hphob_miyazawa',\n",
       " 'hphob_argos',\n",
       " 'hphob_roseman',\n",
       " 'hphob_tanford',\n",
       " 'hphob_wolfenden',\n",
       " 'hphob_welling',\n",
       " 'hphob_wilson',\n",
       " 'hphob_parker',\n",
       " 'hphob_ph3_4',\n",
       " 'hphob_ph7_5',\n",
       " 'hphob_mobility',\n",
       " 'hplchfba',\n",
       " 'hplctfa',\n",
       " 'transmembranetendency',\n",
       " 'hplc2_1',\n",
       " 'hplc7_4',\n",
       " 'buriedresidues',\n",
       " 'accessibleresidues',\n",
       " 'hphob_chothia',\n",
       " 'hphob_rose',\n",
       " 'ratioside',\n",
       " 'averageburied',\n",
       " 'averageflexibility',\n",
       " 'alpha_helixfasman',\n",
       " 'beta_sheetfasman',\n",
       " 'beta_turnfasman',\n",
       " 'alpha_helixroux',\n",
       " 'beta_sheetroux',\n",
       " 'beta_turnroux',\n",
       " 'coilroux',\n",
       " 'alpha_helixlevitt',\n",
       " 'beta_sheetlevitt',\n",
       " 'beta_turnlevitt',\n",
       " 'totalbeta_strand',\n",
       " 'antiparallelbeta_strand',\n",
       " 'parallelbeta_strand',\n",
       " 'a_a_composition',\n",
       " 'a_a_swiss_prot',\n",
       " 'relativemutability',\n",
       " 'hbond_acc',\n",
       " 'hbond_donor',\n",
       " 'meiler0',\n",
       " 'meiler1',\n",
       " 'meiler2',\n",
       " 'meiler3',\n",
       " 'meiler4',\n",
       " 'meiler5',\n",
       " 'meiler6',\n",
       " 'sidechain_vector0',\n",
       " 'sidechain_vector1',\n",
       " 'sidechain_vector2',\n",
       " 'c_beta_vector0',\n",
       " 'c_beta_vector1',\n",
       " 'c_beta_vector2',\n",
       " 'sequence_neighbour_vector_n_to_c0',\n",
       " 'sequence_neighbour_vector_n_to_c1',\n",
       " 'sequence_neighbour_vector_n_to_c2',\n",
       " 'ASAquick_normscore',\n",
       " 'DFLpredScore',\n",
       " 'DRNApredDNAscore',\n",
       " 'DRNApredRNAscore',\n",
       " 'DisoDNAscore',\n",
       " 'DisoPROscore',\n",
       " 'DisoRNAscore',\n",
       " 'MMseq2_conservation_score',\n",
       " 'MoRFchibiScore',\n",
       " 'PSIPRED_helix',\n",
       " 'PSIPRED_strand',\n",
       " 'SCRIBERscore',\n",
       " 'flDPnn_score',\n",
       " 'PTMbinary',\n",
       " 'ACT_SITE',\n",
       " 'BINDING',\n",
       " 'DNA_BIND',\n",
       " 'TOPO_DOM',\n",
       " 'TRANSMEM',\n",
       " 'DISULFID',\n",
       " 'PROPEP',\n",
       " 'SIGNAL',\n",
       " 'TRANSIT',\n",
       " 'STRAND',\n",
       " 'HELIX',\n",
       " 'COILED',\n",
       " 'COMPBIAS',\n",
       " 'DOMAIN',\n",
       " 'REGION',\n",
       " 'REPEAT',\n",
       " 'ZN_FING',\n",
       " 'phyloP100way_vertebrate',\n",
       " 'phyloP30way_mammalian',\n",
       " 'protLM0',\n",
       " 'protLM1',\n",
       " 'protLM2',\n",
       " 'protLM3',\n",
       " 'protLM4',\n",
       " 'protLM5',\n",
       " 'protLM6',\n",
       " 'protLM7',\n",
       " 'protLM8',\n",
       " 'protLM9',\n",
       " 'protLM10',\n",
       " 'protLM11',\n",
       " 'protLM12',\n",
       " 'protLM13',\n",
       " 'protLM14',\n",
       " 'protLM15',\n",
       " 'protLM16',\n",
       " 'protLM17',\n",
       " 'protLM18',\n",
       " 'protLM19',\n",
       " 'protLM20',\n",
       " 'protLM21',\n",
       " 'protLM22',\n",
       " 'protLM23',\n",
       " 'protLM24',\n",
       " 'protLM25',\n",
       " 'protLM26',\n",
       " 'protLM27',\n",
       " 'protLM28',\n",
       " 'protLM29',\n",
       " 'protLM30',\n",
       " 'protLM31',\n",
       " 'protLM32',\n",
       " 'protLM33',\n",
       " 'protLM34',\n",
       " 'protLM35',\n",
       " 'protLM36',\n",
       " 'protLM37',\n",
       " 'protLM38',\n",
       " 'protLM39',\n",
       " 'protLM40',\n",
       " 'protLM41',\n",
       " 'protLM42',\n",
       " 'protLM43',\n",
       " 'protLM44',\n",
       " 'protLM45',\n",
       " 'protLM46',\n",
       " 'protLM47',\n",
       " 'protLM48',\n",
       " 'protLM49',\n",
       " 'protLM50',\n",
       " 'protLM51',\n",
       " 'protLM52',\n",
       " 'protLM53',\n",
       " 'protLM54',\n",
       " 'protLM55',\n",
       " 'protLM56',\n",
       " 'protLM57',\n",
       " 'protLM58',\n",
       " 'protLM59',\n",
       " 'protLM60',\n",
       " 'protLM61',\n",
       " 'protLM62',\n",
       " 'protLM63',\n",
       " 'protLM64',\n",
       " 'protLM65',\n",
       " 'protLM66',\n",
       " 'protLM67',\n",
       " 'protLM68',\n",
       " 'protLM69',\n",
       " 'protLM70',\n",
       " 'protLM71',\n",
       " 'protLM72',\n",
       " 'protLM73',\n",
       " 'protLM74',\n",
       " 'protLM75',\n",
       " 'protLM76',\n",
       " 'protLM77',\n",
       " 'protLM78',\n",
       " 'protLM79',\n",
       " 'protLM80',\n",
       " 'protLM81',\n",
       " 'protLM82',\n",
       " 'protLM83',\n",
       " 'protLM84',\n",
       " 'protLM85',\n",
       " 'protLM86',\n",
       " 'protLM87',\n",
       " 'protLM88',\n",
       " 'protLM89',\n",
       " 'protLM90',\n",
       " 'protLM91',\n",
       " 'protLM92',\n",
       " 'protLM93',\n",
       " 'protLM94',\n",
       " 'protLM95',\n",
       " 'protLM96',\n",
       " 'protLM97',\n",
       " 'protLM98',\n",
       " 'protLM99',\n",
       " 'protLM100',\n",
       " 'protLM101',\n",
       " 'protLM102',\n",
       " 'protLM103',\n",
       " 'protLM104',\n",
       " 'protLM105',\n",
       " 'protLM106',\n",
       " 'protLM107',\n",
       " 'protLM108',\n",
       " 'protLM109',\n",
       " 'protLM110',\n",
       " 'protLM111',\n",
       " 'protLM112',\n",
       " 'protLM113',\n",
       " 'protLM114',\n",
       " 'protLM115',\n",
       " 'protLM116',\n",
       " 'protLM117',\n",
       " 'protLM118',\n",
       " 'protLM119',\n",
       " 'protLM120',\n",
       " 'protLM121',\n",
       " 'protLM122',\n",
       " 'protLM123',\n",
       " 'protLM124',\n",
       " 'protLM125',\n",
       " 'protLM126',\n",
       " 'protLM127',\n",
       " 'protLM128',\n",
       " 'protLM129',\n",
       " 'protLM130',\n",
       " 'protLM131',\n",
       " 'protLM132',\n",
       " 'protLM133',\n",
       " 'protLM134',\n",
       " 'protLM135',\n",
       " 'protLM136',\n",
       " 'protLM137',\n",
       " 'protLM138',\n",
       " 'protLM139',\n",
       " 'protLM140',\n",
       " 'protLM141',\n",
       " 'protLM142',\n",
       " 'protLM143',\n",
       " 'protLM144',\n",
       " 'protLM145',\n",
       " 'protLM146',\n",
       " 'protLM147',\n",
       " 'protLM148',\n",
       " 'protLM149',\n",
       " 'protLM150',\n",
       " 'protLM151',\n",
       " 'protLM152',\n",
       " 'protLM153',\n",
       " 'protLM154',\n",
       " 'protLM155',\n",
       " 'protLM156',\n",
       " 'protLM157',\n",
       " 'protLM158',\n",
       " 'protLM159',\n",
       " 'protLM160',\n",
       " 'protLM161',\n",
       " 'protLM162',\n",
       " 'protLM163',\n",
       " 'protLM164',\n",
       " 'protLM165',\n",
       " 'protLM166',\n",
       " 'protLM167',\n",
       " 'protLM168',\n",
       " 'protLM169',\n",
       " 'protLM170',\n",
       " 'protLM171',\n",
       " 'protLM172',\n",
       " 'protLM173',\n",
       " 'protLM174',\n",
       " 'protLM175',\n",
       " 'protLM176',\n",
       " 'protLM177',\n",
       " 'protLM178',\n",
       " 'protLM179',\n",
       " 'protLM180',\n",
       " 'protLM181',\n",
       " 'protLM182',\n",
       " 'protLM183',\n",
       " 'protLM184',\n",
       " 'protLM185',\n",
       " 'protLM186',\n",
       " 'protLM187',\n",
       " 'protLM188',\n",
       " 'protLM189',\n",
       " 'protLM190',\n",
       " 'protLM191',\n",
       " 'protLM192',\n",
       " 'protLM193',\n",
       " 'protLM194',\n",
       " 'protLM195',\n",
       " 'protLM196',\n",
       " 'protLM197',\n",
       " 'protLM198',\n",
       " 'protLM199',\n",
       " 'protLM200',\n",
       " 'protLM201',\n",
       " 'protLM202',\n",
       " 'protLM203',\n",
       " 'protLM204',\n",
       " 'protLM205',\n",
       " 'protLM206',\n",
       " 'protLM207',\n",
       " 'protLM208',\n",
       " 'protLM209',\n",
       " 'protLM210',\n",
       " 'protLM211',\n",
       " 'protLM212',\n",
       " 'protLM213',\n",
       " 'protLM214',\n",
       " 'protLM215',\n",
       " 'protLM216',\n",
       " 'protLM217',\n",
       " 'protLM218',\n",
       " 'protLM219',\n",
       " 'protLM220',\n",
       " 'protLM221',\n",
       " 'protLM222',\n",
       " 'protLM223',\n",
       " 'protLM224',\n",
       " 'protLM225',\n",
       " 'protLM226',\n",
       " 'protLM227',\n",
       " 'protLM228',\n",
       " 'protLM229',\n",
       " 'protLM230',\n",
       " 'protLM231',\n",
       " 'protLM232',\n",
       " 'protLM233',\n",
       " 'protLM234',\n",
       " 'protLM235',\n",
       " 'protLM236',\n",
       " 'protLM237',\n",
       " 'protLM238',\n",
       " 'protLM239',\n",
       " 'protLM240',\n",
       " 'protLM241',\n",
       " 'protLM242',\n",
       " 'protLM243',\n",
       " 'protLM244',\n",
       " 'protLM245',\n",
       " 'protLM246',\n",
       " 'protLM247',\n",
       " 'protLM248',\n",
       " 'protLM249',\n",
       " 'protLM250',\n",
       " 'protLM251',\n",
       " 'protLM252',\n",
       " 'protLM253',\n",
       " 'protLM254',\n",
       " 'protLM255',\n",
       " 'protLM256',\n",
       " 'protLM257',\n",
       " 'protLM258',\n",
       " 'protLM259',\n",
       " 'protLM260',\n",
       " 'protLM261',\n",
       " 'protLM262',\n",
       " 'protLM263',\n",
       " 'protLM264',\n",
       " 'protLM265',\n",
       " 'protLM266',\n",
       " 'protLM267',\n",
       " 'protLM268',\n",
       " 'protLM269',\n",
       " 'protLM270',\n",
       " 'protLM271',\n",
       " 'protLM272',\n",
       " 'protLM273',\n",
       " 'protLM274',\n",
       " 'protLM275',\n",
       " 'protLM276',\n",
       " 'protLM277',\n",
       " 'protLM278',\n",
       " 'protLM279',\n",
       " 'protLM280',\n",
       " 'protLM281',\n",
       " 'protLM282',\n",
       " 'protLM283',\n",
       " 'protLM284',\n",
       " 'protLM285',\n",
       " 'protLM286',\n",
       " 'protLM287',\n",
       " 'protLM288',\n",
       " 'protLM289',\n",
       " 'protLM290',\n",
       " 'protLM291',\n",
       " 'protLM292',\n",
       " 'protLM293',\n",
       " 'protLM294',\n",
       " 'protLM295',\n",
       " 'protLM296',\n",
       " 'protLM297',\n",
       " 'protLM298',\n",
       " 'protLM299',\n",
       " 'protLM300',\n",
       " 'protLM301',\n",
       " 'protLM302',\n",
       " 'protLM303',\n",
       " 'protLM304',\n",
       " 'protLM305',\n",
       " 'protLM306',\n",
       " 'protLM307',\n",
       " 'protLM308',\n",
       " 'protLM309',\n",
       " 'protLM310',\n",
       " 'protLM311',\n",
       " 'protLM312',\n",
       " 'protLM313',\n",
       " 'protLM314',\n",
       " 'protLM315',\n",
       " 'protLM316',\n",
       " 'protLM317',\n",
       " 'protLM318',\n",
       " 'protLM319',\n",
       " 'protLM320',\n",
       " 'protLM321',\n",
       " 'protLM322',\n",
       " 'protLM323',\n",
       " 'protLM324',\n",
       " 'protLM325',\n",
       " 'protLM326',\n",
       " 'protLM327',\n",
       " 'protLM328',\n",
       " 'protLM329',\n",
       " 'protLM330',\n",
       " 'protLM331',\n",
       " 'protLM332',\n",
       " 'protLM333',\n",
       " 'protLM334',\n",
       " 'protLM335',\n",
       " 'protLM336',\n",
       " 'protLM337',\n",
       " 'protLM338',\n",
       " 'protLM339',\n",
       " 'protLM340',\n",
       " 'protLM341',\n",
       " 'protLM342',\n",
       " 'protLM343',\n",
       " 'protLM344',\n",
       " 'protLM345',\n",
       " 'protLM346',\n",
       " 'protLM347',\n",
       " 'protLM348',\n",
       " 'protLM349',\n",
       " 'protLM350',\n",
       " 'protLM351',\n",
       " 'protLM352',\n",
       " 'protLM353',\n",
       " 'protLM354',\n",
       " 'protLM355',\n",
       " 'protLM356',\n",
       " 'protLM357',\n",
       " 'protLM358',\n",
       " 'protLM359',\n",
       " 'protLM360',\n",
       " 'protLM361',\n",
       " 'protLM362',\n",
       " 'protLM363',\n",
       " 'protLM364',\n",
       " 'protLM365',\n",
       " 'protLM366',\n",
       " 'protLM367',\n",
       " 'protLM368',\n",
       " 'protLM369',\n",
       " 'protLM370',\n",
       " 'protLM371',\n",
       " 'protLM372',\n",
       " 'protLM373',\n",
       " 'protLM374',\n",
       " 'protLM375',\n",
       " 'protLM376',\n",
       " 'protLM377',\n",
       " 'protLM378',\n",
       " 'protLM379',\n",
       " 'protLM380',\n",
       " 'protLM381',\n",
       " 'protLM382',\n",
       " 'protLM383',\n",
       " 'protLM384',\n",
       " 'protLM385',\n",
       " 'protLM386',\n",
       " 'protLM387',\n",
       " 'protLM388',\n",
       " 'protLM389',\n",
       " 'protLM390',\n",
       " 'protLM391',\n",
       " 'protLM392',\n",
       " 'protLM393',\n",
       " 'protLM394',\n",
       " 'protLM395',\n",
       " 'protLM396',\n",
       " 'protLM397',\n",
       " 'protLM398',\n",
       " 'protLM399',\n",
       " 'protLM400',\n",
       " 'protLM401',\n",
       " 'protLM402',\n",
       " 'protLM403',\n",
       " 'protLM404',\n",
       " 'protLM405',\n",
       " 'protLM406',\n",
       " 'protLM407',\n",
       " 'protLM408',\n",
       " 'protLM409',\n",
       " 'protLM410',\n",
       " 'protLM411',\n",
       " 'protLM412',\n",
       " 'protLM413',\n",
       " 'protLM414',\n",
       " 'protLM415',\n",
       " 'protLM416',\n",
       " 'protLM417',\n",
       " 'protLM418',\n",
       " 'protLM419',\n",
       " 'protLM420',\n",
       " 'protLM421',\n",
       " 'protLM422',\n",
       " 'protLM423',\n",
       " 'protLM424',\n",
       " 'protLM425',\n",
       " 'protLM426',\n",
       " 'protLM427',\n",
       " 'protLM428',\n",
       " 'protLM429',\n",
       " 'protLM430',\n",
       " 'protLM431',\n",
       " 'protLM432',\n",
       " 'protLM433',\n",
       " 'protLM434',\n",
       " 'protLM435',\n",
       " 'protLM436',\n",
       " 'protLM437',\n",
       " 'protLM438',\n",
       " 'protLM439',\n",
       " 'protLM440',\n",
       " 'protLM441',\n",
       " 'protLM442',\n",
       " 'protLM443',\n",
       " 'protLM444',\n",
       " 'protLM445',\n",
       " 'protLM446',\n",
       " 'protLM447',\n",
       " 'protLM448',\n",
       " 'protLM449',\n",
       " 'protLM450',\n",
       " 'protLM451',\n",
       " 'protLM452',\n",
       " 'protLM453',\n",
       " 'protLM454',\n",
       " 'protLM455',\n",
       " 'protLM456',\n",
       " 'protLM457',\n",
       " 'protLM458',\n",
       " 'protLM459',\n",
       " 'protLM460',\n",
       " 'protLM461',\n",
       " 'protLM462',\n",
       " 'protLM463',\n",
       " 'protLM464',\n",
       " 'protLM465',\n",
       " 'protLM466',\n",
       " 'protLM467',\n",
       " 'protLM468',\n",
       " 'protLM469',\n",
       " 'protLM470',\n",
       " 'protLM471',\n",
       " 'protLM472',\n",
       " 'protLM473',\n",
       " 'protLM474',\n",
       " 'protLM475',\n",
       " 'protLM476',\n",
       " 'protLM477',\n",
       " 'protLM478',\n",
       " 'protLM479',\n",
       " 'protLM480',\n",
       " 'protLM481',\n",
       " 'protLM482',\n",
       " 'protLM483',\n",
       " 'protLM484',\n",
       " 'protLM485',\n",
       " 'protLM486',\n",
       " 'protLM487',\n",
       " 'protLM488',\n",
       " 'protLM489',\n",
       " 'protLM490',\n",
       " 'protLM491',\n",
       " 'protLM492',\n",
       " 'protLM493',\n",
       " 'protLM494',\n",
       " 'protLM495',\n",
       " 'protLM496',\n",
       " 'protLM497',\n",
       " 'protLM498',\n",
       " 'protLM499',\n",
       " 'protLM500',\n",
       " 'protLM501',\n",
       " 'protLM502',\n",
       " 'protLM503',\n",
       " 'protLM504',\n",
       " 'protLM505',\n",
       " 'protLM506',\n",
       " 'protLM507',\n",
       " 'protLM508',\n",
       " 'protLM509',\n",
       " 'protLM510',\n",
       " 'protLM511',\n",
       " 'protLM512',\n",
       " 'protLM513',\n",
       " 'protLM514',\n",
       " 'protLM515',\n",
       " 'protLM516',\n",
       " 'protLM517',\n",
       " 'protLM518',\n",
       " 'protLM519',\n",
       " 'protLM520',\n",
       " 'protLM521',\n",
       " 'protLM522',\n",
       " 'protLM523',\n",
       " 'protLM524',\n",
       " 'protLM525',\n",
       " 'protLM526',\n",
       " 'protLM527',\n",
       " 'protLM528',\n",
       " 'protLM529',\n",
       " 'protLM530',\n",
       " 'protLM531',\n",
       " 'protLM532',\n",
       " 'protLM533',\n",
       " 'protLM534',\n",
       " 'protLM535',\n",
       " 'protLM536',\n",
       " 'protLM537',\n",
       " 'protLM538',\n",
       " 'protLM539',\n",
       " 'protLM540',\n",
       " 'protLM541',\n",
       " 'protLM542',\n",
       " 'protLM543',\n",
       " 'protLM544',\n",
       " 'protLM545',\n",
       " 'protLM546',\n",
       " 'protLM547',\n",
       " 'protLM548',\n",
       " 'protLM549',\n",
       " 'protLM550',\n",
       " 'protLM551',\n",
       " 'protLM552',\n",
       " 'protLM553',\n",
       " 'protLM554',\n",
       " 'protLM555',\n",
       " 'protLM556',\n",
       " 'protLM557',\n",
       " 'protLM558',\n",
       " 'protLM559',\n",
       " 'protLM560',\n",
       " 'protLM561',\n",
       " 'protLM562',\n",
       " 'protLM563',\n",
       " 'protLM564',\n",
       " 'protLM565',\n",
       " 'protLM566',\n",
       " 'protLM567',\n",
       " 'protLM568',\n",
       " 'protLM569',\n",
       " 'protLM570',\n",
       " 'protLM571',\n",
       " 'protLM572',\n",
       " 'protLM573',\n",
       " 'protLM574',\n",
       " 'protLM575',\n",
       " 'protLM576',\n",
       " 'protLM577',\n",
       " 'protLM578',\n",
       " 'protLM579',\n",
       " 'protLM580',\n",
       " 'protLM581',\n",
       " 'protLM582',\n",
       " 'protLM583',\n",
       " 'protLM584',\n",
       " 'protLM585',\n",
       " 'protLM586',\n",
       " 'protLM587',\n",
       " 'protLM588',\n",
       " 'protLM589',\n",
       " 'protLM590',\n",
       " 'protLM591',\n",
       " 'protLM592',\n",
       " 'protLM593',\n",
       " 'protLM594',\n",
       " 'protLM595',\n",
       " 'protLM596',\n",
       " 'protLM597',\n",
       " 'protLM598',\n",
       " 'protLM599',\n",
       " 'protLM600',\n",
       " 'protLM601',\n",
       " 'protLM602',\n",
       " 'protLM603',\n",
       " 'protLM604',\n",
       " 'protLM605',\n",
       " 'protLM606',\n",
       " 'protLM607',\n",
       " 'protLM608',\n",
       " 'protLM609',\n",
       " 'protLM610',\n",
       " 'protLM611',\n",
       " 'protLM612',\n",
       " 'protLM613',\n",
       " 'protLM614',\n",
       " 'protLM615',\n",
       " 'protLM616',\n",
       " 'protLM617',\n",
       " 'protLM618',\n",
       " 'protLM619',\n",
       " 'protLM620',\n",
       " 'protLM621',\n",
       " 'protLM622',\n",
       " 'protLM623',\n",
       " 'protLM624',\n",
       " 'protLM625',\n",
       " 'protLM626',\n",
       " 'protLM627',\n",
       " 'protLM628',\n",
       " 'protLM629',\n",
       " 'protLM630',\n",
       " 'protLM631',\n",
       " 'protLM632',\n",
       " 'protLM633',\n",
       " 'protLM634',\n",
       " 'protLM635',\n",
       " 'protLM636',\n",
       " 'protLM637',\n",
       " 'protLM638',\n",
       " 'protLM639',\n",
       " 'protLM640',\n",
       " 'protLM641',\n",
       " 'protLM642',\n",
       " 'protLM643',\n",
       " 'protLM644',\n",
       " 'protLM645',\n",
       " 'protLM646',\n",
       " 'protLM647',\n",
       " 'protLM648',\n",
       " 'protLM649',\n",
       " 'protLM650',\n",
       " 'protLM651',\n",
       " 'protLM652',\n",
       " 'protLM653',\n",
       " 'protLM654',\n",
       " 'protLM655',\n",
       " 'protLM656',\n",
       " 'protLM657',\n",
       " 'protLM658',\n",
       " 'protLM659',\n",
       " 'protLM660',\n",
       " 'protLM661',\n",
       " 'protLM662',\n",
       " 'protLM663',\n",
       " 'protLM664',\n",
       " 'protLM665',\n",
       " 'protLM666',\n",
       " 'protLM667',\n",
       " 'protLM668',\n",
       " 'protLM669',\n",
       " 'protLM670',\n",
       " 'protLM671',\n",
       " 'protLM672',\n",
       " 'protLM673',\n",
       " 'protLM674',\n",
       " 'protLM675',\n",
       " 'protLM676',\n",
       " 'protLM677',\n",
       " 'protLM678',\n",
       " 'protLM679',\n",
       " 'protLM680',\n",
       " 'protLM681',\n",
       " 'protLM682',\n",
       " 'protLM683',\n",
       " 'protLM684',\n",
       " 'protLM685',\n",
       " 'protLM686',\n",
       " 'protLM687',\n",
       " 'protLM688',\n",
       " 'protLM689',\n",
       " 'protLM690',\n",
       " 'protLM691',\n",
       " 'protLM692',\n",
       " 'protLM693',\n",
       " 'protLM694',\n",
       " 'protLM695',\n",
       " 'protLM696',\n",
       " 'protLM697',\n",
       " 'protLM698',\n",
       " 'protLM699',\n",
       " 'protLM700',\n",
       " 'protLM701',\n",
       " 'protLM702',\n",
       " 'protLM703',\n",
       " 'protLM704',\n",
       " 'protLM705',\n",
       " 'protLM706',\n",
       " 'protLM707',\n",
       " 'protLM708',\n",
       " 'protLM709',\n",
       " 'protLM710',\n",
       " 'protLM711',\n",
       " 'protLM712',\n",
       " 'protLM713',\n",
       " 'protLM714',\n",
       " 'protLM715',\n",
       " 'protLM716',\n",
       " 'protLM717',\n",
       " 'protLM718',\n",
       " 'protLM719',\n",
       " 'protLM720',\n",
       " 'protLM721',\n",
       " 'protLM722',\n",
       " 'protLM723',\n",
       " 'protLM724',\n",
       " 'protLM725',\n",
       " 'protLM726',\n",
       " 'protLM727',\n",
       " 'protLM728',\n",
       " 'protLM729',\n",
       " 'protLM730',\n",
       " 'protLM731',\n",
       " 'protLM732',\n",
       " 'protLM733',\n",
       " 'protLM734',\n",
       " 'protLM735',\n",
       " 'protLM736',\n",
       " 'protLM737',\n",
       " 'protLM738',\n",
       " 'protLM739',\n",
       " 'protLM740',\n",
       " 'protLM741',\n",
       " 'protLM742',\n",
       " 'protLM743',\n",
       " 'protLM744',\n",
       " 'protLM745',\n",
       " 'protLM746',\n",
       " 'protLM747',\n",
       " 'protLM748',\n",
       " 'protLM749',\n",
       " 'protLM750',\n",
       " 'protLM751',\n",
       " 'protLM752',\n",
       " 'protLM753',\n",
       " 'protLM754',\n",
       " 'protLM755',\n",
       " 'protLM756',\n",
       " 'protLM757',\n",
       " 'protLM758',\n",
       " 'protLM759',\n",
       " 'protLM760',\n",
       " 'protLM761',\n",
       " 'protLM762',\n",
       " 'protLM763',\n",
       " 'protLM764',\n",
       " 'protLM765',\n",
       " 'protLM766',\n",
       " 'protLM767',\n",
       " 'protLM768',\n",
       " 'protLM769',\n",
       " 'protLM770',\n",
       " 'protLM771',\n",
       " 'protLM772',\n",
       " 'protLM773',\n",
       " 'protLM774',\n",
       " 'protLM775',\n",
       " 'protLM776',\n",
       " 'protLM777',\n",
       " 'protLM778',\n",
       " 'protLM779',\n",
       " 'protLM780',\n",
       " 'protLM781',\n",
       " 'protLM782',\n",
       " 'protLM783',\n",
       " 'protLM784',\n",
       " 'protLM785',\n",
       " 'protLM786',\n",
       " 'protLM787',\n",
       " 'protLM788',\n",
       " 'protLM789',\n",
       " 'protLM790',\n",
       " 'protLM791',\n",
       " 'protLM792',\n",
       " 'protLM793',\n",
       " 'protLM794',\n",
       " 'protLM795',\n",
       " 'protLM796',\n",
       " 'protLM797',\n",
       " 'protLM798',\n",
       " 'protLM799',\n",
       " 'protLM800',\n",
       " 'protLM801',\n",
       " 'protLM802',\n",
       " 'protLM803',\n",
       " 'protLM804',\n",
       " 'protLM805',\n",
       " 'protLM806',\n",
       " 'protLM807',\n",
       " 'protLM808',\n",
       " 'protLM809',\n",
       " 'protLM810',\n",
       " 'protLM811',\n",
       " 'protLM812',\n",
       " 'protLM813',\n",
       " 'protLM814',\n",
       " 'protLM815',\n",
       " 'protLM816',\n",
       " 'protLM817',\n",
       " 'protLM818',\n",
       " 'protLM819',\n",
       " 'protLM820',\n",
       " 'protLM821',\n",
       " 'protLM822',\n",
       " 'protLM823',\n",
       " 'protLM824',\n",
       " 'protLM825',\n",
       " 'protLM826',\n",
       " 'protLM827',\n",
       " 'protLM828',\n",
       " 'protLM829',\n",
       " 'protLM830',\n",
       " 'protLM831',\n",
       " 'protLM832',\n",
       " 'protLM833',\n",
       " 'protLM834',\n",
       " 'protLM835',\n",
       " 'protLM836',\n",
       " 'protLM837',\n",
       " 'protLM838',\n",
       " 'protLM839',\n",
       " 'protLM840',\n",
       " 'protLM841',\n",
       " 'protLM842',\n",
       " 'protLM843',\n",
       " 'protLM844',\n",
       " 'protLM845',\n",
       " 'protLM846',\n",
       " 'protLM847',\n",
       " 'protLM848',\n",
       " 'protLM849',\n",
       " 'protLM850',\n",
       " 'protLM851',\n",
       " 'protLM852',\n",
       " 'protLM853',\n",
       " 'protLM854',\n",
       " 'protLM855',\n",
       " 'protLM856',\n",
       " 'protLM857',\n",
       " 'protLM858',\n",
       " 'protLM859',\n",
       " 'protLM860',\n",
       " 'protLM861',\n",
       " 'protLM862',\n",
       " 'protLM863',\n",
       " 'protLM864',\n",
       " 'protLM865',\n",
       " 'protLM866',\n",
       " 'protLM867',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep names for features \n",
    "\n",
    "temp_g = construct_graph(config=config, path=(af_db_path + prefix + \"P49588\" + suffix))\n",
    "\n",
    "# AA onehot 20\n",
    "all_feature_names = [\"aa\"+str(i) for i in range(20)]\n",
    "\n",
    "# expasy 61\n",
    "for n, d in temp_g.nodes(data=True):\n",
    "    all_feature_names += d[\"expasy\"].index.values.tolist()\n",
    "    break\n",
    "\n",
    "# hbond acceptor and donor\n",
    "all_feature_names += [\"hbond_acc\", \"hbond_donor\"]\n",
    "\n",
    "# meiler\n",
    "all_feature_names += [\"meiler\"+str(i) for i in range(7)]\n",
    "\n",
    "# sidechain_vector\n",
    "all_feature_names += [\"sidechain_vector\"+str(i) for i in range(3)]\n",
    "\n",
    "# c_beta_vector\n",
    "all_feature_names += [\"c_beta_vector\"+str(i) for i in range(3)]\n",
    "\n",
    "# sequence_neighbour_vector_n_to_c\n",
    "all_feature_names += [\"sequence_neighbour_vector_n_to_c\"+str(i) for i in range(3)]\n",
    "\n",
    "# describeProt\n",
    "all_feature_names += describe_prot_featnames.tolist()\n",
    "\n",
    "# UniProt\n",
    "all_feature_names += uniprot_features\n",
    "\n",
    "# Phylop\n",
    "all_feature_names += phylop_features\n",
    "\n",
    "# protLM embeddings \n",
    "all_feature_names += [\"protLM\"+str(i) for i in range(1024)]\n",
    "\n",
    "all_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4df8aa0-4144-40d1-b0db-7ef0f6629714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa0',\n",
       " 'aa1',\n",
       " 'aa2',\n",
       " 'aa3',\n",
       " 'aa4',\n",
       " 'aa5',\n",
       " 'aa6',\n",
       " 'aa7',\n",
       " 'aa8',\n",
       " 'aa9',\n",
       " 'aa10',\n",
       " 'aa11',\n",
       " 'aa12',\n",
       " 'aa13',\n",
       " 'aa14',\n",
       " 'aa15',\n",
       " 'aa16',\n",
       " 'aa17',\n",
       " 'aa18',\n",
       " 'aa19',\n",
       " 'ACT_SITE',\n",
       " 'BINDING',\n",
       " 'DNA_BIND',\n",
       " 'TOPO_DOM',\n",
       " 'DISULFID',\n",
       " 'PROPEP',\n",
       " 'SIGNAL',\n",
       " 'TRANSIT',\n",
       " 'STRAND',\n",
       " 'HELIX',\n",
       " 'COILED',\n",
       " 'COMPBIAS',\n",
       " 'DOMAIN',\n",
       " 'REGION',\n",
       " 'REPEAT',\n",
       " 'ZN_FING',\n",
       " 'PSIPRED_helix',\n",
       " 'buriedresidues',\n",
       " 'c_beta_vector0',\n",
       " 'c_beta_vector1',\n",
       " 'c_beta_vector2',\n",
       " 'hbond_acc',\n",
       " 'hphob_argos',\n",
       " 'hphob_welling',\n",
       " 'protLM100',\n",
       " 'protLM1008',\n",
       " 'protLM1009',\n",
       " 'protLM1010',\n",
       " 'protLM1011',\n",
       " 'protLM1015',\n",
       " 'protLM1017',\n",
       " 'protLM1018',\n",
       " 'protLM1020',\n",
       " 'protLM109',\n",
       " 'protLM110',\n",
       " 'protLM117',\n",
       " 'protLM120',\n",
       " 'protLM121',\n",
       " 'protLM126',\n",
       " 'protLM133',\n",
       " 'protLM138',\n",
       " 'protLM141',\n",
       " 'protLM144',\n",
       " 'protLM148',\n",
       " 'protLM15',\n",
       " 'protLM154',\n",
       " 'protLM158',\n",
       " 'protLM159',\n",
       " 'protLM161',\n",
       " 'protLM162',\n",
       " 'protLM164',\n",
       " 'protLM167',\n",
       " 'protLM181',\n",
       " 'protLM182',\n",
       " 'protLM188',\n",
       " 'protLM189',\n",
       " 'protLM196',\n",
       " 'protLM197',\n",
       " 'protLM20',\n",
       " 'protLM200',\n",
       " 'protLM203',\n",
       " 'protLM205',\n",
       " 'protLM210',\n",
       " 'protLM213',\n",
       " 'protLM217',\n",
       " 'protLM219',\n",
       " 'protLM22',\n",
       " 'protLM225',\n",
       " 'protLM23',\n",
       " 'protLM232',\n",
       " 'protLM235',\n",
       " 'protLM242',\n",
       " 'protLM243',\n",
       " 'protLM246',\n",
       " 'protLM247',\n",
       " 'protLM248',\n",
       " 'protLM251',\n",
       " 'protLM260',\n",
       " 'protLM262',\n",
       " 'protLM266',\n",
       " 'protLM27',\n",
       " 'protLM271',\n",
       " 'protLM273',\n",
       " 'protLM274',\n",
       " 'protLM277',\n",
       " 'protLM279',\n",
       " 'protLM282',\n",
       " 'protLM294',\n",
       " 'protLM30',\n",
       " 'protLM312',\n",
       " 'protLM318',\n",
       " 'protLM32',\n",
       " 'protLM321',\n",
       " 'protLM326',\n",
       " 'protLM332',\n",
       " 'protLM333',\n",
       " 'protLM335',\n",
       " 'protLM336',\n",
       " 'protLM338',\n",
       " 'protLM34',\n",
       " 'protLM352',\n",
       " 'protLM353',\n",
       " 'protLM355',\n",
       " 'protLM356',\n",
       " 'protLM357',\n",
       " 'protLM366',\n",
       " 'protLM370',\n",
       " 'protLM372',\n",
       " 'protLM373',\n",
       " 'protLM389',\n",
       " 'protLM391',\n",
       " 'protLM394',\n",
       " 'protLM398',\n",
       " 'protLM399',\n",
       " 'protLM40',\n",
       " 'protLM401',\n",
       " 'protLM402',\n",
       " 'protLM405',\n",
       " 'protLM407',\n",
       " 'protLM41',\n",
       " 'protLM411',\n",
       " 'protLM414',\n",
       " 'protLM421',\n",
       " 'protLM428',\n",
       " 'protLM432',\n",
       " 'protLM436',\n",
       " 'protLM445',\n",
       " 'protLM449',\n",
       " 'protLM45',\n",
       " 'protLM453',\n",
       " 'protLM461',\n",
       " 'protLM470',\n",
       " 'protLM48',\n",
       " 'protLM484',\n",
       " 'protLM487',\n",
       " 'protLM499',\n",
       " 'protLM50',\n",
       " 'protLM500',\n",
       " 'protLM501',\n",
       " 'protLM503',\n",
       " 'protLM504',\n",
       " 'protLM507',\n",
       " 'protLM509',\n",
       " 'protLM511',\n",
       " 'protLM513',\n",
       " 'protLM517',\n",
       " 'protLM519',\n",
       " 'protLM521',\n",
       " 'protLM523',\n",
       " 'protLM525',\n",
       " 'protLM537',\n",
       " 'protLM539',\n",
       " 'protLM543',\n",
       " 'protLM544',\n",
       " 'protLM551',\n",
       " 'protLM556',\n",
       " 'protLM560',\n",
       " 'protLM561',\n",
       " 'protLM568',\n",
       " 'protLM581',\n",
       " 'protLM582',\n",
       " 'protLM583',\n",
       " 'protLM590',\n",
       " 'protLM591',\n",
       " 'protLM599',\n",
       " 'protLM6',\n",
       " 'protLM606',\n",
       " 'protLM612',\n",
       " 'protLM617',\n",
       " 'protLM622',\n",
       " 'protLM63',\n",
       " 'protLM632',\n",
       " 'protLM633',\n",
       " 'protLM635',\n",
       " 'protLM642',\n",
       " 'protLM645',\n",
       " 'protLM646',\n",
       " 'protLM648',\n",
       " 'protLM649',\n",
       " 'protLM654',\n",
       " 'protLM659',\n",
       " 'protLM66',\n",
       " 'protLM661',\n",
       " 'protLM666',\n",
       " 'protLM67',\n",
       " 'protLM674',\n",
       " 'protLM678',\n",
       " 'protLM691',\n",
       " 'protLM694',\n",
       " 'protLM698',\n",
       " 'protLM7',\n",
       " 'protLM702',\n",
       " 'protLM710',\n",
       " 'protLM715',\n",
       " 'protLM72',\n",
       " 'protLM721',\n",
       " 'protLM725',\n",
       " 'protLM732',\n",
       " 'protLM734',\n",
       " 'protLM737',\n",
       " 'protLM74',\n",
       " 'protLM740',\n",
       " 'protLM753',\n",
       " 'protLM758',\n",
       " 'protLM763',\n",
       " 'protLM778',\n",
       " 'protLM779',\n",
       " 'protLM783',\n",
       " 'protLM787',\n",
       " 'protLM79',\n",
       " 'protLM791',\n",
       " 'protLM792',\n",
       " 'protLM798',\n",
       " 'protLM8',\n",
       " 'protLM804',\n",
       " 'protLM805',\n",
       " 'protLM806',\n",
       " 'protLM807',\n",
       " 'protLM816',\n",
       " 'protLM823',\n",
       " 'protLM835',\n",
       " 'protLM839',\n",
       " 'protLM84',\n",
       " 'protLM840',\n",
       " 'protLM846',\n",
       " 'protLM852',\n",
       " 'protLM853',\n",
       " 'protLM857',\n",
       " 'protLM862',\n",
       " 'protLM863',\n",
       " 'protLM865',\n",
       " 'protLM869',\n",
       " 'protLM871',\n",
       " 'protLM872',\n",
       " 'protLM875',\n",
       " 'protLM879',\n",
       " 'protLM883',\n",
       " 'protLM886',\n",
       " 'protLM89',\n",
       " 'protLM896',\n",
       " 'protLM897',\n",
       " 'protLM90',\n",
       " 'protLM902',\n",
       " 'protLM904',\n",
       " 'protLM913',\n",
       " 'protLM916',\n",
       " 'protLM918',\n",
       " 'protLM923',\n",
       " 'protLM939',\n",
       " 'protLM94',\n",
       " 'protLM947',\n",
       " 'protLM948',\n",
       " 'protLM949',\n",
       " 'protLM953',\n",
       " 'protLM955',\n",
       " 'protLM958',\n",
       " 'protLM973',\n",
       " 'protLM974',\n",
       " 'protLM975',\n",
       " 'protLM977',\n",
       " 'protLM978',\n",
       " 'protLM980',\n",
       " 'protLM984',\n",
       " 'protLM986',\n",
       " 'protLM988',\n",
       " 'protLM992',\n",
       " 'protLM995',\n",
       " 'protLM997',\n",
       " 'sequence_neighbour_vector_n_to_c0',\n",
       " 'sequence_neighbour_vector_n_to_c1',\n",
       " 'sequence_neighbour_vector_n_to_c2']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Step 1: Concatenate all graph features\n",
    "# Assuming `all_feature_names` corresponds to columns in the node feature matrix `x`\n",
    "def concatenate_graph_features(graphs):\n",
    "    feature_list = []\n",
    "    for graph in graphs:\n",
    "        feature_list.append(graph.x)\n",
    "    return torch.cat(feature_list, dim=0)\n",
    "\n",
    "# Assuming `graphs` is the list of Data objects\n",
    "concatenated_features = concatenate_graph_features(train_list_norm)\n",
    "\n",
    "# sample 10k amino acids\n",
    "concatenated_features = concatenated_features[torch.randperm(concatenated_features.size(0))[:10_000]]\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "df_features = pd.DataFrame(concatenated_features.numpy(), columns=all_feature_names)\n",
    "\n",
    "# Step 2: Remove features with low variance (except for binary features)\n",
    "# Identify binary features (one-hot encoded)\n",
    "binary_columns = df_features.columns[df_features.nunique() == 2]\n",
    "non_binary_columns = df_features.columns.difference(binary_columns)\n",
    "\n",
    "# Apply variance thresholding on non-binary features\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "filtered_features = selector.fit_transform(df_features[non_binary_columns])\n",
    "\n",
    "# Keep the selected features\n",
    "selected_features = non_binary_columns[selector.get_support()]\n",
    "\n",
    "# Combine back with binary columns that were not filtered\n",
    "df_filtered = pd.concat([df_features[binary_columns], df_features[selected_features]], axis=1)\n",
    "\n",
    "# Step 3: Remove highly correlated features\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_filtered.corr().abs()\n",
    "\n",
    "# Identify highly correlated features (correlation > 0.8)\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Select columns to drop based on the correlation threshold\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "\n",
    "# Drop the highly correlated features\n",
    "df_final = df_filtered.drop(columns=to_drop)\n",
    "\n",
    "# Step 4: Update all_feature_names\n",
    "all_feature_names_filtered = df_final.columns.tolist()\n",
    "\n",
    "all_feature_names_filtered  # Updated feature names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6de2bb55-3e63-4086-b3c0-e1b6040930b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 108,\n",
       " 58,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 81,\n",
       " 43,\n",
       " 47,\n",
       " 232,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1143,\n",
       " 1147,\n",
       " 1149,\n",
       " 1150,\n",
       " 1152,\n",
       " 241,\n",
       " 242,\n",
       " 249,\n",
       " 252,\n",
       " 253,\n",
       " 258,\n",
       " 265,\n",
       " 270,\n",
       " 273,\n",
       " 276,\n",
       " 280,\n",
       " 147,\n",
       " 286,\n",
       " 290,\n",
       " 291,\n",
       " 293,\n",
       " 294,\n",
       " 296,\n",
       " 299,\n",
       " 313,\n",
       " 314,\n",
       " 320,\n",
       " 321,\n",
       " 328,\n",
       " 329,\n",
       " 152,\n",
       " 332,\n",
       " 335,\n",
       " 337,\n",
       " 342,\n",
       " 345,\n",
       " 349,\n",
       " 351,\n",
       " 154,\n",
       " 357,\n",
       " 155,\n",
       " 364,\n",
       " 367,\n",
       " 374,\n",
       " 375,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 383,\n",
       " 392,\n",
       " 394,\n",
       " 398,\n",
       " 159,\n",
       " 403,\n",
       " 405,\n",
       " 406,\n",
       " 409,\n",
       " 411,\n",
       " 414,\n",
       " 426,\n",
       " 162,\n",
       " 444,\n",
       " 450,\n",
       " 164,\n",
       " 453,\n",
       " 458,\n",
       " 464,\n",
       " 465,\n",
       " 467,\n",
       " 468,\n",
       " 470,\n",
       " 166,\n",
       " 484,\n",
       " 485,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 498,\n",
       " 502,\n",
       " 504,\n",
       " 505,\n",
       " 521,\n",
       " 523,\n",
       " 526,\n",
       " 530,\n",
       " 531,\n",
       " 172,\n",
       " 533,\n",
       " 534,\n",
       " 537,\n",
       " 539,\n",
       " 173,\n",
       " 543,\n",
       " 546,\n",
       " 553,\n",
       " 560,\n",
       " 564,\n",
       " 568,\n",
       " 577,\n",
       " 581,\n",
       " 177,\n",
       " 585,\n",
       " 593,\n",
       " 602,\n",
       " 180,\n",
       " 616,\n",
       " 619,\n",
       " 631,\n",
       " 182,\n",
       " 632,\n",
       " 633,\n",
       " 635,\n",
       " 636,\n",
       " 639,\n",
       " 641,\n",
       " 643,\n",
       " 645,\n",
       " 649,\n",
       " 651,\n",
       " 653,\n",
       " 655,\n",
       " 657,\n",
       " 669,\n",
       " 671,\n",
       " 675,\n",
       " 676,\n",
       " 683,\n",
       " 688,\n",
       " 692,\n",
       " 693,\n",
       " 700,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 722,\n",
       " 723,\n",
       " 731,\n",
       " 138,\n",
       " 738,\n",
       " 744,\n",
       " 749,\n",
       " 754,\n",
       " 195,\n",
       " 764,\n",
       " 765,\n",
       " 767,\n",
       " 774,\n",
       " 777,\n",
       " 778,\n",
       " 780,\n",
       " 781,\n",
       " 786,\n",
       " 791,\n",
       " 198,\n",
       " 793,\n",
       " 798,\n",
       " 199,\n",
       " 806,\n",
       " 810,\n",
       " 823,\n",
       " 826,\n",
       " 830,\n",
       " 139,\n",
       " 834,\n",
       " 842,\n",
       " 847,\n",
       " 204,\n",
       " 853,\n",
       " 857,\n",
       " 864,\n",
       " 866,\n",
       " 869,\n",
       " 206,\n",
       " 872,\n",
       " 885,\n",
       " 890,\n",
       " 895,\n",
       " 910,\n",
       " 911,\n",
       " 915,\n",
       " 919,\n",
       " 211,\n",
       " 923,\n",
       " 924,\n",
       " 930,\n",
       " 140,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 948,\n",
       " 955,\n",
       " 967,\n",
       " 971,\n",
       " 216,\n",
       " 972,\n",
       " 978,\n",
       " 984,\n",
       " 985,\n",
       " 989,\n",
       " 994,\n",
       " 995,\n",
       " 997,\n",
       " 1001,\n",
       " 1003,\n",
       " 1004,\n",
       " 1007,\n",
       " 1011,\n",
       " 1015,\n",
       " 1018,\n",
       " 221,\n",
       " 1028,\n",
       " 1029,\n",
       " 222,\n",
       " 1034,\n",
       " 1036,\n",
       " 1045,\n",
       " 1048,\n",
       " 1050,\n",
       " 1055,\n",
       " 1071,\n",
       " 226,\n",
       " 1079,\n",
       " 1080,\n",
       " 1081,\n",
       " 1085,\n",
       " 1087,\n",
       " 1090,\n",
       " 1105,\n",
       " 1106,\n",
       " 1107,\n",
       " 1109,\n",
       " 1110,\n",
       " 1112,\n",
       " 1116,\n",
       " 1118,\n",
       " 1120,\n",
       " 1124,\n",
       " 1127,\n",
       " 1129,\n",
       " 96,\n",
       " 97,\n",
       " 98]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_indexes = [all_feature_names.index(item) for item in all_feature_names_filtered]\n",
    "selected_feat_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce24748d-48f1-49b4-8aad-0ea4a1c51202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_list_norm)):\n",
    "    train_list_norm[i].x = train_list_norm[i].x[:, selected_feat_indexes]\n",
    "\n",
    "for i in range(len(test_list_norm)):\n",
    "    test_list_norm[i].x = test_list_norm[i].x[:, selected_feat_indexes]\n",
    "\n",
    "for i in range(len(val_list_norm)):\n",
    "    val_list_norm[i].x = val_list_norm[i].x[:, selected_feat_indexes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6796195-f637-4db9-8ba6-e446c01585a3",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a00a2291-dcc2-48e2-85d6-04be1a576458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=0.8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=0.8)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=0.8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Concatenate node features from all graphs in train_list\n",
    "all_train_features = torch.cat([graph.x for graph in train_list_norm], dim=0)\n",
    "\n",
    "# Fit PCA on the concatenated features\n",
    "#pca = PCA(n_components=64)  # Specify the number of components\n",
    "pca = PCA(n_components=0.8) \n",
    "pca.fit(all_train_features.numpy())  # Convert to numpy for PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19e795e2-9575-4fe8-8917-098671c3a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Function to apply PCA to the node features of a list of graphs\n",
    "def apply_pca(graph_list, pca_model):\n",
    "    for graph in graph_list:\n",
    "        graph.x = torch.tensor(pca_model.transform(graph.x.numpy()), dtype=torch.float)\n",
    "\n",
    "#make_copy \n",
    "train_list_norm_pca = copy.deepcopy(train_list_norm)\n",
    "val_list_norm_pca = copy.deepcopy(val_list_norm)\n",
    "test_list_norm_pca = copy.deepcopy(test_list_norm)\n",
    "\n",
    "# Apply PCA to train, val, and test lists\n",
    "apply_pca(train_list_norm_pca, pca)\n",
    "apply_pca(val_list_norm_pca, pca)\n",
    "apply_pca(test_list_norm_pca, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c0346-12d0-4f43-b7c1-723d1a42cab0",
   "metadata": {},
   "source": [
    "### Define a GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5030964-6750-4ced-ba19-ff31c65cac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "train_loader_nopca = DataLoader(train_list_norm, batch_size=batch_size, shuffle=True)\n",
    "test_loader_nopca = DataLoader(test_list_norm, batch_size=batch_size, shuffle=True)\n",
    "val_loader_nopca = DataLoader(val_list_norm, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_loader_pca = DataLoader(train_list_norm_pca, batch_size=batch_size, shuffle=True)\n",
    "test_loader_pca = DataLoader(test_list_norm_pca, batch_size=batch_size, shuffle=True)\n",
    "val_loader_pca = DataLoader(val_list_norm_pca, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5f64253-4b80-4dd7-bd54-4ac4c337b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FbetaLoss(nn.Module):\n",
    "    def __init__(self, beta=0.5, epsilon=1e-8, reduction='mean'):\n",
    "        \"\"\"\n",
    "        F-beta loss (for custom balancing precision and recall).\n",
    "        \n",
    "        Args:\n",
    "            beta: Weighting between precision and recall (beta < 1 favors precision).\n",
    "            epsilon: Small constant to avoid division by zero.\n",
    "            reduction: How to reduce the loss ('mean', 'sum', or 'none').\n",
    "        \"\"\"\n",
    "        super(FbetaLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        Forward pass for F-beta loss.\n",
    "        \n",
    "        Args:\n",
    "            logits: Raw output (logits) from the model of shape [batch_size, num_classes].\n",
    "            target: Ground truth binary labels of shape [batch_size, num_classes].\n",
    "        \n",
    "        Returns:\n",
    "            loss: The negative F-beta score (to maximize it).\n",
    "        \"\"\"\n",
    "        # Apply sigmoid to logits to get predicted probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Use soft probabilities instead of hard binary predictions\n",
    "        true_positives = (probs * target).sum(dim=0)\n",
    "        false_positives = ((1 - target) * probs).sum(dim=0)\n",
    "        false_negatives = (target * (1 - probs)).sum(dim=0)\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = true_positives / (true_positives + false_positives + self.epsilon)\n",
    "        \n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = true_positives / (true_positives + false_negatives + self.epsilon)\n",
    "        \n",
    "        # F-beta score: (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)\n",
    "        beta_squared = self.beta ** 2\n",
    "        f_beta = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall + self.epsilon)\n",
    "\n",
    "        # Loss is 1 - F-beta score (we want to maximize F-beta, so minimize 1 - F-beta)\n",
    "        loss = 1 - f_beta\n",
    "\n",
    "        # Reduce the loss over all classes\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif self.reduction == 'product':\n",
    "            return torch.prod(loss)\n",
    "        else:\n",
    "            return loss  # No reduction, return per-class loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "de11cdfd-93c3-469c-a1d9-7274edd4eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MCCLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Custom loss to maximize the Matthews Correlation Coefficient (MCC).\n",
    "        \n",
    "        Args:\n",
    "            epsilon: Small constant to avoid division by zero.\n",
    "            reduction: How to reduce the loss ('mean', 'sum', or 'none').\n",
    "        \"\"\"\n",
    "        super(MCCLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        Forward pass for MCC loss.\n",
    "        \n",
    "        Args:\n",
    "            logits: The raw output (logits) from the model of shape [batch_size, num_classes].\n",
    "            target: Ground truth binary labels of shape [batch_size, num_classes].\n",
    "        \n",
    "        Returns:\n",
    "            loss: Negative MCC (we minimize the negative to maximize MCC).\n",
    "        \"\"\"\n",
    "        # Apply sigmoid to logits to get predicted probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Compute predicted soft labels (use probabilities instead of hard labels)\n",
    "        pred_pos = probs  # Predicted positives (soft)\n",
    "        pred_neg = 1 - probs  # Predicted negatives (soft)\n",
    "\n",
    "        # Compute actual positives and negatives\n",
    "        true_pos = target  # Ground truth positives\n",
    "        true_neg = 1 - target  # Ground truth negatives\n",
    "\n",
    "        # True Positives, False Positives, True Negatives, False Negatives\n",
    "        TP = (pred_pos * true_pos).sum(dim=0)\n",
    "        TN = (pred_neg * true_neg).sum(dim=0)\n",
    "        FP = (pred_pos * true_neg).sum(dim=0)\n",
    "        FN = (pred_neg * true_pos).sum(dim=0)\n",
    "\n",
    "        # MCC numerator: TP * TN - FP * FN\n",
    "        numerator = TP * TN - FP * FN\n",
    "\n",
    "        # MCC denominator: sqrt((TP + FP)(TP + FN)(TN + FP)(TN + FN))\n",
    "        denominator = torch.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) + self.epsilon)\n",
    "\n",
    "        # MCC: numerator / denominator\n",
    "        mcc = numerator / (denominator + self.epsilon)\n",
    "\n",
    "        # Loss is the negative MCC (to maximize it)\n",
    "        loss = 1 - mcc  # Maximize MCC by minimizing (1 - MCC)\n",
    "\n",
    "        # Reduction: 'mean' or 'sum' over all classes\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()  # Mean loss across classes\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()   # Sum loss across classes\n",
    "        elif self.reduction == 'product':\n",
    "            return torch.prod(loss)\n",
    "        else:\n",
    "            return loss  # No reduction, return per-class loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7159888-e3a0-456c-b7b1-2f41ccee6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn import GINConv, GATv2Conv, GCNConv, global_add_pool\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"GNN model for multi-label classification\"\"\"\n",
    "    def __init__(self, arch, dim_in, dim_h, dim_out, dim_graph_feature, n_layer=2, heads=2, use_graph_feat=False, dropout_p=0.5):\n",
    "        super(GNN, self).__init__()\n",
    "        # Store the initialization arguments\n",
    "        self.args = (arch, dim_in, dim_h, dim_out, dim_graph_feature, n_layer, heads, use_graph_feat, dropout_p)\n",
    "        self.use_graph_feat = use_graph_feat\n",
    "        self.dropout_p = dropout_p\n",
    "        self.n_layer = n_layer\n",
    "        self.heads = heads\n",
    "\n",
    "        hidden_dims = [dim_h]  # Initialize with the input hidden dimension\n",
    "        for i in range(1, n_layer):\n",
    "            hidden_dims.append(hidden_dims[-1] // 2)  # Divide by 2 progressively for each layer\n",
    "\n",
    "        if arch == \"GCN\":\n",
    "            self.convs = torch.nn.ModuleList([GCNConv(dim_in if i == 0 else hidden_dims[i-1], hidden_dims[i]) for i in range(n_layer)])\n",
    "\n",
    "        elif arch == \"GAT\":\n",
    "            self.convs = torch.nn.ModuleList([\n",
    "                GATv2Conv(dim_in if i == 0 else hidden_dims[i-1] * heads, hidden_dims[i], heads=heads, concat=True)\n",
    "                for i in range(n_layer)\n",
    "            ])\n",
    "\n",
    "        elif arch == \"GIN\":\n",
    "            self.convs = torch.nn.ModuleList([\n",
    "                GINConv(\n",
    "                    Sequential(Linear(dim_in if i == 0 else hidden_dims[i-1], hidden_dims[i]),\n",
    "                               BatchNorm1d(hidden_dims[i]), ReLU(),\n",
    "                               Linear(hidden_dims[i], hidden_dims[i]), ReLU())\n",
    "                )\n",
    "                for i in range(n_layer)\n",
    "            ])\n",
    "\n",
    "        # Calculate total input dimension for lin1 based on the number of layers and heads (if applicable)\n",
    "        if arch == \"GAT\":\n",
    "            total_dim = sum([dim * heads for dim in hidden_dims])\n",
    "        else:\n",
    "            total_dim = sum(hidden_dims)\n",
    "\n",
    "        if self.use_graph_feat:\n",
    "            total_dim += dim_graph_feature  # Include graph features if applicable\n",
    "\n",
    "        self.lin1 = Linear(total_dim, dim_h * n_layer)  # Adjusted lin1 to match the total dimension\n",
    "        self.lin2 = Linear(dim_h * n_layer, dim_out)    # Output layer for multi-label prediction\n",
    "\n",
    "    def forward(self, x, edge_index, batch, graph_features):\n",
    "        #x, edge_index, batch, graph_features = data.x, data.edge_index, data.batch, data.u\n",
    "\n",
    "        h_list = []\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index)\n",
    "            h = F.dropout(h, p=self.dropout_p, training=self.training)\n",
    "            h_list.append(global_add_pool(h, batch))\n",
    "\n",
    "        # Concatenate graph embeddings from each layer\n",
    "        h = torch.cat(h_list, dim=1)\n",
    "\n",
    "        if self.use_graph_feat:\n",
    "            # Concatenate graph-level features to the pooled graph embeddings\n",
    "            graph_features = graph_features.reshape(h.shape[0], -1)\n",
    "            h = torch.cat((h, graph_features), dim=1)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=self.dropout_p, training=self.training)  # Dropout for regularization\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h  # Returning raw logits for BCEWithLogitsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1071016-2e1a-4609-b456-232c2039d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "def calculate_metrics(logits, target, threshold=0.5):\n",
    "    # Convert logits to probabilities using sigmoid\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    # Convert probabilities to binary predictions (threshold 0.5)\n",
    "    pred = (probs > threshold).float()  # Binary predictions\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    target_np = target.cpu().numpy()\n",
    "\n",
    "    # F1 Score\n",
    "    f1_per_class = f1_score(target_np, pred_np, average=None, zero_division=0) # (Per class)\n",
    "    f1_micro = f1_score(target_np, pred_np, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(target_np, pred_np, average='macro', zero_division=0) \n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision_per_class = precision_score(target_np, pred_np, average=None, zero_division=0)\n",
    "    precision_micro = precision_score(target_np, pred_np, average='micro', zero_division=0)\n",
    "    precision_macro = precision_score(target_np, pred_np, average='macro', zero_division=0) \n",
    "    \n",
    "    recall_per_class = recall_score(target_np, pred_np, average=None, zero_division=0)\n",
    "    recall_micro = recall_score(target_np, pred_np, average='micro', zero_division=0)\n",
    "    recall_macro = recall_score(target_np, pred_np, average='macro', zero_division=0) \n",
    "\n",
    "    # Matthews Correlation Coefficient (MCC) per class\n",
    "    class_mccs = []\n",
    "    for i in range(target_np.shape[1]):  # Iterate over each class (column in multi-label)\n",
    "        class_mcc = matthews_corrcoef(target_np[:, i], pred_np[:, i])\n",
    "        class_mccs.append(class_mcc)\n",
    "        \n",
    "    # MCC for the entire dataset (flattened)\n",
    "    mcc_overall = matthews_corrcoef(target_np.ravel(), pred_np.ravel())\n",
    "    \n",
    "    return {\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'mcc_per_class': class_mccs,\n",
    "        'mcc_overall': mcc_overall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6970cd9-1065-4cfa-870a-a677bd2a7e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2822, 0.9405, 1.3732])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 3  # Number of classes in the multi-label setting\n",
    "\n",
    "# Initialize counters for positive and negative labels\n",
    "positive_counts = torch.zeros(num_classes)\n",
    "negative_counts = torch.zeros(num_classes)\n",
    "\n",
    "# Loop through each graph and count positives and negatives\n",
    "for graph in train_list_norm:\n",
    "    label = torch.tensor(graph.y)  # Assume graph.y is a tensor of shape [3] for the label [1, 1, 0]\n",
    "    \n",
    "    positive_counts += (label == 1).float()  # Count positives (1s)\n",
    "    negative_counts += (label == 0).float()  # Count negatives (0s)\n",
    "\n",
    "# Calculate the pos_weight for each class\n",
    "pos_weight = negative_counts / positive_counts\n",
    "pos_weight = pos_weight.to(torch.float32)\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19764512-187e-4472-aa98-a085f23b3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "input_dim_nopca = train_list_norm[0].x.shape[1]  # Adjust this based on your feature size\n",
    "input_dim_pca = train_list_norm_pca[0].x.shape[1]  # Adjust this based on your feature size\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # Number of labels\n",
    "n_layer = 2\n",
    "graph_features_dim = train_list_norm[0].u.shape[0]\n",
    "\n",
    "# define potential models\n",
    "gcn_pca   = GNN(\"GCN\", input_dim_pca, hidden_dim, output_dim, graph_features_dim, n_layer=n_layer, use_graph_feat=False).to(device)\n",
    "gcn_nopca = GNN(\"GCN\", input_dim_nopca, hidden_dim, output_dim, graph_features_dim, n_layer=n_layer, use_graph_feat=False).to(device)\n",
    "\n",
    "gat_pca   = GNN(\"GAT\", input_dim_pca, hidden_dim, output_dim, graph_features_dim, n_layer=n_layer, heads=1, use_graph_feat=False).to(device)\n",
    "gat_nopca = GNN(\"GAT\", input_dim_nopca, hidden_dim, output_dim, graph_features_dim, n_layer=n_layer, heads=1, use_graph_feat=False).to(device)\n",
    "\n",
    "gin_pca   = GNN(\"GIN\", input_dim_pca, hidden_dim, output_dim, graph_features_dim, n_layer=n_layer, use_graph_feat=False).to(device)\n",
    "gin_nopca = GNN(\"GIN\", input_dim_nopca, hidden_dim, output_dim, graph_features_dim, n_layer=n_layer, use_graph_feat=False).to(device)\n",
    "\n",
    "LR = 5e-4\n",
    "WD = 5e-4\n",
    "# define optimizers\n",
    "opt_gcn_pca = torch.optim.AdamW(gcn_pca.parameters(), lr=LR, weight_decay=WD)\n",
    "opt_gcn_nopca = torch.optim.AdamW(gcn_nopca.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "opt_gat_pca = torch.optim.AdamW(gat_pca.parameters(), lr=LR, weight_decay=WD)\n",
    "opt_gat_nopca = torch.optim.AdamW(gat_nopca.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "opt_gin_pca = torch.optim.AdamW(gin_pca.parameters(), lr=LR, weight_decay=WD)\n",
    "opt_gin_nopca = torch.optim.AdamW(gin_nopca.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "# define schedulers\n",
    "scheduler_gcn_pca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gcn_pca, mode='min', factor=0.1, patience=3)\n",
    "scheduler_gcn_nopca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gcn_nopca, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "scheduler_gat_pca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gat_pca, mode='min', factor=0.1, patience=3)\n",
    "scheduler_gat_nopca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gat_nopca, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "scheduler_gin_pca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gin_pca, mode='min', factor=0.1, patience=3)\n",
    "scheduler_gin_nopca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gin_nopca, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "# define loss\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = MCCLoss(reduction=\"product\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e353cf93-a8b6-4171-a507-dec8c0564c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train(model, optimizer, train_loader, val_loader, model_arch, scheduler=None, epochs=100, patience=7):\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "    best_model_wts = None  # Variable to store the best model weights\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Ensure input features are in float32\n",
    "            data.x = data.x.float()\n",
    "            \n",
    "            # If data.y is a list, convert it to a tensor, then cast to float32\n",
    "            if isinstance(data.y, list):\n",
    "                data.y = torch.tensor(data.y, dtype=torch.float32).to(device)\n",
    "            else:\n",
    "                data.y = data.y.float()  # Otherwise, cast directly\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if model_arch == \"EGNN\":\n",
    "                out = model(data.x, data.edge_index, data.coords.float(), data.batch)\n",
    "            else:\n",
    "                out = model(data.x, data.edge_index, data.batch, data.u)\n",
    "            loss = criterion(out, data.y)  # Multi-label target\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            all_train_preds.append(out.detach())\n",
    "            all_train_targets.append(data.y)\n",
    "        \n",
    "        all_train_preds = torch.cat(all_train_preds, dim=0)\n",
    "        all_train_targets = torch.cat(all_train_targets, dim=0)\n",
    "        \n",
    "        # Calculate metrics for train\n",
    "        train_metrics = calculate_metrics(all_train_preds, all_train_targets)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, model_arch)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.3f}, Train metrics: {train_metrics}')\n",
    "        print(f'            Val Loss:   {val_loss:.3f}, Val F1:   {val_metrics}')\n",
    "        print('*****')\n",
    "        \n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            print(f'Validation loss decreased ({best_val_loss:.4f} -> {val_loss:.4f}). Saving model...')\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = model.state_dict()  # Save the best model weights\n",
    "            patience_counter = 0  # Reset patience counter when improvement occurs\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'No improvement for {patience_counter} epochs.')\n",
    "\n",
    "        # Early stopping: stop if no improvement for 'patience' number of epochs\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {patience_counter} epochs without improvement.')\n",
    "            break\n",
    "\n",
    "        # Step the learning rate scheduler (if provided)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            print(f\"Learning rate adjusted: {scheduler.get_last_lr()}\")\n",
    "    \n",
    "    # Load the best model weights at the end of training\n",
    "    if best_model_wts is not None:\n",
    "        # Step 1: Create a new instance of the same model class (assume the class is called `model.__class__`)\n",
    "        copied_model = model.__class__(*model.args)  # Make sure to pass the same initialization arguments\n",
    "        \n",
    "        # Step 2: Load the state_dict of the original model into the copied model\n",
    "        copied_model.load_state_dict(best_model_wts)\n",
    "\n",
    "        # Step 3: Move the copied model to the same device as the original model (if you're using GPU)\n",
    "        copied_model = copied_model.to(next(model.parameters()).device)\n",
    "    \n",
    "    return copied_model\n",
    "    '''\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, model_arch, scheduler=None, epochs=100, patience=5):\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "    best_model_wts = None  # Variable to store the best model weights\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Ensure input features are in float32\n",
    "            data.x = data.x.float()\n",
    "            \n",
    "            # If data.y is a list, convert it to a tensor, then cast to float32\n",
    "            if isinstance(data.y, list):\n",
    "                data.y = torch.tensor(data.y, dtype=torch.float32).to(device)\n",
    "            else:\n",
    "                data.y = data.y.float()  # Otherwise, cast directly\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if model_arch == \"EGNN\":\n",
    "                out = model(data.x, data.edge_index, data.coords.float(), data.batch)\n",
    "            else:\n",
    "                out = model(data.x, data.edge_index, data.batch, data.u)\n",
    "            loss = criterion(out, data.y)  # Multi-label target\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            all_train_preds.append(out.detach())\n",
    "            all_train_targets.append(data.y)\n",
    "        \n",
    "        all_train_preds = torch.cat(all_train_preds, dim=0)\n",
    "        all_train_targets = torch.cat(all_train_targets, dim=0)\n",
    "        \n",
    "        # Calculate metrics for train\n",
    "        train_metrics = calculate_metrics(all_train_preds, all_train_targets)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, model_arch)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.3f}, Train metrics: {train_metrics}')\n",
    "        print(f'            Val Loss:   {val_loss:.3f}, Val F1:   {val_metrics}')\n",
    "        print('*****')\n",
    "        \n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            print(f'Validation loss decreased ({best_val_loss:.4f} -> {val_loss:.4f}). Saving model...')\n",
    "            best_val_loss = val_loss\n",
    "            # Save both model state and arguments used to create it\n",
    "            best_model_wts = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'args': model.args  # Save initialization arguments\n",
    "            }\n",
    "            patience_counter = 0  # Reset patience counter when improvement occurs\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'No improvement for {patience_counter} epochs.')\n",
    "\n",
    "        # Early stopping: stop if no improvement for 'patience' number of epochs\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {patience_counter} epochs without improvement.')\n",
    "            break\n",
    "\n",
    "        # Step the learning rate scheduler (if provided)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            print(f\"Learning rate adjusted: {scheduler.get_last_lr()}\")\n",
    "    \n",
    "    # Load the best model weights at the end of training\n",
    "    if best_model_wts is not None:\n",
    "        # Step 1: Create a new instance of the same model class with saved args\n",
    "        copied_model = model.__class__(*best_model_wts['args'])  # Use saved args\n",
    "        \n",
    "        # Step 2: Load the state_dict of the original model into the copied model\n",
    "        copied_model.load_state_dict(best_model_wts['state_dict'])\n",
    "\n",
    "        # Step 3: Move the copied model to the same device as the original model (if you're using GPU)\n",
    "        copied_model = copied_model.to(next(model.parameters()).device)\n",
    "    \n",
    "    return copied_model\n",
    "\n",
    "\n",
    "# Evaluation loop remains the same\n",
    "def evaluate(model, loader, model_arch, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Ensure input features are in float32\n",
    "            data.x = data.x.float()\n",
    "            \n",
    "            # If data.y is a list, convert it to a tensor, then cast to float32\n",
    "            if isinstance(data.y, list):\n",
    "                data.y = torch.tensor(data.y, dtype=torch.float32).to(device)\n",
    "            else:\n",
    "                data.y = data.y.float()  # Otherwise, cast directly\n",
    "\n",
    "            if model_arch == \"EGNN\":\n",
    "                out = model(data.x, data.edge_index, data.coords.float(), data.batch)\n",
    "            else:\n",
    "                out = model(data.x, data.edge_index, data.batch, data.u)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(out)\n",
    "            all_targets.append(data.y)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    # Calculate metrics for validation\n",
    "    metrics = calculate_metrics(all_preds, all_targets, threshold)\n",
    "    \n",
    "    return total_loss / len(loader), metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2e19d02-7eaa-4338-b059-0092864c87cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 803.724, Train metrics: {'f1_per_class': array([0.5075594 , 0.57459506, 0.52291105]), 'f1_micro': 0.537359900373599, 'f1_macro': 0.5350218339582626, 'precision_per_class': array([0.48654244, 0.51687117, 0.42358079]), 'precision_micro': 0.47365532381997805, 'precision_macro': 0.47566479824485164, 'recall_per_class': array([0.53047404, 0.64683301, 0.68309859]), 'recall_micro': 0.620863309352518, 'recall_macro': 0.6201352152056835, 'mcc_per_class': [0.09322658644708566, 0.0041521986071913625, 0.006534389430713968], 'mcc_overall': 0.0378207498513887}\n",
      "            Val Loss:   106.778, Val F1:   {'f1_per_class': array([0.58974359, 0.65945946, 0.59217877]), 'f1_micro': 0.6153846153846154, 'f1_macro': 0.6137939400509232, 'precision_per_class': array([0.45544554, 0.50833333, 0.42063492]), 'precision_micro': 0.4610951008645533, 'precision_macro': 0.46147126617423645, 'recall_per_class': array([0.83636364, 0.93846154, 1.        ]), 'recall_micro': 0.9248554913294798, 'recall_macro': 0.9249417249417249, 'mcc_per_class': [0.07674954901938891, -0.06747097830170132, 0.0], 'mcc_overall': 0.0229879780934763}\n",
      "*****\n",
      "Validation loss decreased (inf -> 106.7777). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 2041.014, Train metrics: {'f1_per_class': array([0.26875   , 0.52380952, 0.53075571]), 'f1_micro': 0.46888260254596886, 'f1_macro': 0.4411050785281892, 'precision_per_class': array([0.43654822, 0.51984877, 0.4241573 ]), 'precision_micro': 0.4610570236439499, 'precision_macro': 0.4601847659958603, 'recall_per_class': array([0.19413093, 0.52783109, 0.70892019]), 'recall_micro': 0.47697841726618706, 'recall_macro': 0.476960735783744, 'mcc_per_class': [-0.0016179385186601903, 0.009469535426978239, 0.008726452596040338], 'mcc_overall': 0.0052689651563636855}\n",
      "            Val Loss:   478.864, Val F1:   {'f1_per_class': array([0.        , 0.37623762, 0.        ]), 'f1_micro': 0.18009478672985782, 'f1_macro': 0.1254125412541254, 'precision_per_class': array([0.        , 0.52777778, 0.        ]), 'precision_micro': 0.5, 'precision_macro': 0.17592592592592593, 'recall_per_class': array([0.        , 0.29230769, 0.        ]), 'recall_micro': 0.10982658959537572, 'recall_macro': 0.09743589743589744, 'mcc_per_class': [-0.11177799767078231, 0.015066058828554761, 0.0], 'mcc_overall': 0.02840353148391933}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 804.141, Train metrics: {'f1_per_class': array([0.4211793 , 0.54005401, 0.45188285]), 'f1_micro': 0.47688060731538995, 'f1_macro': 0.4710387175448509, 'precision_per_class': array([0.45103093, 0.50847458, 0.40754717]), 'precision_micro': 0.4582228116710875, 'precision_macro': 0.4556842246391863, 'recall_per_class': array([0.39503386, 0.57581574, 0.50704225]), 'recall_micro': 0.49712230215827335, 'recall_macro': 0.49263061750993503, 'mcc_per_class': [0.02043999477023775, -0.016241994257457933, -0.029374684077528444], 'mcc_overall': -0.00013832398361538064}\n",
      "            Val Loss:   219.570, Val F1:   {'f1_per_class': array([0.25287356, 0.37623762, 0.59090909]), 'f1_micro': 0.45054945054945056, 'f1_macro': 0.40667342596328604, 'precision_per_class': array([0.34375   , 0.52777778, 0.42276423]), 'precision_micro': 0.4293193717277487, 'precision_macro': 0.4314306684733514, 'recall_per_class': array([0.2       , 0.29230769, 0.98113208]), 'recall_micro': 0.47398843930635837, 'recall_macro': 0.4911465892597968, 'mcc_per_class': [-0.1091244749534345, 0.015066058828554761, 0.02761858293969059], 'mcc_overall': -0.05751490061570446}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 998.906, Train metrics: {'f1_per_class': array([0.39601494, 0.55613126, 0.53621982]), 'f1_micro': 0.5079063883617964, 'f1_macro': 0.49612200719131333, 'precision_per_class': array([0.44166667, 0.50549451, 0.41548387]), 'precision_micro': 0.45316027088036115, 'precision_macro': 0.4542150143763047, 'recall_per_class': array([0.35891648, 0.61804223, 0.75586854]), 'recall_micro': 0.5776978417266188, 'recall_macro': 0.5776090832145893, 'mcc_per_class': [0.005225692450513116, -0.025687639651977515, -0.02158355036576742], 'mcc_overall': -0.012209402873093966}\n",
      "            Val Loss:   285.661, Val F1:   {'f1_per_class': array([0.24096386, 0.67724868, 0.51006711]), 'f1_micro': 0.5320665083135392, 'f1_macro': 0.4760932155881079, 'precision_per_class': array([0.35714286, 0.51612903, 0.39583333]), 'precision_micro': 0.45161290322580644, 'precision_macro': 0.4230350742447517, 'recall_per_class': array([0.18181818, 0.98461538, 0.71698113]), 'recall_micro': 0.6473988439306358, 'recall_macro': 0.6278048995030128, 'mcc_per_class': [-0.08553728447254387, 0.004033783220492441, -0.08987225648903338], 'mcc_overall': -0.016797739311701906}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 942.043, Train metrics: {'f1_per_class': array([0.37197452, 0.54003724, 0.50133809]), 'f1_micro': 0.4812080536912752, 'f1_macro': 0.4711166190770131, 'precision_per_class': array([0.42690058, 0.5244123 , 0.40431655]), 'precision_micro': 0.4509433962264151, 'precision_macro': 0.4518764760407023, 'recall_per_class': array([0.32957111, 0.55662188, 0.65962441]), 'recall_micro': 0.5158273381294964, 'recall_macro': 0.5152724667461429, 'mcc_per_class': [-0.01625408266464577, 0.01996617885159731, -0.05120376417504936], 'mcc_overall': -0.015481874731637557}\n",
      "            Val Loss:   298.207, Val F1:   {'f1_per_class': array([0.58441558, 0.35294118, 0.61176471]), 'f1_micro': 0.539906103286385, 'f1_macro': 0.5163738222561752, 'precision_per_class': array([0.45454545, 0.48648649, 0.44444444]), 'precision_micro': 0.45454545454545453, 'precision_macro': 0.4618254618254618, 'recall_per_class': array([0.81818182, 0.27692308, 0.98113208]), 'recall_micro': 0.6647398843930635, 'recall_macro': 0.6920789901921977, 'mcc_per_class': [0.06964220602680632, -0.037914305571701355, 0.17389759109658814], 'mcc_overall': -0.008928036130804024}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 6, Train Loss: 2209.774, Train metrics: {'f1_per_class': array([0.42857143, 0.54151625, 0.51377778]), 'f1_micro': 0.5004881223560039, 'f1_macro': 0.494621817278857, 'precision_per_class': array([0.4534005 , 0.51107325, 0.41344778]), 'precision_micro': 0.45692216280451575, 'precision_macro': 0.45930718005262733, 'recall_per_class': array([0.40632054, 0.57581574, 0.67840376]), 'recall_micro': 0.5532374100719425, 'recall_macro': 0.5535133455309329, 'mcc_per_class': [0.024666904659104615, -0.010025045706271175, -0.02399946018866182], 'mcc_overall': -0.003069929514205535}\n",
      "            Val Loss:   311.911, Val F1:   {'f1_per_class': array([0.125     , 0.26373626, 0.61176471]), 'f1_micro': 0.41846153846153844, 'f1_macro': 0.33350032320620554, 'precision_per_class': array([0.44444444, 0.46153846, 0.44444444]), 'precision_micro': 0.4473684210526316, 'precision_macro': 0.4501424501424502, 'recall_per_class': array([0.07272727, 0.18461538, 0.98113208]), 'recall_micro': 0.3930635838150289, 'recall_macro': 0.4128249109381185, 'mcc_per_class': [0.004438311756502355, -0.055438533373787326, 0.17389759109658814], 'mcc_overall': -0.01696077571089925}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 7, Train Loss: 1124.356, Train metrics: {'f1_per_class': array([0.35638298, 0.42936596, 0.53027523]), 'f1_micro': 0.44947099598686613, 'f1_macro': 0.43867472342046754, 'precision_per_class': array([0.43365696, 0.51058201, 0.43524096]), 'precision_micro': 0.45595854922279794, 'precision_macro': 0.45982664412207835, 'recall_per_class': array([0.30248307, 0.37044146, 0.67840376]), 'recall_micro': 0.44316546762589926, 'recall_macro': 0.45044276152639223, 'mcc_per_class': [-0.006048091252576807, -0.007343650045088126, 0.03887329459334184], 'mcc_overall': -0.004197414778481485}\n",
      "            Val Loss:   332.948, Val F1:   {'f1_per_class': array([0.03333333, 0.26666667, 0.53793103]), 'f1_micro': 0.3525423728813559, 'f1_macro': 0.2793103448275862, 'precision_per_class': array([0.2       , 0.48      , 0.42391304]), 'precision_micro': 0.4262295081967213, 'precision_macro': 0.36797101449275366, 'recall_per_class': array([0.01818182, 0.18461538, 0.73584906]), 'recall_micro': 0.30057803468208094, 'recall_macro': 0.3128820864669921, 'mcc_per_class': [-0.09693889434337498, -0.03571298541646928, 0.010923224499058725], 'mcc_overall': -0.04356804997430442}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 8, Train Loss: 2183.994, Train metrics: {'f1_per_class': array([0.3       , 0.39526627, 0.50046339]), 'f1_micro': 0.4122541603630862, 'f1_macro': 0.3985765547396687, 'precision_per_class': array([0.3898917 , 0.5154321 , 0.41347626]), 'precision_micro': 0.43460925039872406, 'precision_macro': 0.43960001963867607, 'recall_per_class': array([0.24379233, 0.32053743, 0.63380282]), 'recall_micro': 0.3920863309352518, 'recall_macro': 0.39937752332695825, 'mcc_per_class': [-0.059787322231998626, 0.0001384350827299914, -0.021576954164547912], 'mcc_overall': -0.039906289301899774}\n",
      "            Val Loss:   415.434, Val F1:   {'f1_per_class': array([0.03389831, 0.26373626, 0.53146853]), 'f1_micro': 0.34812286689419797, 'f1_macro': 0.2763677000965137, 'precision_per_class': array([0.25      , 0.46153846, 0.42222222]), 'precision_micro': 0.425, 'precision_macro': 0.3779202279202279, 'recall_per_class': array([0.01818182, 0.18461538, 0.71698113]), 'recall_micro': 0.2947976878612717, 'recall_macro': 0.3065927782908915, 'mcc_per_class': [-0.06809377690001776, -0.055438533373787326, 0.005083942560314576], 'mcc_overall': -0.04472473186429682}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(326.05749130249023,\n",
       " {'f1_per_class': array([0.06779661, 0.23913043, 0.54545455]),\n",
       "  'f1_micro': 0.35374149659863946,\n",
       "  'f1_macro': 0.2841271968022152,\n",
       "  'precision_per_class': array([0.66666667, 0.40740741, 0.43333333]),\n",
       "  'precision_micro': 0.43333333333333335,\n",
       "  'precision_macro': 0.5024691358024691,\n",
       "  'recall_per_class': array([0.03571429, 0.16923077, 0.73584906]),\n",
       "  'recall_micro': 0.2988505747126437,\n",
       "  'recall_macro': 0.3135980371829428,\n",
       "  'mcc_per_class': [0.07071373773001521,\n",
       "   -0.10852970474001519,\n",
       "   0.05063784071198787],\n",
       "  'mcc_overall': -0.03179805667945898})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCN PCA\n",
    "trained_gcn_pca = train(gcn_pca, opt_gcn_pca, train_loader_pca, val_loader_pca, model_arch=\"GCN\", scheduler=scheduler_gcn_pca, epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "trained_gcn_pca.eval()\n",
    "val_loss, val_metrics = evaluate(trained_gcn_pca, test_loader_pca, model_arch=\"GCN\")\n",
    "val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ce2cb16-66c6-4e65-929d-c9e361cec3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 699.970, Train metrics: {'f1_per_class': array([0.45708155, 0.50579151, 0.39206534]), 'f1_micro': 0.4552212389380531, 'f1_macro': 0.4516461316933069, 'precision_per_class': array([0.43558282, 0.50873786, 0.38979118]), 'precision_micro': 0.4480836236933798, 'precision_macro': 0.44470395648607425, 'recall_per_class': array([0.48081264, 0.50287908, 0.3943662 ]), 'recall_micro': 0.46258992805755395, 'recall_macro': 0.45935263898714584, 'mcc_per_class': [-0.0050663980947809484, -0.013443503017492616, -0.05512136207557225], 'mcc_overall': -0.019415365383042005}\n",
      "            Val Loss:   161.274, Val F1:   {'f1_per_class': array([0.53781513, 0.43971631, 0.55757576]), 'f1_micro': 0.5129411764705882, 'f1_macro': 0.5117023985609718, 'precision_per_class': array([0.5       , 0.40789474, 0.41071429]), 'precision_micro': 0.43253968253968256, 'precision_macro': 0.4395363408521303, 'recall_per_class': array([0.58181818, 0.47692308, 0.86792453]), 'recall_micro': 0.630057803468208, 'recall_macro': 0.6422219290143819, 'mcc_per_class': [0.13006894274418304, -0.26638339194093524, -0.056840205792838584], 'mcc_overall': -0.07134090610363834}\n",
      "*****\n",
      "Validation loss decreased (inf -> 161.2737). Saving model...\n",
      "Epoch 2, Train Loss: 528.333, Train metrics: {'f1_per_class': array([0.47167868, 0.49615385, 0.48106448]), 'f1_micro': 0.48326639892904955, 'f1_macro': 0.48296567034559396, 'precision_per_class': array([0.43371212, 0.49710983, 0.42649728]), 'precision_micro': 0.4518147684605757, 'precision_macro': 0.45243974182622254, 'recall_per_class': array([0.51693002, 0.49520154, 0.55164319]), 'recall_micro': 0.5194244604316547, 'recall_macro': 0.5212582501900879, 'mcc_per_class': [-0.009415032548962539, -0.03744727208579907, 0.011375662731341588], 'mcc_overall': -0.01371848441032659}\n",
      "            Val Loss:   262.262, Val F1:   {'f1_per_class': array([0.54961832, 0.6       , 0.52307692]), 'f1_micro': 0.5614849187935035, 'f1_macro': 0.5575650812292033, 'precision_per_class': array([0.47368421, 0.48571429, 0.44155844]), 'precision_micro': 0.4689922480620155, 'precision_macro': 0.46698564593301434, 'recall_per_class': array([0.65454545, 0.78461538, 0.64150943]), 'recall_micro': 0.6994219653179191, 'recall_macro': 0.6935567577077011, 'mcc_per_class': [0.0924161092366757, -0.13494195660340266, 0.05313164600504543], 'mcc_overall': 0.03331720916206726}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 3, Train Loss: 812.905, Train metrics: {'f1_per_class': array([0.49544995, 0.57721519, 0.50533049]), 'f1_micro': 0.5295629820051414, 'f1_macro': 0.5259985432408059, 'precision_per_class': array([0.44871795, 0.51506024, 0.46289062]), 'precision_micro': 0.47851335656213706, 'precision_macro': 0.4755562715606014, 'recall_per_class': array([0.5530474 , 0.65642994, 0.55633803]), 'recall_micro': 0.5928057553956835, 'recall_macro': 0.5886051248835485, 'mcc_per_class': [0.02301443062916325, -0.0007504222618529854, 0.08518624275334272], 'mcc_overall': 0.046512444604320725}\n",
      "            Val Loss:   211.339, Val F1:   {'f1_per_class': array([0.39285714, 0.52631579, 0.45454545]), 'f1_micro': 0.46524064171123, 'f1_macro': 0.45790612895876054, 'precision_per_class': array([0.38596491, 0.45977011, 0.43859649]), 'precision_micro': 0.43283582089552236, 'precision_macro': 0.42811050615043356, 'recall_per_class': array([0.4       , 0.61538462, 0.47169811]), 'recall_micro': 0.5028901734104047, 'recall_macro': 0.4956942428640542, 'mcc_per_class': [-0.09262626084582558, -0.1676724036358171, 0.03306955662633732], 'mcc_overall': -0.0531235704239742}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Epoch 4, Train Loss: 741.227, Train metrics: {'f1_per_class': array([0.40147783, 0.4871001 , 0.49193548]), 'f1_micro': 0.46375766318067074, 'f1_macro': 0.4601711398608191, 'precision_per_class': array([0.44173442, 0.52678571, 0.43109541]), 'precision_micro': 0.46493130874909616, 'precision_macro': 0.4665385126634372, 'recall_per_class': array([0.36794582, 0.45297505, 0.57276995]), 'recall_micro': 0.46258992805755395, 'recall_macro': 0.46456360832135113, 'mcc_per_class': [0.005431087452984317, 0.020445129292452485, 0.022224309316875284], 'mcc_overall': 0.012199172184279806}\n",
      "            Val Loss:   246.918, Val F1:   {'f1_per_class': array([0.24719101, 0.32989691, 0.52702703]), 'f1_micro': 0.39520958083832336, 'f1_macro': 0.36803831515982566, 'precision_per_class': array([0.32352941, 0.5       , 0.41052632]), 'precision_micro': 0.40993788819875776, 'precision_macro': 0.41135190918472647, 'recall_per_class': array([0.2       , 0.24615385, 0.73584906]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.3940009675858733, 'mcc_per_class': [-0.13848459662372284, -0.0185318792684545, -0.035846219176605514], 'mcc_overall': -0.08252838561204498}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Epoch 5, Train Loss: 1297.655, Train metrics: {'f1_per_class': array([0.41457859, 0.50160772, 0.54304636]), 'f1_micro': 0.4902370990237099, 'f1_macro': 0.486410887452337, 'precision_per_class': array([0.4183908 , 0.56796117, 0.4548336 ]), 'precision_micro': 0.47564276048714477, 'precision_macro': 0.48039518903686235, 'recall_per_class': array([0.41083521, 0.44913628, 0.67370892]), 'recall_micro': 0.5057553956834533, 'recall_macro': 0.5112268036754335, 'mcc_per_class': [-0.03466068028573164, 0.0873375998021667, 0.08734318207344678], 'mcc_overall': 0.03394952947959293}\n",
      "            Val Loss:   146.109, Val F1:   {'f1_per_class': array([0.52892562, 0.6       , 0.50909091]), 'f1_micro': 0.5511811023622047, 'f1_macro': 0.5460055096418733, 'precision_per_class': array([0.48484848, 0.52941176, 0.49122807]), 'precision_micro': 0.5048076923076923, 'precision_macro': 0.5018294399099352, 'recall_per_class': array([0.58181818, 0.69230769, 0.52830189]), 'recall_micro': 0.6069364161849711, 'recall_macro': 0.6008092536394423, 'mcc_per_class': [0.10222754728341485, 0.039007221387497064, 0.12997104813606994], 'mcc_overall': 0.10465242704389677}\n",
      "*****\n",
      "Validation loss decreased (161.2737 -> 146.1090). Saving model...\n",
      "Epoch 6, Train Loss: 670.010, Train metrics: {'f1_per_class': array([0.48040201, 0.5115346 , 0.37027027]), 'f1_micro': 0.46193265007320644, 'f1_macro': 0.4540689613773186, 'precision_per_class': array([0.43297101, 0.53571429, 0.43630573]), 'precision_micro': 0.470193740685544, 'precision_macro': 0.4683303442303719, 'recall_per_class': array([0.53950339, 0.48944338, 0.32159624]), 'recall_micro': 0.4539568345323741, 'recall_macro': 0.4501810027516573, 'mcc_per_class': [-0.011513122329700263, 0.03847046763606492, 0.020309029899414025], 'mcc_overall': 0.02127927302908191}\n",
      "            Val Loss:   231.764, Val F1:   {'f1_per_class': array([0.56060606, 0.3960396 , 0.35294118]), 'f1_micro': 0.4528301886792453, 'f1_macro': 0.4365289470123483, 'precision_per_class': array([0.48051948, 0.55555556, 0.46875   ]), 'precision_micro': 0.496551724137931, 'precision_macro': 0.5016083453583454, 'recall_per_class': array([0.67272727, 0.30769231, 0.28301887]), 'recall_micro': 0.4161849710982659, 'recall_macro': 0.42114614944803624, 'mcc_per_class': [0.11124332298042591, 0.05022019609518254, 0.056867410019562185], 'mcc_overall': 0.061563245362459496}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 7, Train Loss: 700.151, Train metrics: {'f1_per_class': array([0.48870637, 0.50770812, 0.46990741]), 'f1_micro': 0.4895055140519388, 'f1_macro': 0.4887739640431327, 'precision_per_class': array([0.44821092, 0.54646018, 0.46347032]), 'precision_micro': 0.48416608022519353, 'precision_macro': 0.48604713980434927, 'recall_per_class': array([0.53724605, 0.47408829, 0.47652582]), 'recall_micro': 0.4949640287769784, 'recall_macro': 0.4959533876680949, 'mcc_per_class': [0.02126384497849156, 0.056009313816297215, 0.07455302828907245], 'mcc_overall': 0.04875550449841366}\n",
      "            Val Loss:   191.145, Val F1:   {'f1_per_class': array([0.5483871 , 0.64634146, 0.41304348]), 'f1_micro': 0.5578947368421052, 'f1_macro': 0.5359240128165658, 'precision_per_class': array([0.49275362, 0.53535354, 0.48717949]), 'precision_micro': 0.5120772946859904, 'precision_macro': 0.5050955485738094, 'recall_per_class': array([0.61818182, 0.81538462, 0.35849057]), 'recall_micro': 0.6127167630057804, 'recall_macro': 0.5973523332013898, 'mcc_per_class': [0.1247775249410708, 0.07464233184075404, 0.0902519217068529], 'mcc_overall': 0.12014907736276577}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Epoch 8, Train Loss: 672.581, Train metrics: {'f1_per_class': array([0.42594859, 0.5529522 , 0.42775882]), 'f1_micro': 0.4755700325732899, 'f1_macro': 0.4688865372284381, 'precision_per_class': array([0.46524064, 0.54029304, 0.41501104]), 'precision_micro': 0.4785142024763292, 'precision_macro': 0.4735149065106213, 'recall_per_class': array([0.39277652, 0.56621881, 0.44131455]), 'recall_micro': 0.47266187050359715, 'recall_macro': 0.46676996255781594, 'mcc_per_class': [0.04179063625360251, 0.054122518888833814, -0.011594287251254344], 'mcc_overall': 0.03691078911942065}\n",
      "            Val Loss:   449.251, Val F1:   {'f1_per_class': array([0.38636364, 0.53731343, 0.47692308]), 'f1_micro': 0.4772727272727273, 'f1_macro': 0.46686671537417807, 'precision_per_class': array([0.51515152, 0.52173913, 0.4025974 ]), 'precision_micro': 0.4692737430167598, 'precision_macro': 0.47982934939456684, 'recall_per_class': array([0.30909091, 0.55384615, 0.58490566]), 'recall_micro': 0.48554913294797686, 'recall_macro': 0.4826142411048071, 'mcc_per_class': [0.09445800946205503, 0.012914767944180243, -0.04580314310779778], 'mcc_overall': 0.022085977274376598}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Epoch 9, Train Loss: 530.508, Train metrics: {'f1_per_class': array([0.47559709, 0.53360489, 0.4266055 ]), 'f1_micro': 0.4806531771388001, 'f1_macro': 0.478602494996795, 'precision_per_class': array([0.44038462, 0.56832972, 0.41704036]), 'precision_micro': 0.4744218640504555, 'precision_macro': 0.47525156404444946, 'recall_per_class': array([0.51693002, 0.50287908, 0.43661972]), 'recall_micro': 0.4870503597122302, 'recall_macro': 0.48547627319268, 'mcc_per_class': [0.004572618689789686, 0.09708806076176388, -0.007781444522329175], 'mcc_overall': 0.030514968092619846}\n",
      "            Val Loss:   183.144, Val F1:   {'f1_per_class': array([0.55033557, 0.5511811 , 0.55405405]), 'f1_micro': 0.5518867924528302, 'f1_macro': 0.5518569089620192, 'precision_per_class': array([0.43617021, 0.56451613, 0.43157895]), 'precision_micro': 0.46613545816733065, 'precision_macro': 0.4774217630555455, 'recall_per_class': array([0.74545455, 0.53846154, 0.77358491]), 'recall_micro': 0.6763005780346821, 'recall_macro': 0.6858336631921538, 'mcc_per_class': [-0.0011671066839939518, 0.09580235148669548, 0.03880871662921754], 'mcc_overall': 0.023882338080867038}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Epoch 10, Train Loss: 493.841, Train metrics: {'f1_per_class': array([0.49849246, 0.56954887, 0.52533609]), 'f1_micro': 0.5320555188367482, 'f1_macro': 0.5311258084983704, 'precision_per_class': array([0.44927536, 0.55801105, 0.46950092]), 'precision_micro': 0.49205378973105135, 'precision_macro': 0.49226244541900505, 'recall_per_class': array([0.55981941, 0.5815739 , 0.59624413]), 'recall_micro': 0.579136690647482, 'recall_macro': 0.5792124803003723, 'mcc_per_class': [0.024523307005677338, 0.0919882899806771, 0.10458937678280748], 'mcc_overall': 0.07332694509381477}\n",
      "            Val Loss:   99.540, Val F1:   {'f1_per_class': array([0.36956522, 0.63087248, 0.48695652]), 'f1_micro': 0.5168539325842697, 'f1_macro': 0.4957980741173038, 'precision_per_class': array([0.45945946, 0.55952381, 0.4516129 ]), 'precision_micro': 0.5027322404371585, 'precision_macro': 0.49019872406969184, 'recall_per_class': array([0.30909091, 0.72307692, 0.52830189]), 'recall_micro': 0.5317919075144508, 'recall_macro': 0.5201565729867617, 'mcc_per_class': [0.029838527903289554, 0.12352534954341743, 0.06176325408855523], 'mcc_overall': 0.08761812791732723}\n",
      "*****\n",
      "Validation loss decreased (146.1090 -> 99.5398). Saving model...\n",
      "Epoch 11, Train Loss: 657.898, Train metrics: {'f1_per_class': array([0.45560748, 0.57617729, 0.47511312]), 'f1_micro': 0.5079702444208289, 'f1_macro': 0.5022992947086732, 'precision_per_class': array([0.47215496, 0.55516014, 0.45851528]), 'precision_micro': 0.5003489183531054, 'precision_macro': 0.4952767966239789, 'recall_per_class': array([0.44018059, 0.59884837, 0.49295775]), 'recall_micro': 0.5158273381294964, 'recall_macro': 0.5106622339694652, 'mcc_per_class': [0.05690603238962498, 0.08916128603930032, 0.06847009116045155], 'mcc_overall': 0.07988137658230184}\n",
      "            Val Loss:   223.392, Val F1:   {'f1_per_class': array([0.55462185, 0.53731343, 0.5       ]), 'f1_micro': 0.5295629820051414, 'f1_macro': 0.5306450938584389, 'precision_per_class': array([0.515625  , 0.52173913, 0.40963855]), 'precision_micro': 0.47685185185185186, 'precision_macro': 0.4823342282172167, 'recall_per_class': array([0.6       , 0.55384615, 0.64150943]), 'recall_micro': 0.5953757225433526, 'recall_macro': 0.5984518626028059, 'mcc_per_class': [0.16207809662263434, 0.012914767944180243, -0.03094747362736325], 'mcc_overall': 0.04445364661850771}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 12, Train Loss: 796.287, Train metrics: {'f1_per_class': array([0.49733191, 0.60337178, 0.51282051]), 'f1_micro': 0.5412605588044185, 'f1_macro': 0.5378414022229026, 'precision_per_class': array([0.47165992, 0.56105611, 0.44217687]), 'precision_micro': 0.4934834123222749, 'precision_macro': 0.49163096512906684, 'recall_per_class': array([0.52595937, 0.65259117, 0.61032864]), 'recall_micro': 0.5992805755395684, 'recall_macro': 0.5962930590896041, 'mcc_per_class': [0.06595951747284193, 0.11191657192129625, 0.049693356521295785], 'mcc_overall': 0.07912368820712248}\n",
      "            Val Loss:   417.540, Val F1:   {'f1_per_class': array([0.36170213, 0.5915493 , 0.36144578]), 'f1_micro': 0.46394984326018807, 'f1_macro': 0.4382324021889175, 'precision_per_class': array([0.43589744, 0.54545455, 0.5       ]), 'precision_micro': 0.5068493150684932, 'precision_macro': 0.4937839937839938, 'recall_per_class': array([0.30909091, 0.64615385, 0.28301887]), 'recall_micro': 0.4277456647398844, 'recall_macro': 0.4127545410564279, 'mcc_per_class': [-0.00082417380255889, 0.07420221991555018, 0.08987225648903338], 'mcc_overall': 0.07830499044312102}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Epoch 13, Train Loss: 525.981, Train metrics: {'f1_per_class': array([0.49576271, 0.58681523, 0.48654971]), 'f1_micro': 0.5271210013908206, 'f1_macro': 0.523042548983499, 'precision_per_class': array([0.46706587, 0.56834532, 0.48484848]), 'precision_micro': 0.5100942126514132, 'precision_macro': 0.506753225617655, 'recall_per_class': array([0.5282167 , 0.60652591, 0.48826291]), 'recall_micro': 0.5453237410071943, 'recall_macro': 0.5410018422651048, 'mcc_per_class': [0.057702420107699416, 0.11726172219968742, 0.11038153699848226], 'mcc_overall': 0.1018961512691275}\n",
      "            Val Loss:   151.958, Val F1:   {'f1_per_class': array([0.54263566, 0.61428571, 0.47540984]), 'f1_micro': 0.5473145780051151, 'f1_macro': 0.5441104030886722, 'precision_per_class': array([0.47297297, 0.57333333, 0.42028986]), 'precision_micro': 0.4908256880733945, 'precision_macro': 0.48886538712625666, 'recall_per_class': array([0.63636364, 0.66153846, 0.54716981]), 'recall_micro': 0.6184971098265896, 'recall_macro': 0.6150239697409509, 'mcc_per_class': [0.0877103544893964, 0.13943201833975705, -0.0007690594564264494], 'mcc_overall': 0.07767700548285997}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Epoch 14, Train Loss: 610.760, Train metrics: {'f1_per_class': array([0.47826087, 0.5492823 , 0.43919717]), 'f1_micro': 0.49288762446657186, 'f1_macro': 0.4889134442286096, 'precision_per_class': array([0.46121593, 0.54770992, 0.44180523]), 'precision_micro': 0.4873417721518987, 'precision_macro': 0.48357702741045827, 'recall_per_class': array([0.496614  , 0.55086372, 0.43661972]), 'recall_micro': 0.4985611510791367, 'recall_macro': 0.4946991458012106, 'mcc_per_class': [0.043880252479638844, 0.06720368103000655, 0.034967893138728494], 'mcc_overall': 0.0547758584610299}\n",
      "            Val Loss:   300.633, Val F1:   {'f1_per_class': array([0.27586207, 0.6442953 , 0.52380952]), 'f1_micro': 0.5138121546961326, 'f1_macro': 0.4813222982628213, 'precision_per_class': array([0.375     , 0.57142857, 0.45205479]), 'precision_micro': 0.49206349206349204, 'precision_macro': 0.4661611219830398, 'recall_per_class': array([0.21818182, 0.73846154, 0.62264151]), 'recall_micro': 0.5375722543352601, 'recall_macro': 0.5264282886924396, 'mcc_per_class': [-0.072360614407625, 0.15721408123707672, 0.07469630395451021], 'mcc_overall': 0.06903087371204007}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Epoch 15, Train Loss: 396.819, Train metrics: {'f1_per_class': array([0.49052397, 0.6129318 , 0.53306205]), 'f1_micro': 0.5503489531405783, 'f1_macro': 0.545505940590029, 'precision_per_class': array([0.4845815 , 0.56907895, 0.47037702]), 'precision_micro': 0.5114268066707844, 'precision_macro': 0.5080124883048104, 'recall_per_class': array([0.496614  , 0.66410749, 0.61502347]), 'recall_micro': 0.5956834532374101, 'recall_macro': 0.5919149850894458, 'mcc_per_class': [0.08443203711012603, 0.1320968721974701, 0.10994382075843172], 'mcc_overall': 0.11410983356827945}\n",
      "            Val Loss:   187.257, Val F1:   {'f1_per_class': array([0.45378151, 0.59677419, 0.53424658]), 'f1_micro': 0.5295629820051414, 'f1_macro': 0.5282674271652983, 'precision_per_class': array([0.421875  , 0.62711864, 0.41935484]), 'precision_micro': 0.47685185185185186, 'precision_macro': 0.48944949425915807, 'recall_per_class': array([0.49090909, 0.56923077, 0.73584906]), 'recall_micro': 0.5953757225433526, 'recall_macro': 0.5986629722478779, 'mcc_per_class': [-0.029976826648073437, 0.20889137402036517, -0.004353047939217705], 'mcc_overall': 0.04445364661850771}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Epoch 16, Train Loss: 647.485, Train metrics: {'f1_per_class': array([0.4642487 , 0.49690722, 0.48515865]), 'f1_micro': 0.48214285714285715, 'f1_macro': 0.4821048566944464, 'precision_per_class': array([0.42911877, 0.53674833, 0.43012704]), 'precision_micro': 0.4612352168199737, 'precision_macro': 0.4653313817700093, 'recall_per_class': array([0.50564334, 0.46257198, 0.55633803]), 'recall_micro': 0.5050359712230216, 'recall_macro': 0.5081844486647241, 'mcc_per_class': [-0.018868781689441193, 0.03830428735634457, 0.019420989783727978], 'mcc_overall': 0.005928241264499537}\n",
      "            Val Loss:   186.460, Val F1:   {'f1_per_class': array([0.41269841, 0.57364341, 0.55769231]), 'f1_micro': 0.5125348189415042, 'f1_macro': 0.5146780437478112, 'precision_per_class': array([0.36619718, 0.578125  , 0.56862745]), 'precision_micro': 0.4946236559139785, 'precision_macro': 0.5043165446929946, 'recall_per_class': array([0.47272727, 0.56923077, 0.54716981]), 'recall_micro': 0.5317919075144508, 'recall_macro': 0.5297092844262655, 'mcc_per_class': [-0.16107554417413572, 0.12655994854295036, 0.2472096571711864], 'mcc_overall': 0.0730015494688705}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Epoch 17, Train Loss: 379.472, Train metrics: {'f1_per_class': array([0.49698189, 0.60767947, 0.54189336]), 'f1_micro': 0.5528768884603021, 'f1_macro': 0.5488515731582545, 'precision_per_class': array([0.44827586, 0.53766617, 0.50507099]), 'precision_micro': 0.4997094712376525, 'precision_macro': 0.4970043434273827, 'recall_per_class': array([0.55756208, 0.69865643, 0.58450704]), 'recall_micro': 0.6187050359712231, 'recall_macro': 0.6135751829817918, 'mcc_per_class': [0.02226971021355921, 0.06362646515676197, 0.16538024739760723], 'mcc_overall': 0.09520331415004558}\n",
      "            Val Loss:   199.990, Val F1:   {'f1_per_class': array([0.43137255, 0.57746479, 0.47272727]), 'f1_micro': 0.5028248587570622, 'f1_macro': 0.49385487015975826, 'precision_per_class': array([0.46808511, 0.53246753, 0.45614035]), 'precision_micro': 0.49171270718232046, 'precision_macro': 0.4855643299092347, 'recall_per_class': array([0.4       , 0.63076923, 0.49056604]), 'recall_micro': 0.5144508670520231, 'recall_macro': 0.5071117561683599, 'mcc_per_class': [0.04910985058600563, 0.0416256355623818, 0.06537005379624819], 'mcc_overall': 0.06549333502016716}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92.01296520233154,\n",
       " {'f1_per_class': array([0.41904762, 0.59210526, 0.48333333]),\n",
       "  'f1_micro': 0.5092838196286472,\n",
       "  'f1_macro': 0.4981620718462824,\n",
       "  'precision_per_class': array([0.44897959, 0.51724138, 0.43283582]),\n",
       "  'precision_micro': 0.4729064039408867,\n",
       "  'precision_macro': 0.4663522640142006,\n",
       "  'recall_per_class': array([0.39285714, 0.69230769, 0.54716981]),\n",
       "  'recall_micro': 0.5517241379310345,\n",
       "  'recall_macro': 0.5441115488285299,\n",
       "  'mcc_per_class': [0.01282630240723671,\n",
       "   0.01602171880926478,\n",
       "   0.03324352819429573],\n",
       "  'mcc_overall': 0.034759996137153616})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCN no PCA\n",
    "trained_gcn_nopca = train(gcn_nopca, opt_gcn_nopca, train_loader_nopca, val_loader_nopca, model_arch=\"GCN\", epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "val_loss, val_metrics = evaluate(trained_gcn_nopca, test_loader_nopca, model_arch=\"GCN\")\n",
    "val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cd0b97d-5b0b-453e-8abd-414dff47acce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 600.572, Train metrics: {'f1_per_class': array([0.44281217, 0.52486188, 0.49230769]), 'f1_micro': 0.4884702825592725, 'f1_macro': 0.48666058094962455, 'precision_per_class': array([0.41372549, 0.50442478, 0.41693811]), 'precision_micro': 0.44523386619301364, 'precision_macro': 0.4450294599021087, 'recall_per_class': array([0.47629797, 0.54702495, 0.60093897]), 'recall_micro': 0.5410071942446043, 'recall_macro': 0.5414206291829322, 'mcc_per_class': [-0.04972796221297122, -0.02456289160605297, -0.011149478562935787], 'mcc_overall': -0.029379643716940292}\n",
      "            Val Loss:   239.678, Val F1:   {'f1_per_class': array([0.50406504, 0.36893204, 0.44210526]), 'f1_micro': 0.4423676012461059, 'f1_macro': 0.43836744754775087, 'precision_per_class': array([0.45588235, 0.5       , 0.5       ]), 'precision_micro': 0.4797297297297297, 'precision_macro': 0.4852941176470588, 'recall_per_class': array([0.56363636, 0.29230769, 0.39622642]), 'recall_micro': 0.41040462427745666, 'recall_macro': 0.41739015701279847, 'mcc_per_class': [0.042298904117990706, -0.020871749898527078, 0.11368041158567717], 'mcc_overall': 0.035515701884054285}\n",
      "*****\n",
      "Validation loss decreased (inf -> 239.6779). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 959.483, Train metrics: {'f1_per_class': array([0.45117845, 0.52380952, 0.47198276]), 'f1_micro': 0.48448936911815965, 'f1_macro': 0.48232357786955493, 'precision_per_class': array([0.44866071, 0.51984877, 0.43625498]), 'precision_micro': 0.46991210277214335, 'precision_macro': 0.4682548218773121, 'recall_per_class': array([0.4537246 , 0.52783109, 0.51408451]), 'recall_micro': 0.5, 'recall_macro': 0.4985467353527658, 'mcc_per_class': [0.0188429977526449, 0.009469535426978239, 0.029947180477085972], 'mcc_overall': 0.02275151346647355}\n",
      "            Val Loss:   371.014, Val F1:   {'f1_per_class': array([0.14705882, 0.68085106, 0.10526316]), 'f1_micro': 0.46006389776357826, 'f1_macro': 0.31105768175131193, 'precision_per_class': array([0.38461538, 0.5203252 , 0.75      ]), 'precision_micro': 0.5142857142857142, 'precision_macro': 0.5516468626224724, 'recall_per_class': array([0.09090909, 0.98461538, 0.05660377]), 'recall_micro': 0.4161849710982659, 'recall_macro': 0.37737608303646036, 'mcc_per_class': [-0.03548930616273921, 0.057044570137671634, 0.12080886723401561], 'mcc_overall': 0.08715440264114858}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 673.939, Train metrics: {'f1_per_class': array([0.43627451, 0.47609562, 0.44470046]), 'f1_micro': 0.4538690476190476, 'f1_macro': 0.4523568627210984, 'precision_per_class': array([0.4772118 , 0.49482402, 0.43665158]), 'precision_micro': 0.4699537750385208, 'precision_macro': 0.4695624655067343, 'recall_per_class': array([0.40180587, 0.45873321, 0.45305164]), 'recall_micro': 0.43884892086330934, 'recall_macro': 0.4378635725470869, 'mcc_per_class': [0.06015026140504488, -0.039246430081863355, 0.02728564879653259], 'mcc_overall': 0.02024389262550292}\n",
      "            Val Loss:   356.535, Val F1:   {'f1_per_class': array([0.59459459, 0.68085106, 0.33333333]), 'f1_micro': 0.580952380952381, 'f1_macro': 0.5362596639192384, 'precision_per_class': array([0.47311828, 0.5203252 , 0.4516129 ]), 'precision_micro': 0.4939271255060729, 'precision_macro': 0.4816854620159105, 'recall_per_class': array([0.8       , 0.98461538, 0.26415094]), 'recall_micro': 0.7052023121387283, 'recall_macro': 0.6829221093372038, 'mcc_per_class': [0.12392197571627402, 0.057044570137671634, 0.035846219176605514], 'mcc_overall': 0.09992509422916626}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 840.755, Train metrics: {'f1_per_class': array([0.5089877 , 0.48951049, 0.43312102]), 'f1_micro': 0.4811818501582835, 'f1_macro': 0.47720640321981705, 'precision_per_class': array([0.43811075, 0.51041667, 0.4735376 ]), 'precision_micro': 0.47075017205781144, 'precision_macro': 0.4740216734363863, 'recall_per_class': array([0.60722348, 0.47024952, 0.39906103]), 'recall_micro': 0.4920863309352518, 'recall_macro': 0.49217800977178966, 'mcc_per_class': [-0.00017362529061081984, -0.009349827587617937, 0.07840332665011862], 'mcc_overall': 0.02397731964652897}\n",
      "            Val Loss:   163.478, Val F1:   {'f1_per_class': array([0.6       , 0.20253165, 0.39175258]), 'f1_micro': 0.4508670520231214, 'f1_macro': 0.3980947409630693, 'precision_per_class': array([0.44347826, 0.57142857, 0.43181818]), 'precision_micro': 0.4508670520231214, 'precision_macro': 0.4822416713721061, 'recall_per_class': array([0.92727273, 0.12307692, 0.35849057]), 'recall_micro': 0.4508670520231214, 'recall_macro': 0.46961340546246205, 'mcc_per_class': [0.04544285325321721, 0.039303520309269174, 0.016594304699230466], 'mcc_overall': -0.012547582123220076}\n",
      "*****\n",
      "Validation loss decreased (239.6779 -> 163.4784). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 947.492, Train metrics: {'f1_per_class': array([0.4670913 , 0.51034483, 0.48590022]), 'f1_micro': 0.48836401528308443, 'f1_macro': 0.48777877987423984, 'precision_per_class': array([0.44088176, 0.5242915 , 0.4516129 ]), 'precision_micro': 0.47212894560107455, 'precision_macro': 0.47226205490952305, 'recall_per_class': array([0.496614  , 0.49712092, 0.5258216 ]), 'recall_micro': 0.5057553956834533, 'recall_macro': 0.5065188376782137, 'mcc_per_class': [0.005375694397489613, 0.017525377921818925, 0.06011753081091836], 'mcc_overall': 0.027271334862190022}\n",
      "            Val Loss:   340.006, Val F1:   {'f1_per_class': array([0.12307692, 0.68062827, 0.26315789]), 'f1_micro': 0.4759036144578313, 'f1_macro': 0.3556210300216914, 'precision_per_class': array([0.4       , 0.51587302, 0.43478261]), 'precision_micro': 0.4968553459119497, 'precision_macro': 0.45021854152288937, 'recall_per_class': array([0.07272727, 1.        , 0.18867925]), 'recall_micro': 0.45664739884393063, 'recall_macro': 0.4204688393367639, 'mcc_per_class': [-0.021613162460827576, 0.0, 0.013542597204444257], 'mcc_overall': 0.06701473580499004}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 891.655, Train metrics: {'f1_per_class': array([0.42626728, 0.55964126, 0.44919786]), 'f1_micro': 0.4845784784098698, 'f1_macro': 0.4783687992246463, 'precision_per_class': array([0.43529412, 0.52525253, 0.41257367]), 'precision_micro': 0.462696335078534, 'precision_macro': 0.4577067722566394, 'recall_per_class': array([0.41760722, 0.59884837, 0.49295775]), 'recall_micro': 0.5086330935251798, 'recall_macro': 0.5031377794924147, 'mcc_per_class': [-0.004953388381646635, 0.023693125084710897, -0.017927887320145157], 'mcc_overall': 0.00890652259038666}\n",
      "            Val Loss:   545.599, Val F1:   {'f1_per_class': array([0.53623188, 0.61333333, 0.50331126]), 'f1_micro': 0.55125284738041, 'f1_macro': 0.55095882522315, 'precision_per_class': array([0.44578313, 0.54117647, 0.3877551 ]), 'precision_micro': 0.4548872180451128, 'precision_macro': 0.45823823505305733, 'recall_per_class': array([0.67272727, 0.70769231, 0.71698113]), 'recall_micro': 0.6994219653179191, 'recall_macro': 0.699133570831684, 'mcc_per_class': [0.025982919104303715, 0.07290315169663245, -0.12460475488371231], 'mcc_overall': -0.00861406648881949}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 716.781, Train metrics: {'f1_per_class': array([0.45027322, 0.55910267, 0.44668911]), 'f1_micro': 0.4917369308600337, 'f1_macro': 0.4853550040396939, 'precision_per_class': array([0.43644068, 0.50783699, 0.42795699]), 'precision_micro': 0.46285714285714286, 'precision_macro': 0.45741155260300825, 'recall_per_class': array([0.46501129, 0.621881  , 0.46713615]), 'recall_micro': 0.5244604316546763, 'recall_macro': 0.5180094783323572, 'mcc_per_class': [-0.003280474600300776, -0.01961214019753532, 0.012320151429103119], 'mcc_overall': 0.009522493846268137}\n",
      "            Val Loss:   352.972, Val F1:   {'f1_per_class': array([0.58974359, 0.23809524, 0.27848101]), 'f1_micro': 0.4200626959247649, 'f1_macro': 0.36877328016568517, 'precision_per_class': array([0.45544554, 0.52631579, 0.42307692]), 'precision_micro': 0.4589041095890411, 'precision_macro': 0.46827941903502096, 'recall_per_class': array([0.83636364, 0.15384615, 0.20754717]), 'recall_micro': 0.3872832369942196, 'recall_macro': 0.399252320007037, 'mcc_per_class': [0.07674954901938891, 0.008805412858990253, 0.0025223418645415976], 'mcc_overall': 0.0019619525976905783}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 635.762, Train metrics: {'f1_per_class': array([0.38164251, 0.41939121, 0.48903509]), 'f1_micro': 0.43167110772744577, 'f1_macro': 0.43002293537000297, 'precision_per_class': array([0.41038961, 0.50819672, 0.45884774]), 'precision_micro': 0.4583670169765562, 'precision_macro': 0.45914468944220005, 'recall_per_class': array([0.35665914, 0.35700576, 0.52347418]), 'recall_micro': 0.4079136690647482, 'recall_macro': 0.412379692924445, 'mcc_per_class': [-0.04392515143003732, -0.010753915637431537, 0.07303623362841137], 'mcc_overall': 0.00012475045034308243}\n",
      "            Val Loss:   415.160, Val F1:   {'f1_per_class': array([0.14925373, 0.26966292, 0.50793651]), 'f1_micro': 0.3475177304964539, 'f1_macro': 0.308951053542702, 'precision_per_class': array([0.41666667, 0.5       , 0.43835616]), 'precision_micro': 0.44954128440366975, 'precision_macro': 0.4516742770167428, 'recall_per_class': array([0.09090909, 0.18461538, 0.60377358]), 'recall_micro': 0.2832369942196532, 'recall_macro': 0.29309935347671195, 'mcc_per_class': [-0.012979790623128613, -0.015406852855221774, 0.04212974928922202], 'mcc_overall': -0.01038856199845085}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 9, Train Loss: 728.748, Train metrics: {'f1_per_class': array([0.21523179, 0.24778761, 0.53703704]), 'f1_micro': 0.37171888230313294, 'f1_macro': 0.3333521452453254, 'precision_per_class': array([0.40372671, 0.53503185, 0.44342508]), 'precision_micro': 0.45164609053497945, 'precision_macro': 0.4607278772202972, 'recall_per_class': array([0.14672686, 0.16122841, 0.68075117]), 'recall_micro': 0.3158273381294964, 'recall_macro': 0.3295688143070641, 'mcc_per_class': [-0.030221051651162196, 0.01690177118768531, 0.06046868085360834], 'mcc_overall': -0.009160150904094387}\n",
      "            Val Loss:   364.620, Val F1:   {'f1_per_class': array([0.17142857, 0.27272727, 0.512     ]), 'f1_micro': 0.35335689045936397, 'f1_macro': 0.31871861471861473, 'precision_per_class': array([0.4       , 0.52173913, 0.44444444]), 'precision_micro': 0.45454545454545453, 'precision_macro': 0.45539452495974236, 'recall_per_class': array([0.10909091, 0.18461538, 0.60377358]), 'recall_micro': 0.28901734104046245, 'recall_macro': 0.299159959537318, 'mcc_per_class': [-0.02706022807597063, 0.005546829226315484, 0.05569180042689723], 'mcc_overall': -0.004020495965277185}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 10, Train Loss: 1015.238, Train metrics: {'f1_per_class': array([0.24842767, 0.25581395, 0.5223301 ]), 'f1_micro': 0.37043330501274424, 'f1_macro': 0.3421905745105753, 'precision_per_class': array([0.40932642, 0.52694611, 0.44536424]), 'precision_micro': 0.45228215767634855, 'precision_macro': 0.4605455903551645, 'recall_per_class': array([0.17832957, 0.16890595, 0.6314554 ]), 'recall_micro': 0.31366906474820144, 'recall_macro': 0.32623030675436565, 'mcc_per_class': [-0.028247307490487383, 0.010337873964723493, 0.059208957744077105], 'mcc_overall': -0.00823334379669584}\n",
      "            Val Loss:   335.647, Val F1:   {'f1_per_class': array([0.22222222, 0.26966292, 0.48780488]), 'f1_micro': 0.352112676056338, 'f1_macro': 0.32656334053977243, 'precision_per_class': array([0.47058824, 0.5       , 0.42857143]), 'precision_micro': 0.45045045045045046, 'precision_macro': 0.4663865546218487, 'recall_per_class': array([0.14545455, 0.18461538, 0.56603774]), 'recall_micro': 0.28901734104046245, 'recall_macro': 0.2987025553063289, 'mcc_per_class': [0.027137799617199594, -0.015406852855221774, 0.017974451297806674], 'mcc_overall': -0.009345994612395984}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 11, Train Loss: 777.611, Train metrics: {'f1_per_class': array([0.25766871, 0.28691983, 0.50336215]), 'f1_micro': 0.37271214642262895, 'f1_macro': 0.3493168982190693, 'precision_per_class': array([0.40191388, 0.53684211, 0.42601626]), 'precision_micro': 0.4418145956607495, 'precision_macro': 0.4549240803412819, 'recall_per_class': array([0.18961625, 0.19577735, 0.61502347]), 'recall_micro': 0.32230215827338127, 'recall_macro': 0.33347235941589165, 'mcc_per_class': [-0.03731319754508547, 0.020705943858702912, 0.011738981148158932], 'mcc_overall': -0.023436297843186245}\n",
      "            Val Loss:   366.002, Val F1:   {'f1_per_class': array([0.21621622, 0.26966292, 0.36363636]), 'f1_micro': 0.2900763358778626, 'f1_macro': 0.2831718337336315, 'precision_per_class': array([0.42105263, 0.5       , 0.39130435]), 'precision_micro': 0.42696629213483145, 'precision_macro': 0.43745232646834475, 'recall_per_class': array([0.14545455, 0.18461538, 0.33962264]), 'recall_micro': 0.21965317919075145, 'recall_macro': 0.22323085719312133, 'mcc_per_class': [-0.013131747264432156, -0.015406852855221774, -0.045053200422712276], 'mcc_overall': -0.03420238684290089}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(326.95761489868164,\n",
       " {'f1_per_class': array([0.31707317, 0.19277108, 0.34615385]),\n",
       "  'f1_micro': 0.2899628252788104,\n",
       "  'f1_macro': 0.2853327004076343,\n",
       "  'precision_per_class': array([0.5       , 0.44444444, 0.35294118]),\n",
       "  'precision_micro': 0.4105263157894737,\n",
       "  'precision_macro': 0.43246187363834426,\n",
       "  'recall_per_class': array([0.23214286, 0.12307692, 0.33962264]),\n",
       "  'recall_micro': 0.22413793103448276,\n",
       "  'recall_macro': 0.23161414057640473,\n",
       "  'mcc_per_class': [0.06034816410142874,\n",
       "   -0.054766967746567065,\n",
       "   -0.10695236098677958],\n",
       "  'mcc_overall': -0.053416057880103555})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GAT PCA\n",
    "trained_gat_pca = train(gat_pca, opt_gat_pca, train_loader_pca, val_loader_pca, model_arch=\"GAT\", scheduler=scheduler_gat_pca, epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "trained_gat_pca.eval()\n",
    "val_loss, val_metrics = evaluate(trained_gat_pca, test_loader_pca, model_arch=\"GAT\")\n",
    "val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c031e0f3-5eff-45c0-b8fa-c573fb3aea60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 507.109, Train metrics: {'f1_per_class': array([0.47276688, 0.50898204, 0.45022624]), 'f1_micro': 0.47860199714693297, 'f1_macro': 0.47732505493454186, 'precision_per_class': array([0.45684211, 0.53014553, 0.43449782]), 'precision_micro': 0.47454031117397455, 'precision_macro': 0.47382848400085814, 'recall_per_class': array([0.48984199, 0.48944338, 0.46713615]), 'recall_micro': 0.4827338129496403, 'recall_macro': 0.48214050493657523, 'mcc_per_class': [0.0354078624077418, 0.02823880597868139, 0.02420454739836681], 'mcc_overall': 0.030475618527708043}\n",
      "            Val Loss:   67.757, Val F1:   {'f1_per_class': array([0.32608696, 0.51666667, 0.58108108]), 'f1_micro': 0.49444444444444446, 'f1_macro': 0.474611568089829, 'precision_per_class': array([0.40540541, 0.56363636, 0.45263158]), 'precision_micro': 0.47593582887700536, 'precision_macro': 0.4738911159963792, 'recall_per_class': array([0.27272727, 0.47692308, 0.81132075]), 'recall_micro': 0.5144508670520231, 'recall_macro': 0.520323701455777, 'mcc_per_class': [-0.040435388280158746, 0.08411934431687783, 0.11346365243504059], 'mcc_overall': 0.03627344244986539}\n",
      "*****\n",
      "Validation loss decreased (inf -> 67.7569). Saving model...\n",
      "Epoch 2, Train Loss: 451.874, Train metrics: {'f1_per_class': array([0.42615558, 0.46088795, 0.49140546]), 'f1_micro': 0.4606661941885188, 'f1_macro': 0.4594829966431678, 'precision_per_class': array([0.42567568, 0.51294118, 0.43161634]), 'precision_micro': 0.45391061452513964, 'precision_macro': 0.4567443977254864, 'recall_per_class': array([0.42663657, 0.4184261 , 0.57042254]), 'recall_micro': 0.4676258992805755, 'recall_macro': 0.47182840256895303, 'mcc_per_class': [-0.02230159869959453, -0.004072961162357131, 0.02327367274632975], 'mcc_overall': -0.00831659267696623}\n",
      "            Val Loss:   127.097, Val F1:   {'f1_per_class': array([0.46666667, 0.39622642, 0.46315789]), 'f1_micro': 0.4423676012461059, 'f1_macro': 0.4420169921659494, 'precision_per_class': array([0.43076923, 0.51219512, 0.52380952]), 'precision_micro': 0.4797297297297297, 'precision_macro': 0.48892462550999144, 'recall_per_class': array([0.50909091, 0.32307692, 0.41509434]), 'recall_micro': 0.41040462427745666, 'recall_macro': 0.4157540572634912, 'mcc_per_class': [-0.011944438619012863, -0.005111291078361684, 0.1477845350613803], 'mcc_overall': 0.035515701884054285}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 3, Train Loss: 390.673, Train metrics: {'f1_per_class': array([0.46636771, 0.52432969, 0.4676504 ]), 'f1_micro': 0.48776978417266187, 'f1_macro': 0.48611593414507426, 'precision_per_class': array([0.46325167, 0.54320988, 0.45274725]), 'precision_micro': 0.48776978417266187, 'precision_macro': 0.48640293322302725, 'recall_per_class': array([0.46952596, 0.50671785, 0.48356808]), 'recall_micro': 0.48776978417266187, 'recall_macro': 0.4866039615910749, 'mcc_per_class': [0.04516614912337395, 0.053671344390021795, 0.05749380434024959], 'mcc_overall': 0.05441616274843789}\n",
      "            Val Loss:   105.300, Val F1:   {'f1_per_class': array([0.38834951, 0.58992806, 0.58139535]), 'f1_micro': 0.5362318840579711, 'f1_macro': 0.5198909736514242, 'precision_per_class': array([0.41666667, 0.55405405, 0.42016807]), 'precision_micro': 0.4605809128630705, 'precision_macro': 0.46362959598253717, 'recall_per_class': array([0.36363636, 0.63076923, 0.94339623]), 'recall_micro': 0.6416184971098265, 'recall_macro': 0.6459339402735629, 'mcc_per_class': [-0.03138360340042792, 0.09114038744344889, -0.0038992059490723405], 'mcc_overall': 0.007744209389103322}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Epoch 4, Train Loss: 388.321, Train metrics: {'f1_per_class': array([0.37712895, 0.56363636, 0.53667954]), 'f1_micro': 0.5023664638269101, 'f1_macro': 0.49248161802906326, 'precision_per_class': array([0.40897098, 0.53540587, 0.4557377 ]), 'precision_micro': 0.47385204081632654, 'precision_macro': 0.46670485112158927, 'recall_per_class': array([0.34988713, 0.5950096 , 0.65258216]), 'recall_micro': 0.5345323741007194, 'recall_macro': 0.5324929632454133, 'mcc_per_class': [-0.04558831352507988, 0.046502550101333924, 0.08585686992646549], 'mcc_overall': 0.03230783286216053}\n",
      "            Val Loss:   119.406, Val F1:   {'f1_per_class': array([0.55033557, 0.6519337 , 0.28205128]), 'f1_micro': 0.5441176470588235, 'f1_macro': 0.49477351805951314, 'precision_per_class': array([0.43617021, 0.50862069, 0.44      ]), 'precision_micro': 0.4723404255319149, 'precision_macro': 0.46159696747370993, 'recall_per_class': array([0.74545455, 0.90769231, 0.20754717]), 'recall_micro': 0.6416184971098265, 'recall_macro': 0.6202313409860579, 'mcc_per_class': [-0.0011671066839939518, -0.04942597837522774, 0.0195164030680492], 'mcc_overall': 0.03774352723752728}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Epoch 5, Train Loss: 409.236, Train metrics: {'f1_per_class': array([0.48510638, 0.60224331, 0.45558087]), 'f1_micro': 0.5220020154517971, 'f1_macro': 0.5143101872611345, 'precision_per_class': array([0.45875252, 0.54702194, 0.44247788]), 'precision_micro': 0.4896030245746692, 'precision_macro': 0.4827507782568019, 'recall_per_class': array([0.51467269, 0.66986564, 0.46948357]), 'recall_micro': 0.5589928057553957, 'recall_macro': 0.5513406324332025, 'mcc_per_class': [0.04077169168678427, 0.08293168344597011, 0.03844850350718234], 'mcc_overall': 0.06583338101360225}\n",
      "            Val Loss:   160.737, Val F1:   {'f1_per_class': array([0.43243243, 0.49557522, 0.52336449]), 'f1_micro': 0.48338368580060426, 'f1_macro': 0.4837907132175596, 'precision_per_class': array([0.42857143, 0.58333333, 0.51851852]), 'precision_micro': 0.5063291139240507, 'precision_macro': 0.5101410934744268, 'recall_per_class': array([0.43636364, 0.43076923, 0.52830189]), 'recall_micro': 0.4624277456647399, 'recall_macro': 0.4651449179751066, 'mcc_per_class': [-0.014313125339290265, 0.10589382779262747, 0.17171638464959982], 'mcc_overall': 0.08276671986567025}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Epoch 6, Train Loss: 610.310, Train metrics: {'f1_per_class': array([0.456621  , 0.43160377, 0.48042328]), 'f1_micro': 0.45710003746721617, 'f1_macro': 0.4562160195247987, 'precision_per_class': array([0.46189376, 0.55963303, 0.43737958]), 'precision_micro': 0.4769351055512119, 'precision_macro': 0.48630212268833856, 'recall_per_class': array([0.45146727, 0.3512476 , 0.53286385]), 'recall_micro': 0.43884892086330934, 'recall_macro': 0.44519290638534575, 'mcc_per_class': [0.04136712062333186, 0.06129149306811862, 0.033310825902544305], 'mcc_overall': 0.03195083179355417}\n",
      "            Val Loss:   265.293, Val F1:   {'f1_per_class': array([0.53543307, 0.43564356, 0.47863248]), 'f1_micro': 0.48695652173913045, 'f1_macro': 0.48323637128501873, 'precision_per_class': array([0.47222222, 0.61111111, 0.4375    ]), 'precision_micro': 0.4883720930232558, 'precision_macro': 0.5069444444444445, 'recall_per_class': array([0.61818182, 0.33846154, 0.52830189]), 'recall_micro': 0.48554913294797686, 'recall_macro': 0.4949817478119365, 'mcc_per_class': [0.08315174410687586, 0.12052847062843809, 0.03470992791753517], 'mcc_overall': 0.05630704306877417}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Epoch 7, Train Loss: 736.160, Train metrics: {'f1_per_class': array([0.5059761 , 0.57549858, 0.50052576]), 'f1_micro': 0.5285904255319149, 'f1_macro': 0.5273334778238402, 'precision_per_class': array([0.45276292, 0.56954887, 0.45333333]), 'precision_micro': 0.49134734239802225, 'precision_macro': 0.4918817096216477, 'recall_per_class': array([0.57336343, 0.5815739 , 0.55868545]), 'recall_micro': 0.5719424460431655, 'recall_macro': 0.5712075911712661, 'mcc_per_class': [0.03281665431875312, 0.11433045671614714, 0.06728993930644969], 'mcc_overall': 0.07094100724995071}\n",
      "            Val Loss:   263.782, Val F1:   {'f1_per_class': array([0.56338028, 0.64516129, 0.34782609]), 'f1_micro': 0.5449871465295629, 'f1_macro': 0.5187892196564144, 'precision_per_class': array([0.45977011, 0.55555556, 0.41025641]), 'precision_micro': 0.49074074074074076, 'precision_macro': 0.47519402691816487, 'recall_per_class': array([0.72727273, 0.76923077, 0.30188679]), 'recall_micro': 0.6127167630057804, 'recall_macro': 0.599463429652109, 'mcc_per_class': [0.07005477321750564, 0.12555049023795636, -0.014075987789142194], 'mcc_overall': 0.07664421830777192}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Epoch 8, Train Loss: 542.930, Train metrics: {'f1_per_class': array([0.46424642, 0.53620352, 0.47250509]), 'f1_micro': 0.4922760041194645, 'f1_macro': 0.49098501293235036, 'precision_per_class': array([0.4527897 , 0.54690619, 0.41726619]), 'precision_micro': 0.47078135259356535, 'precision_macro': 0.4723206914153086, 'recall_per_class': array([0.47629797, 0.52591171, 0.54460094]), 'recall_micro': 0.5158273381294964, 'recall_macro': 0.5156035385392621, 'mcc_per_class': [0.027227673184745917, 0.062619426324784, -0.009176068523132104], 'mcc_overall': 0.02517349176238972}\n",
      "            Val Loss:   114.409, Val F1:   {'f1_per_class': array([0.50847458, 0.65789474, 0.56692913]), 'f1_micro': 0.5843828715365239, 'f1_macro': 0.5777661489905198, 'precision_per_class': array([0.47619048, 0.57471264, 0.48648649]), 'precision_micro': 0.5178571428571429, 'precision_macro': 0.5124632021183745, 'recall_per_class': array([0.54545455, 0.76923077, 0.67924528]), 'recall_micro': 0.6705202312138728, 'recall_macro': 0.6646435325680607, 'mcc_per_class': [0.08001280307281942, 0.17585154527658867, 0.15912978448467927], 'mcc_overall': 0.14569514085206497}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(215.9801540374756,\n",
       " {'f1_per_class': array([0.50406504, 0.57142857, 0.54961832]),\n",
       "  'f1_micro': 0.5431472081218274,\n",
       "  'f1_macro': 0.5417039775632216,\n",
       "  'precision_per_class': array([0.46268657, 0.53333333, 0.46153846]),\n",
       "  'precision_micro': 0.4863636363636364,\n",
       "  'precision_macro': 0.48585278734532467,\n",
       "  'recall_per_class': array([0.55357143, 0.61538462, 0.67924528]),\n",
       "  'recall_micro': 0.6149425287356322,\n",
       "  'recall_macro': 0.6160671089916373,\n",
       "  'mcc_per_class': [0.04627383716579194,\n",
       "   0.051709344744503984,\n",
       "   0.11312926733198664],\n",
       "  'mcc_overall': 0.0696292488302312})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GAT no PCA\n",
    "trained_gat_nopca = train(gat_nopca, opt_gat_nopca, train_loader_nopca, val_loader_nopca, model_arch=\"GAT\", epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "val_loss, val_metrics = evaluate(trained_gat_nopca, test_loader_nopca, model_arch=\"GAT\")\n",
    "val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1679ef71-ce66-467c-aea3-e3cf8b2a3577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 13.200, Train metrics: {'f1_per_class': array([0.45073375, 0.49245283, 0.45474138]), 'f1_micro': 0.46702923181509176, 'f1_macro': 0.4659759873731897, 'precision_per_class': array([0.42074364, 0.48423006, 0.42031873]), 'precision_micro': 0.4426546391752577, 'precision_macro': 0.4417641402266503, 'recall_per_class': array([0.48532731, 0.50095969, 0.49530516]), 'recall_micro': 0.49424460431654677, 'recall_macro': 0.4938640569957577, 'mcc_per_class': [-0.035526865704130446, -0.06650222435516744, -0.002104267941422873], 'mcc_overall': -0.03212782667556666}\n",
      "            Val Loss:   1.194, Val F1:   {'f1_per_class': array([0.03508772, 0.12820513, 0.16129032]), 'f1_micro': 0.1116751269035533, 'f1_macro': 0.10819439002800631, 'precision_per_class': array([0.5       , 0.38461538, 0.55555556]), 'precision_micro': 0.4583333333333333, 'precision_macro': 0.48005698005698005, 'recall_per_class': array([0.01818182, 0.07692308, 0.09433962]), 'recall_micro': 0.06358381502890173, 'recall_macro': 0.06314817258213486, 'mcc_per_class': [0.01625861784302288, -0.08908528345577679, 0.07580151406774355], 'mcc_overall': 0.0003456559335625107}\n",
      "*****\n",
      "Validation loss decreased (inf -> 1.1937). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 5.423, Train metrics: {'f1_per_class': array([0.47200878, 0.56485741, 0.4381161 ]), 'f1_micro': 0.4960494675369289, 'f1_macro': 0.4916607626764005, 'precision_per_class': array([0.45940171, 0.54240283, 0.41067762]), 'precision_micro': 0.4746877054569362, 'precision_macro': 0.47082738477554936, 'recall_per_class': array([0.48532731, 0.58925144, 0.46948357]), 'recall_micro': 0.5194244604316547, 'recall_macro': 0.5146874404614055, 'mcc_per_class': [0.03970800745816229, 0.06109059164809686, -0.02086594157703804], 'mcc_overall': 0.033003642088767694}\n",
      "            Val Loss:   1.143, Val F1:   {'f1_per_class': array([0.        , 0.16438356, 0.07272727]), 'f1_micro': 0.08743169398907104, 'f1_macro': 0.07903694479036945, 'precision_per_class': array([0.  , 0.75, 1.  ]), 'precision_micro': 0.8, 'precision_macro': 0.5833333333333334, 'recall_per_class': array([0.        , 0.09230769, 0.03773585]), 'recall_micro': 0.046242774566473986, 'recall_macro': 0.043347847121432026, 'mcc_per_class': [0.0, 0.1219844324717202, 0.1490485139988275], 'mcc_overall': 0.11326882227345707}\n",
      "*****\n",
      "Validation loss decreased (1.1937 -> 1.1430). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 3.086, Train metrics: {'f1_per_class': array([0.44108108, 0.55463728, 0.47150838]), 'f1_micro': 0.4929529047782743, 'f1_macro': 0.48907558095978615, 'precision_per_class': array([0.42323651, 0.53169014, 0.44989339]), 'precision_micro': 0.4720210664911126, 'precision_macro': 0.4682733485199299, 'recall_per_class': array([0.46049661, 0.57965451, 0.49530516]), 'recall_micro': 0.5158273381294964, 'recall_macro': 0.5118187629571186, 'mcc_per_class': [-0.028749036118834233, 0.03706445539491774, 0.053744334907199647], 'mcc_overall': 0.027599384047265616}\n",
      "            Val Loss:   0.786, Val F1:   {'f1_per_class': array([0.03508772, 0.63354037, 0.        ]), 'f1_micro': 0.3837638376383764, 'f1_macro': 0.222876030656351, 'precision_per_class': array([0.5    , 0.53125, 0.     ]), 'precision_micro': 0.5306122448979592, 'precision_macro': 0.34375, 'recall_per_class': array([0.01818182, 0.78461538, 0.        ]), 'recall_micro': 0.30057803468208094, 'recall_macro': 0.2675990675990676, 'mcc_per_class': [0.01625861784302288, 0.05504211387770371, 0.0], 'mcc_overall': 0.08661503975918469}\n",
      "*****\n",
      "Validation loss decreased (1.1430 -> 0.7863). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 1.418, Train metrics: {'f1_per_class': array([0.4756871 , 0.48804781, 0.44064171]), 'f1_micro': 0.468630849220104, 'f1_macro': 0.4681255411963224, 'precision_per_class': array([0.4473161 , 0.50724638, 0.40471513]), 'precision_micro': 0.45217391304347826, 'precision_macro': 0.4530925359642304, 'recall_per_class': array([0.50790068, 0.47024952, 0.48356808]), 'recall_micro': 0.48633093525179855, 'recall_macro': 0.48723942415727484, 'mcc_per_class': [0.01832260950057769, -0.015472828636228932, -0.03395361152939958], 'mcc_overall': -0.012106338740421154}\n",
      "            Val Loss:   0.846, Val F1:   {'f1_per_class': array([0.06451613, 0.66304348, 0.03703704]), 'f1_micro': 0.4266666666666667, 'f1_macro': 0.25486554811005485, 'precision_per_class': array([0.28571429, 0.51260504, 1.        ]), 'precision_micro': 0.5039370078740157, 'precision_macro': 0.5994397759103641, 'recall_per_class': array([0.03636364, 0.93846154, 0.01886792]), 'recall_micro': 0.3699421965317919, 'recall_macro': 0.3312310331178256, 'mcc_per_class': [-0.07374262977586608, -0.026961992496638103, 0.10497079557919099], 'mcc_overall': 0.06605563247522502}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 0.886, Train metrics: {'f1_per_class': array([0.38451935, 0.48407643, 0.34361233]), 'f1_micro': 0.41171617161716173, 'f1_macro': 0.40406937291142225, 'precision_per_class': array([0.4301676 , 0.5415677 , 0.45882353]), 'precision_micro': 0.4825918762088975, 'precision_macro': 0.4768529410463744, 'recall_per_class': array([0.3476298 , 0.43761996, 0.27464789]), 'recall_micro': 0.35899280575539566, 'recall_macro': 0.35329921525865227, 'mcc_per_class': [-0.011957044979897911, 0.04434580681492243, 0.04405837862867426], 'mcc_overall': 0.0350753502993107}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.47787611, 0.60689655, 0.3908046 ]), 'f1_micro': 0.5101449275362319, 'f1_macro': 0.49185908520665916, 'precision_per_class': array([0.46551724, 0.55      , 0.5       ]), 'precision_micro': 0.5116279069767442, 'precision_macro': 0.5051724137931034, 'recall_per_class': array([0.49090909, 0.67692308, 0.32075472]), 'recall_micro': 0.5086705202312138, 'recall_macro': 0.4961956282711, 'mcc_per_class': [0.0540202871868315, 0.09005605895851808, 0.09773411393894649], 'mcc_overall': 0.09896047449161312}\n",
      "*****\n",
      "Validation loss decreased (0.7863 -> 0.7509). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 0.766, Train metrics: {'f1_per_class': array([0.26984127, 0.34519104, 0.21908127]), 'f1_micro': 0.2843989769820972, 'f1_macro': 0.27803786092309674, 'precision_per_class': array([0.45454545, 0.55042017, 0.44285714]), 'precision_micro': 0.4920353982300885, 'precision_macro': 0.48260758848994145, 'recall_per_class': array([0.19187359, 0.25143954, 0.14553991]), 'recall_micro': 0.2, 'recall_macro': 0.19628434487182692, 'mcc_per_class': [0.01571302745286593, 0.038958449318402644, 0.01745031945434038], 'mcc_overall': 0.032402994527223936}\n",
      "            Val Loss:   0.750, Val F1:   {'f1_per_class': array([0.        , 0.64444444, 0.03703704]), 'f1_micro': 0.4083044982698962, 'f1_macro': 0.22716049382716053, 'precision_per_class': array([0.        , 0.50434783, 1.        ]), 'precision_micro': 0.5086206896551724, 'precision_macro': 0.5014492753623189, 'recall_per_class': array([0.        , 0.89230769, 0.01886792]), 'recall_micro': 0.34104046242774566, 'recall_macro': 0.30372520561199806, 'mcc_per_class': [0.0, -0.07456750509567203, 0.10497079557919099], 'mcc_overall': 0.0680461481565953}\n",
      "*****\n",
      "Validation loss decreased (0.7509 -> 0.7503). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.1827957 , 0.23809524, 0.17153285]), 'f1_micro': 0.20022497187851518, 'f1_macro': 0.19747459457843256, 'precision_per_class': array([0.44347826, 0.52980132, 0.3852459 ]), 'precision_micro': 0.4587628865979381, 'precision_macro': 0.4528418290040736, 'recall_per_class': array([0.11512415, 0.15355086, 0.11032864]), 'recall_micro': 0.12805755395683452, 'recall_macro': 0.12633455190671078, 'mcc_per_class': [0.003825619356596051, 0.012132243408160954, -0.027097809165408215], 'mcc_overall': 0.00036187191768689}\n",
      "            Val Loss:   0.764, Val F1:   {'f1_per_class': array([0.        , 0.68085106, 0.03703704]), 'f1_micro': 0.4377104377104377, 'f1_macro': 0.23929603362227478, 'precision_per_class': array([0.       , 0.5203252, 1.       ]), 'precision_micro': 0.5241935483870968, 'precision_macro': 0.5067750677506776, 'recall_per_class': array([0.        , 0.98461538, 0.01886792]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.33449443638122883, 'mcc_per_class': [0.0, 0.057044570137671634, 0.10497079557919099], 'mcc_overall': 0.09329286446651108}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.11045365, 0.30958904, 0.10766046]), 'f1_micro': 0.1941860465116279, 'f1_macro': 0.17590104849920674, 'precision_per_class': array([0.4375    , 0.54066986, 0.45614035]), 'precision_micro': 0.5060606060606061, 'precision_macro': 0.4781034024455077, 'recall_per_class': array([0.06320542, 0.2168906 , 0.06103286]), 'recall_micro': 0.12014388489208633, 'recall_macro': 0.11370962548886188, 'mcc_per_class': [-0.00035629659258212023, 0.025882196971007077, 0.01721485501595582], 'mcc_overall': 0.03349821400279434}\n",
      "            Val Loss:   0.749, Val F1:   {'f1_per_class': array([0.46551724, 0.63225806, 0.        ]), 'f1_micro': 0.4691358024691358, 'f1_macro': 0.36592510196514644, 'precision_per_class': array([0.44262295, 0.54444444, 0.        ]), 'precision_micro': 0.5033112582781457, 'precision_macro': 0.32902246508803884, 'recall_per_class': array([0.49090909, 0.75384615, 0.        ]), 'recall_micro': 0.4393063583815029, 'recall_macro': 0.4149184149184149, 'mcc_per_class': [0.011944438619012863, 0.09039635297132857, 0.0], 'mcc_overall': 0.07471474925762721}\n",
      "*****\n",
      "Validation loss decreased (0.7503 -> 0.7491). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.10358566, 0.09171076, 0.03508772]), 'f1_micro': 0.07868852459016394, 'f1_macro': 0.07679471168206287, 'precision_per_class': array([0.44067797, 0.56521739, 0.26666667]), 'precision_micro': 0.4444444444444444, 'precision_macro': 0.4241873413575698, 'recall_per_class': array([0.05869074, 0.04990403, 0.01877934]), 'recall_micro': 0.04316546762589928, 'recall_macro': 0.04245803945139023, 'mcc_per_class': [0.0012533309300216161, 0.021793576671892256, -0.054787330468353636], 'mcc_overall': -0.005998474296425931}\n",
      "            Val Loss:   0.756, Val F1:   {'f1_per_class': array([0.53030303, 0.24390244, 0.        ]), 'f1_micro': 0.33707865168539325, 'f1_macro': 0.2580684897758068, 'precision_per_class': array([0.45454545, 0.58823529, 0.        ]), 'precision_micro': 0.4787234042553192, 'precision_macro': 0.34759358288770054, 'recall_per_class': array([0.63636364, 0.15384615, 0.        ]), 'recall_micro': 0.26011560693641617, 'recall_macro': 0.2634032634032634, 'mcc_per_class': [0.04559152581164996, 0.0571837184694056, 0.0], 'mcc_overall': 0.02430963952414427}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.09411765, 0.11538462, 0.03139013]), 'f1_micro': 0.08376963350785341, 'f1_macro': 0.08029746565752897, 'precision_per_class': array([0.35820896, 0.64705882, 0.35      ]), 'precision_micro': 0.463768115942029, 'precision_macro': 0.4517559262510975, 'recall_per_class': array([0.05417607, 0.06333973, 0.01643192]), 'recall_micro': 0.0460431654676259, 'recall_macro': 0.04464924280112686, 'mcc_per_class': [-0.0429397395713225, 0.06075192882674995, -0.020532024481659927], 'mcc_overall': 0.002399521271271747}\n",
      "            Val Loss:   0.757, Val F1:   {'f1_per_class': array([0.59171598, 0.31578947, 0.        ]), 'f1_micro': 0.41009463722397477, 'f1_macro': 0.30250181667185716, 'precision_per_class': array([0.43859649, 0.5       , 0.        ]), 'precision_micro': 0.4513888888888889, 'precision_macro': 0.3128654970760234, 'recall_per_class': array([0.90909091, 0.23076923, 0.        ]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3799533799533799, 'mcc_per_class': [0.012979790623128613, -0.017755520605710874, 0.0], 'mcc_overall': -0.009893203653643459}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.09716599, 0.02941176, 0.04081633]), 'f1_micro': 0.05544286680189317, 'f1_macro': 0.05579802771310954, 'precision_per_class': array([0.47058824, 0.34782609, 0.6       ]), 'precision_micro': 0.4606741573033708, 'precision_macro': 0.47280477408354643, 'recall_per_class': array([0.05417607, 0.01535509, 0.02112676]), 'recall_micro': 0.029496402877697843, 'recall_macro': 0.030219306390168035, 'mcc_per_class': [0.015054963757654135, -0.05113852177652391, 0.04439672897425259], 'mcc_overall': 0.0008312293797365712}\n",
      "            Val Loss:   0.746, Val F1:   {'f1_per_class': array([0.03571429, 0.63013699, 0.13559322]), 'f1_micro': 0.39080459770114945, 'f1_macro': 0.26714816411821285, 'precision_per_class': array([1.        , 0.56790123, 0.66666667]), 'precision_micro': 0.5795454545454546, 'precision_macro': 0.7448559670781892, 'recall_per_class': array([0.01818182, 0.70769231, 0.0754717 ]), 'recall_micro': 0.2947976878612717, 'recall_macro': 0.2671152746624445, 'mcc_per_class': [0.10162318990896088, 0.13967676209825888, 0.1114415980464014], 'mcc_overall': 0.13475454663030342}\n",
      "*****\n",
      "Validation loss decreased (0.7491 -> 0.7464). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.09716599, 0.0511883 , 0.10084034]), 'f1_micro': 0.08174027686222808, 'f1_macro': 0.08306487595149081, 'precision_per_class': array([0.47058824, 0.53846154, 0.48      ]), 'precision_micro': 0.4881889763779528, 'precision_macro': 0.4963499245852187, 'recall_per_class': array([0.05417607, 0.0268714 , 0.05633803]), 'recall_micro': 0.04460431654676259, 'recall_macro': 0.04579516718513618, 'mcc_per_class': [0.015054963757654135, 0.007519377497311532, 0.02708624302250848], 'mcc_overall': 0.01254370433802682}\n",
      "            Val Loss:   0.749, Val F1:   {'f1_per_class': array([0.51006711, 0.08695652, 0.17647059]), 'f1_micro': 0.32867132867132864, 'f1_macro': 0.2578314080227948, 'precision_per_class': array([0.40425532, 0.75      , 0.4       ]), 'precision_micro': 0.415929203539823, 'precision_macro': 0.5180851063829787, 'recall_per_class': array([0.69090909, 0.04615385, 0.11320755]), 'recall_micro': 0.27167630057803466, 'recall_macro': 0.2834234947442495, 'mcc_per_class': [-0.11145868832142239, 0.08483020186740119, -0.015365904271790187], 'mcc_overall': -0.05471283293537811}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.07984032, 0.1048951 , 0.04910714]), 'f1_micro': 0.08021038790269559, 'f1_macro': 0.07794752237117507, 'precision_per_class': array([0.34482759, 0.58823529, 0.5       ]), 'precision_micro': 0.46564885496183206, 'precision_macro': 0.47768762677484783, 'recall_per_class': array([0.04514673, 0.05758157, 0.0258216 ]), 'recall_micro': 0.04388489208633094, 'recall_macro': 0.04284996566759571, 'mcc_per_class': [-0.046416064505300364, 0.03362286520314385, 0.023751858136809384], 'mcc_overall': 0.0031370271805952667}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.38596491, 0.69189189, 0.07142857]), 'f1_micro': 0.49577464788732395, 'f1_macro': 0.3830951252003883, 'precision_per_class': array([0.37288136, 0.53333333, 0.66666667]), 'precision_micro': 0.4835164835164835, 'precision_macro': 0.5242937853107345, 'recall_per_class': array([0.4       , 0.98461538, 0.03773585]), 'recall_micro': 0.5086705202312138, 'recall_macro': 0.47411707789066276, 'mcc_per_class': [-0.12038911293867528, 0.15624858133025568, 0.07783418828458256], 'mcc_overall': 0.04998826286062458}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 14, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.1167002 , 0.07104796, 0.0969697 ]), 'f1_micro': 0.09389067524115756, 'f1_macro': 0.09490595184938867, 'precision_per_class': array([0.53703704, 0.47619048, 0.34782609]), 'precision_micro': 0.44242424242424244, 'precision_macro': 0.45368453339467835, 'recall_per_class': array([0.06546275, 0.03838772, 0.05633803]), 'recall_micro': 0.05251798561151079, 'recall_macro': 0.053396166016751595, 'mcc_per_class': [0.047328586950380565, -0.016305254582390304, -0.04030738612960992], 'mcc_overall': -0.0076386650786497345}\n",
      "            Val Loss:   0.757, Val F1:   {'f1_per_class': array([0.59036145, 0.        , 0.46616541]), 'f1_micro': 0.43956043956043955, 'f1_macro': 0.3521756197723224, 'precision_per_class': array([0.44144144, 0.        , 0.3875    ]), 'precision_micro': 0.418848167539267, 'precision_macro': 0.2763138138138138, 'recall_per_class': array([0.89090909, 0.        , 0.58490566]), 'recall_micro': 0.4624277456647399, 'recall_macro': 0.4919382504288164, 'mcc_per_class': [0.02706022807597063, 0.0, -0.08851628788932883], 'mcc_overall': -0.07875635878154352}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 15, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.05833333, 0.02641509, 0.02272727]), 'f1_micro': 0.03586206896551724, 'f1_macro': 0.035825233466742906, 'precision_per_class': array([0.37837838, 0.77777778, 0.35714286]), 'precision_micro': 0.43333333333333335, 'precision_macro': 0.5044330044330044, 'recall_per_class': array([0.03160271, 0.0134357 , 0.01173709]), 'recall_micro': 0.01870503597122302, 'recall_macro': 0.01892516619376847, 'mcc_per_class': [-0.02349143855461549, 0.04976938856460541, -0.015412374513939337], 'mcc_overall': -0.007116195458944732}\n",
      "            Val Loss:   0.765, Val F1:   {'f1_per_class': array([0.575     , 0.08823529, 0.57988166]), 'f1_micro': 0.49370277078085645, 'f1_macro': 0.4143723169741269, 'precision_per_class': array([0.43809524, 1.        , 0.42241379]), 'precision_micro': 0.4375, 'precision_macro': 0.6201696770662287, 'recall_per_class': array([0.83636364, 0.04615385, 0.9245283 ]), 'recall_micro': 0.5664739884393064, 'recall_macro': 0.602348594801425, 'mcc_per_class': [0.007156562669645132, 0.15129212079991172, 0.012272837629134187], 'mcc_overall': -0.0488318878679998}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 16, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.0516129 , 0.01876173, 0.06181015]), 'f1_micro': 0.042729152308752585, 'f1_macro': 0.04406159460999734, 'precision_per_class': array([0.54545455, 0.41666667, 0.51851852]), 'precision_micro': 0.5081967213114754, 'precision_macro': 0.49354657687991016, 'recall_per_class': array([0.02708804, 0.00959693, 0.03286385]), 'recall_micro': 0.022302158273381296, 'recall_macro': 0.02318293828845508, 'mcc_per_class': [0.03224667363322482, -0.021637340605573742, 0.032592025829969895], 'mcc_overall': 0.014349188264448615}\n",
      "            Val Loss:   0.768, Val F1:   {'f1_per_class': array([0.47482014, 0.        , 0.56804734]), 'f1_micro': 0.4343163538873995, 'f1_macro': 0.3476224937209995, 'precision_per_class': array([0.39285714, 0.        , 0.4137931 ]), 'precision_micro': 0.405, 'precision_macro': 0.26888341543513955, 'recall_per_class': array([0.6       , 0.        , 0.90566038]), 'recall_micro': 0.4682080924855491, 'recall_macro': 0.5018867924528302, 'mcc_per_class': [-0.12447071039597578, 0.0, -0.04720322165051611], 'mcc_overall': -0.11206662027799287}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 17, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.02591793, 0.04104478, 0.04474273]), 'f1_micro': 0.03734439834024896, 'f1_macro': 0.03723514399725514, 'precision_per_class': array([0.3       , 0.73333333, 0.47619048]), 'precision_micro': 0.48214285714285715, 'precision_macro': 0.5031746031746032, 'recall_per_class': array([0.01354402, 0.02111324, 0.02347418]), 'recall_micro': 0.019424460431654675, 'recall_macro': 0.019377146741480923, 'mcc_per_class': [-0.03956384991806134, 0.053531652345935585, 0.016171228101666556], 'mcc_overall': 0.006565270591725581}\n",
      "            Val Loss:   0.748, Val F1:   {'f1_per_class': array([0.03508772, 0.64615385, 0.5       ]), 'f1_micro': 0.47619047619047616, 'f1_macro': 0.3937471884840306, 'precision_per_class': array([0.5       , 0.64615385, 0.42666667]), 'precision_micro': 0.528169014084507, 'precision_macro': 0.5242735042735043, 'recall_per_class': array([0.01818182, 0.64615385, 0.60377358]), 'recall_micro': 0.43352601156069365, 'recall_macro': 0.42270308308044163, 'mcc_per_class': [0.01625861784302288, 0.2691046658259773, 0.014816982606474894], 'mcc_overall': 0.10976173719018471}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 18, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.05316973, 0.01506591, 0.04063205]), 'f1_micro': 0.0355434039644566, 'f1_macro': 0.03628923389946653, 'precision_per_class': array([0.2826087 , 0.4       , 0.52941176]), 'precision_micro': 0.3561643835616438, 'precision_macro': 0.40400682011935213, 'recall_per_class': array([0.02934537, 0.00767754, 0.02112676]), 'recall_micro': 0.01870503597122302, 'recall_macro': 0.01938322540335244, 'mcc_per_class': [-0.06845732611747524, -0.023065592231442743, 0.028616170731323616], 'mcc_overall': -0.032188853578469095}\n",
      "            Val Loss:   0.750, Val F1:   {'f1_per_class': array([0.        , 0.53658537, 0.20253165]), 'f1_micro': 0.3178294573643411, 'f1_macro': 0.24637233714109294, 'precision_per_class': array([0.        , 0.56896552, 0.30769231]), 'precision_micro': 0.4823529411764706, 'precision_macro': 0.2922192749778957, 'recall_per_class': array([0.        , 0.50769231, 0.1509434 ]), 'recall_micro': 0.23699421965317918, 'recall_macro': 0.21954523463957423, 'mcc_per_class': [-0.07872218936609646, 0.09811641616566942, -0.11665831123504888], 'mcc_overall': 0.026682709758339876}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.747761681675911,\n",
       " {'f1_per_class': array([0.        , 0.63157895, 0.27586207]),\n",
       "  'f1_micro': 0.391304347826087,\n",
       "  'f1_macro': 0.3024803387779794,\n",
       "  'precision_per_class': array([0.        , 0.61764706, 0.35294118]),\n",
       "  'precision_micro': 0.5294117647058824,\n",
       "  'precision_macro': 0.3235294117647059,\n",
       "  'recall_per_class': array([0.        , 0.64615385, 0.22641509]),\n",
       "  'recall_micro': 0.3103448275862069,\n",
       "  'recall_macro': 0.290856313497823,\n",
       "  'mcc_per_class': [0.0, 0.2273072095172018, -0.07894235325030699],\n",
       "  'mcc_overall': 0.0882693997132293})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# GIN PCA\n",
    "trained_gin_pca = train(gin_pca, opt_gin_pca, train_loader_pca, val_loader_pca, model_arch=\"GIN\", scheduler=scheduler_gin_pca, epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "trained_gin_pca.eval()\n",
    "val_loss, val_metrics = evaluate(trained_gin_pca, test_loader_pca, model_arch=\"GIN\")\n",
    "val_loss, val_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2063b65f-57fc-4af2-b40c-84ab01e132ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 12.525, Train metrics: {'f1_per_class': array([0.41939121, 0.51509434, 0.46069869]), 'f1_micro': 0.46804051694027243, 'f1_macro': 0.4650614119641298, 'precision_per_class': array([0.41891892, 0.50649351, 0.43061224]), 'precision_micro': 0.4548540393754243, 'precision_macro': 0.45200822343679486, 'recall_per_class': array([0.41986456, 0.52399232, 0.49530516]), 'recall_micro': 0.48201438848920863, 'recall_macro': 0.4797206821984919, 'mcc_per_class': [-0.03435232894293502, -0.01889749306670323, 0.018161873963814593], 'mcc_overall': -0.006705039817868339}\n",
      "            Val Loss:   1.511, Val F1:   {'f1_per_class': array([0.49315068, 0.35294118, 0.13559322]), 'f1_micro': 0.3793103448275862, 'f1_macro': 0.32722836058035937, 'precision_per_class': array([0.3956044 , 0.75      , 0.66666667]), 'precision_micro': 0.4700854700854701, 'precision_macro': 0.604090354090354, 'recall_per_class': array([0.65454545, 0.23076923, 0.0754717 ]), 'recall_micro': 0.3179190751445087, 'recall_macro': 0.32026212780929764, 'mcc_per_class': [-0.1329865150245352, 0.20349909648881673, 0.1114415980464014], 'mcc_overall': 0.016682424875830843}\n",
      "*****\n",
      "Validation loss decreased (inf -> 1.5110). Saving model...\n",
      "Epoch 2, Train Loss: 6.579, Train metrics: {'f1_per_class': array([0.47302905, 0.53594771, 0.41760722]), 'f1_micro': 0.47928791509756935, 'f1_macro': 0.4755279938459174, 'precision_per_class': array([0.43761996, 0.52181818, 0.40217391]), 'precision_micro': 0.457217504898759, 'precision_macro': 0.45387068549131476, 'recall_per_class': array([0.51467269, 0.55086372, 0.4342723 ]), 'recall_micro': 0.5035971223021583, 'recall_macro': 0.4999362367693923, 'mcc_per_class': [-0.0011639359528955431, 0.014177427438876252, -0.035511643888500755], 'mcc_overall': -0.002177468104020455}\n",
      "            Val Loss:   1.131, Val F1:   {'f1_per_class': array([0.20588235, 0.5952381 , 0.56790123]), 'f1_micro': 0.5175879396984925, 'f1_macro': 0.45634056091572434, 'precision_per_class': array([0.53846154, 0.48543689, 0.42201835]), 'precision_micro': 0.4577777777777778, 'precision_macro': 0.48197226009642513, 'recall_per_class': array([0.12727273, 0.76923077, 0.86792453]), 'recall_micro': 0.5953757225433526, 'recall_macro': 0.5881426749351278, 'mcc_per_class': [0.06972604857855821, -0.12888220849380094, 0.007096047704073131], 'mcc_overall': 0.0002575760938854431}\n",
      "*****\n",
      "Validation loss decreased (1.5110 -> 1.1306). Saving model...\n",
      "Epoch 3, Train Loss: 3.837, Train metrics: {'f1_per_class': array([0.45977011, 0.49060336, 0.48582996]), 'f1_micro': 0.4796096200766818, 'f1_macro': 0.47873447915454087, 'precision_per_class': array([0.46838407, 0.50612245, 0.42704626]), 'precision_micro': 0.4651791751183232, 'precision_macro': 0.46718426242207983, 'recall_per_class': array([0.45146727, 0.47600768, 0.56338028]), 'recall_micro': 0.4949640287769784, 'recall_macro': 0.49695174261878394, 'mcc_per_class': [0.052053255149148346, -0.017869873477221982, 0.012872398177192901], 'mcc_overall': 0.013484609247639976}\n",
      "            Val Loss:   0.923, Val F1:   {'f1_per_class': array([0.10344828, 0.68421053, 0.47706422]), 'f1_micro': 0.5266106442577031, 'f1_macro': 0.4215743407871149, 'precision_per_class': array([1.        , 0.52      , 0.46428571]), 'precision_micro': 0.5108695652173914, 'precision_macro': 0.6614285714285715, 'recall_per_class': array([0.05454545, 1.        , 0.49056604]), 'recall_micro': 0.5433526011560693, 'recall_macro': 0.5150371640937679, 'mcc_per_class': [0.1774417864629089, 0.09232870714969654, 0.07908758571034936], 'mcc_overall': 0.10399009273708182}\n",
      "*****\n",
      "Validation loss decreased (1.1306 -> 0.9233). Saving model...\n",
      "Epoch 4, Train Loss: 2.435, Train metrics: {'f1_per_class': array([0.45258621, 0.48715313, 0.53471503]), 'f1_micro': 0.4919748778785764, 'f1_macro': 0.4914847891461455, 'precision_per_class': array([0.43298969, 0.52433628, 0.47866419]), 'precision_micro': 0.47764227642276424, 'precision_macro': 0.47866338895246585, 'recall_per_class': array([0.47404063, 0.45489443, 0.6056338 ]), 'recall_micro': 0.5071942446043165, 'recall_macro': 0.5115229562174225, 'mcc_per_class': [-0.01004495543524664, 0.016202342801691604, 0.1240053503731441], 'mcc_overall': 0.03781199403829559}\n",
      "            Val Loss:   0.825, Val F1:   {'f1_per_class': array([0.17391304, 0.67724868, 0.56603774]), 'f1_micro': 0.5515587529976019, 'f1_macro': 0.4723998188586649, 'precision_per_class': array([0.42857143, 0.51612903, 0.4245283 ]), 'precision_micro': 0.4713114754098361, 'precision_macro': 0.4564095875720952, 'recall_per_class': array([0.10909091, 0.98461538, 0.8490566 ]), 'recall_micro': 0.6647398843930635, 'recall_macro': 0.6475876324932929, 'mcc_per_class': [-0.0056577595634534445, 0.004033783220492441, 0.018156663646356354], 'mcc_overall': 0.03694310094878359}\n",
      "*****\n",
      "Validation loss decreased (0.9233 -> 0.8253). Saving model...\n",
      "Epoch 5, Train Loss: 1.527, Train metrics: {'f1_per_class': array([0.47578947, 0.4989605 , 0.51018221]), 'f1_micro': 0.49490333919156415, 'f1_macro': 0.49497739352537123, 'precision_per_class': array([0.44575937, 0.54421769, 0.46942801]), 'precision_micro': 0.4838487972508591, 'precision_macro': 0.4864683546002227, 'recall_per_class': array([0.51015801, 0.46065259, 0.55868545]), 'recall_micro': 0.5064748201438849, 'recall_macro': 0.5098320169080777, 'mcc_per_class': [0.01532130451151455, 0.05084034850872411, 0.09762661131854315], 'mcc_overall': 0.04925253789927048}\n",
      "            Val Loss:   0.764, Val F1:   {'f1_per_class': array([0.29411765, 0.65921788, 0.40816327]), 'f1_micro': 0.49604221635883905, 'f1_macro': 0.45383292981997264, 'precision_per_class': array([0.31914894, 0.51754386, 0.44444444]), 'precision_micro': 0.4563106796116505, 'precision_macro': 0.4270457467545934, 'recall_per_class': array([0.27272727, 0.90769231, 0.37735849]), 'recall_micro': 0.5433526011560693, 'recall_macro': 0.5192593569952061, 'mcc_per_class': [-0.18252056768595676, 0.010304966806225734, 0.03594890259561335], 'mcc_overall': -0.002990253790225482}\n",
      "*****\n",
      "Validation loss decreased (0.8253 -> 0.7636). Saving model...\n",
      "Epoch 6, Train Loss: 1.183, Train metrics: {'f1_per_class': array([0.40914561, 0.45270988, 0.53193518]), 'f1_micro': 0.4693371144984048, 'f1_macro': 0.46459688905436086, 'precision_per_class': array([0.43814433, 0.50714286, 0.44783307]), 'precision_micro': 0.46261355695317957, 'precision_macro': 0.464373417616786, 'recall_per_class': array([0.38374718, 0.40882917, 0.65492958]), 'recall_micro': 0.4762589928057554, 'recall_macro': 0.48250197681948914, 'mcc_per_class': [-5.676650005620482e-05, -0.013812408160181356, 0.06792333991031305], 'mcc_overall': 0.008197151644651342}\n",
      "            Val Loss:   0.775, Val F1:   {'f1_per_class': array([0.54304636, 0.69189189, 0.5862069 ]), 'f1_micro': 0.611764705882353, 'f1_macro': 0.6070483820198368, 'precision_per_class': array([0.42708333, 0.53333333, 0.4214876 ]), 'precision_micro': 0.4629080118694362, 'precision_macro': 0.46063475665748393, 'recall_per_class': array([0.74545455, 0.98461538, 0.96226415]), 'recall_micro': 0.9017341040462428, 'recall_macro': 0.8974446936711088, 'mcc_per_class': [-0.03399367268081437, 0.15624858133025568, 0.008497013340781338], 'mcc_overall': 0.030131376562951735}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 7, Train Loss: 0.966, Train metrics: {'f1_per_class': array([0.38684211, 0.37777778, 0.525     ]), 'f1_micro': 0.441635687732342, 'f1_macro': 0.4298732943469785, 'precision_per_class': array([0.4637224 , 0.52941176, 0.42363112]), 'precision_micro': 0.45692307692307693, 'precision_macro': 0.47225509536717714, 'recall_per_class': array([0.33182844, 0.29366603, 0.69014085]), 'recall_micro': 0.4273381294964029, 'recall_macro': 0.438545104793249, 'mcc_per_class': [0.034792556647365504, 0.01782501342157108, 0.006790551157726683], 'mcc_overall': -0.0023797741397048075}\n",
      "            Val Loss:   0.761, Val F1:   {'f1_per_class': array([0.27586207, 0.62111801, 0.59393939]), 'f1_micro': 0.5375302663438256, 'f1_macro': 0.4969731584424238, 'precision_per_class': array([0.375     , 0.52083333, 0.4375    ]), 'precision_micro': 0.4625, 'precision_macro': 0.4444444444444445, 'recall_per_class': array([0.21818182, 0.76923077, 0.9245283 ]), 'recall_micro': 0.6416184971098265, 'recall_macro': 0.63731362976646, 'mcc_per_class': [-0.072360614407625, 0.017755520605710874, 0.09662834984782559], 'mcc_overall': 0.012779944398705358}\n",
      "*****\n",
      "Validation loss decreased (0.7636 -> 0.7614). Saving model...\n",
      "Epoch 8, Train Loss: 0.905, Train metrics: {'f1_per_class': array([0.24038462, 0.32887701, 0.57692308]), 'f1_micro': 0.4259541984732824, 'f1_macro': 0.38206156588509527, 'precision_per_class': array([0.41436464, 0.54185022, 0.4379562 ]), 'precision_micro': 0.45365853658536587, 'precision_macro': 0.46472368850928564, 'recall_per_class': array([0.16930023, 0.23608445, 0.84507042]), 'recall_micro': 0.4014388489208633, 'recall_macro': 0.4168183670812979, 'mcc_per_class': [-0.022414696423784207, 0.02855249646381602, 0.0700731646027848], 'mcc_overall': -0.007681002522535158}\n",
      "            Val Loss:   0.758, Val F1:   {'f1_per_class': array([0.11940299, 0.6125    , 0.55757576]), 'f1_micro': 0.5051020408163265, 'f1_macro': 0.4298262475501282, 'precision_per_class': array([0.33333333, 0.51578947, 0.41071429]), 'precision_micro': 0.4520547945205479, 'precision_macro': 0.41994569757727646, 'recall_per_class': array([0.07272727, 0.75384615, 0.86792453]), 'recall_micro': 0.5722543352601156, 'recall_macro': 0.5648326516251044, 'mcc_per_class': [-0.06749491124026878, -0.0002926413924559543, -0.056840205792838584], 'mcc_overall': -0.013232208980603129}\n",
      "*****\n",
      "Validation loss decreased (0.7614 -> 0.7578). Saving model...\n",
      "Epoch 9, Train Loss: 0.817, Train metrics: {'f1_per_class': array([0.2541806 , 0.31593039, 0.58897638]), 'f1_micro': 0.4344168260038241, 'f1_macro': 0.38636245605966324, 'precision_per_class': array([0.49032258, 0.52212389, 0.44312796]), 'precision_micro': 0.4636734693877551, 'precision_macro': 0.48519147884525976, 'recall_per_class': array([0.17155756, 0.22648752, 0.87793427]), 'recall_micro': 0.40863309352517985, 'recall_macro': 0.4253264527898471, 'mcc_per_class': [0.044719449437865655, 0.007292656659234725, 0.09908312884230305], 'mcc_overall': 0.008890091860284277}\n",
      "            Val Loss:   0.754, Val F1:   {'f1_per_class': array([0.13888889, 0.67403315, 0.52991453]), 'f1_micro': 0.5243243243243243, 'f1_macro': 0.44761218932489655, 'precision_per_class': array([0.29411765, 0.52586207, 0.484375  ]), 'precision_micro': 0.49238578680203043, 'precision_macro': 0.4347849053414469, 'recall_per_class': array([0.09090909, 0.93846154, 0.58490566]), 'recall_micro': 0.5606936416184971, 'recall_macro': 0.5380920965826625, 'mcc_per_class': [-0.11338395730473803, 0.0680772909696533, 0.13118310992362556], 'mcc_overall': 0.07269225892105285}\n",
      "*****\n",
      "Validation loss decreased (0.7578 -> 0.7539). Saving model...\n",
      "Epoch 10, Train Loss: 0.811, Train metrics: {'f1_per_class': array([0.20748299, 0.2435312 , 0.58015267]), 'f1_micro': 0.4078277886497065, 'f1_macro': 0.3437222891294387, 'precision_per_class': array([0.42068966, 0.58823529, 0.42986425]), 'precision_micro': 0.4472103004291845, 'precision_macro': 0.4795964008945754, 'recall_per_class': array([0.13769752, 0.15355086, 0.89201878]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3944223866654513, 'mcc_per_class': [-0.01442444269734064, 0.057510967749416095, 0.045412376572092704], 'mcc_overall': -0.017564339809300916}\n",
      "            Val Loss:   0.748, Val F1:   {'f1_per_class': array([0.46268657, 0.59872611, 0.5511811 ]), 'f1_micro': 0.5406698564593302, 'f1_macro': 0.5375312613920218, 'precision_per_class': array([0.39240506, 0.51086957, 0.47297297]), 'precision_micro': 0.46122448979591835, 'precision_macro': 0.4587492004938345, 'recall_per_class': array([0.56363636, 0.72307692, 0.66037736]), 'recall_micro': 0.653179190751445, 'recall_macro': 0.6490302150679509, 'mcc_per_class': [-0.1152899700922806, -0.016469219792943062, 0.12647448669140632], 'mcc_overall': 0.009678031987536978}\n",
      "*****\n",
      "Validation loss decreased (0.7539 -> 0.7479). Saving model...\n",
      "Epoch 11, Train Loss: 0.790, Train metrics: {'f1_per_class': array([0.12781955, 0.19354839, 0.601044  ]), 'f1_micro': 0.39871640593662255, 'f1_macro': 0.307470644328702, 'precision_per_class': array([0.38202247, 0.60606061, 0.44043716]), 'precision_micro': 0.45058930190389845, 'precision_macro': 0.4761734121468879, 'recall_per_class': array([0.07674944, 0.11516315, 0.94600939]), 'recall_micro': 0.3575539568345324, 'recall_macro': 0.3793073243766607, 'mcc_per_class': [-0.035165176651348364, 0.05981385003870598, 0.11924592258063015], 'mcc_overall': -0.011687051907406699}\n",
      "            Val Loss:   0.741, Val F1:   {'f1_per_class': array([0.17391304, 0.65536723, 0.34210526]), 'f1_micro': 0.4782608695652174, 'f1_macro': 0.39046184609152457, 'precision_per_class': array([0.42857143, 0.51785714, 0.56521739]), 'precision_micro': 0.5167785234899329, 'precision_macro': 0.5038819875776398, 'recall_per_class': array([0.10909091, 0.89230769, 0.24528302]), 'recall_micro': 0.44508670520231214, 'recall_macro': 0.41556054008884197, 'mcc_per_class': [-0.0056577595634534445, 0.011229577231219765, 0.13839873728444252], 'mcc_overall': 0.09569801678450034}\n",
      "*****\n",
      "Validation loss decreased (0.7479 -> 0.7414). Saving model...\n",
      "Epoch 12, Train Loss: 0.769, Train metrics: {'f1_per_class': array([0.1328125 , 0.19575856, 0.58493353]), 'f1_micro': 0.3953206938281565, 'f1_macro': 0.30450153157261467, 'precision_per_class': array([0.49275362, 0.65217391, 0.42672414]), 'precision_micro': 0.44995408631772266, 'precision_macro': 0.5238838913876395, 'recall_per_class': array([0.07674944, 0.11516315, 0.92957746]), 'recall_micro': 0.35251798561151076, 'recall_macro': 0.373830016082451, 'mcc_per_class': [0.029768492972591355, 0.0866346440959698, 0.036291047359947086], 'mcc_overall': -0.012524940860047767}\n",
      "            Val Loss:   0.744, Val F1:   {'f1_per_class': array([0.11594203, 0.57692308, 0.32      ]), 'f1_micro': 0.4066666666666667, 'f1_macro': 0.337621701969528, 'precision_per_class': array([0.28571429, 0.49450549, 0.54545455]), 'precision_micro': 0.48031496062992124, 'precision_macro': 0.44189144189144186, 'recall_per_class': array([0.07272727, 0.69230769, 0.22641509]), 'recall_micro': 0.35260115606936415, 'recall_macro': 0.33048335312486254, 'mcc_per_class': [-0.10749743170561545, -0.06894293534848245, 0.11629170123453067], 'mcc_overall': 0.0323288935166905}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 13, Train Loss: 0.782, Train metrics: {'f1_per_class': array([0.12977099, 0.18619583, 0.59463487]), 'f1_micro': 0.39453595821615106, 'f1_macro': 0.303533897445025, 'precision_per_class': array([0.41975309, 0.56862745, 0.43558952]), 'precision_micro': 0.4467697907188353, 'precision_macro': 0.4746566856836001, 'recall_per_class': array([0.07674944, 0.11132438, 0.93661972]), 'recall_micro': 0.3532374100719424, 'recall_macro': 0.3748978433917965, 'mcc_per_class': [-0.010960471196279277, 0.03572298941458554, 0.0894524870459567], 'mcc_overall': -0.01743239758800764}\n",
      "            Val Loss:   0.743, Val F1:   {'f1_per_class': array([0.09677419, 0.65895954, 0.25352113]), 'f1_micro': 0.45098039215686275, 'f1_macro': 0.33641828596040163, 'precision_per_class': array([0.42857143, 0.52777778, 0.5       ]), 'precision_micro': 0.518796992481203, 'precision_macro': 0.4854497354497354, 'recall_per_class': array([0.05454545, 0.87692308, 0.16981132]), 'recall_micro': 0.3988439306358382, 'recall_macro': 0.36709328407441616, 'mcc_per_class': [-0.003881191040835057, 0.05835059493597381, 0.06563341623057815], 'mcc_overall': 0.09039693707507944}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Epoch 14, Train Loss: 0.761, Train metrics: {'f1_per_class': array([0.0969697 , 0.16905901, 0.58654573]), 'f1_micro': 0.3803680981595092, 'f1_macro': 0.2841914791789479, 'precision_per_class': array([0.46153846, 0.5       , 0.43255295]), 'precision_micro': 0.44075829383886256, 'precision_macro': 0.46469713861018214, 'recall_per_class': array([0.05417607, 0.10172745, 0.91079812]), 'recall_micro': 0.3345323741007194, 'recall_macro': 0.3555672138391271, 'mcc_per_class': [0.010962544276260406, -0.010498891306007136, 0.06355701297029871], 'mcc_overall': -0.025700170906917896}\n",
      "            Val Loss:   0.743, Val F1:   {'f1_per_class': array([0.12698413, 0.6741573 , 0.15384615]), 'f1_micro': 0.45098039215686275, 'f1_macro': 0.3183291947336891, 'precision_per_class': array([0.5       , 0.53097345, 0.41666667]), 'precision_micro': 0.518796992481203, 'precision_macro': 0.48254670599803345, 'recall_per_class': array([0.07272727, 0.92307692, 0.09433962]), 'recall_micro': 0.3988439306358382, 'recall_macro': 0.3633812728152351, 'mcc_per_class': [0.03333369502903513, 0.08908528345577679, -0.002608007505063564], 'mcc_overall': 0.09039693707507944}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Epoch 15, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.06666667, 0.14309484, 0.59511473]), 'f1_micro': 0.37911184210526316, 'f1_macro': 0.268292079475513, 'precision_per_class': array([0.43243243, 0.5375    , 0.43459459]), 'precision_micro': 0.44241842610364684, 'precision_macro': 0.4681756756756757, 'recall_per_class': array([0.03611738, 0.08253359, 0.94366197]), 'recall_micro': 0.3316546762589928, 'recall_macro': 0.35410431419075583, 'mcc_per_class': [-0.00225778242701937, 0.013002999741417056, 0.08786928383129194], 'mcc_overall': -0.023047427608398417}\n",
      "            Val Loss:   0.742, Val F1:   {'f1_per_class': array([0.1       , 0.67039106, 0.25714286]), 'f1_micro': 0.46601941747572817, 'f1_macro': 0.342511306198457, 'precision_per_class': array([0.6       , 0.52631579, 0.52941176]), 'precision_micro': 0.5294117647058824, 'precision_macro': 0.5519091847265222, 'recall_per_class': array([0.05454545, 0.92307692, 0.16981132]), 'recall_micro': 0.4161849710982659, 'recall_macro': 0.3824778994590316, 'mcc_per_class': [0.0670114504521317, 0.06440604253891084, 0.08701995342363365], 'mcc_overall': 0.10794779934653571}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Epoch 16, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.08695652, 0.13013699, 0.59011628]), 'f1_micro': 0.38067949242734345, 'f1_macro': 0.2690699290367559, 'precision_per_class': array([0.525     , 0.6031746 , 0.42736842]), 'precision_micro': 0.4415954415954416, 'precision_macro': 0.5185143414090783, 'recall_per_class': array([0.04740406, 0.07293666, 0.95305164]), 'recall_micro': 0.3345323741007194, 'recall_macro': 0.35779745555553993, 'mcc_per_class': [0.035515275495008526, 0.04531153197044691, 0.04798054594998333], 'mcc_overall': -0.024437563405224426}\n",
      "            Val Loss:   0.741, Val F1:   {'f1_per_class': array([0.06779661, 0.65909091, 0.27027027]), 'f1_micro': 0.45307443365695793, 'f1_macro': 0.33238592984355697, 'precision_per_class': array([0.5       , 0.52252252, 0.47619048]), 'precision_micro': 0.5147058823529411, 'precision_macro': 0.4995709995709996, 'recall_per_class': array([0.03636364, 0.89230769, 0.18867925]), 'recall_micro': 0.4046242774566474, 'recall_macro': 0.3724501913181159, 'mcc_per_class': [0.023180860221282643, 0.03619547112256257, 0.05032846363385869], 'mcc_overall': 0.08581967128092265}\n",
      "*****\n",
      "Validation loss decreased (0.7414 -> 0.7408). Saving model...\n",
      "Epoch 17, Train Loss: 0.759, Train metrics: {'f1_per_class': array([0.06302521, 0.16965742, 0.58345643]), 'f1_micro': 0.37822349570200575, 'f1_macro': 0.2720463526674908, 'precision_per_class': array([0.45454545, 0.56521739, 0.42564655]), 'precision_micro': 0.43874643874643876, 'precision_macro': 0.48180313252464674, 'recall_per_class': array([0.03386005, 0.09980806, 0.92723005]), 'recall_micro': 0.33237410071942447, 'recall_macro': 0.3536327178384764, 'mcc_per_class': [0.0060588501908376315, 0.03158271123548868, 0.028993861822773956], 'mcc_overall': -0.028607415996394363}\n",
      "            Val Loss:   0.740, Val F1:   {'f1_per_class': array([0.1       , 0.66666667, 0.46601942]), 'f1_micro': 0.5, 'f1_macro': 0.41089536138079824, 'precision_per_class': array([0.6 , 0.55, 0.48]), 'precision_micro': 0.5290322580645161, 'precision_macro': 0.5433333333333333, 'recall_per_class': array([0.05454545, 0.84615385, 0.45283019]), 'recall_micro': 0.47398843930635837, 'recall_macro': 0.45117649645951535, 'mcc_per_class': [0.0670114504521317, 0.13392454691420533, 0.09753948068937729], 'mcc_overall': 0.11941583090706219}\n",
      "*****\n",
      "Validation loss decreased (0.7408 -> 0.7404). Saving model...\n",
      "Epoch 18, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.075     , 0.1530782 , 0.57591623]), 'f1_micro': 0.37138130686517784, 'f1_macro': 0.2679981444538335, 'precision_per_class': array([0.48648649, 0.575     , 0.42261251]), 'precision_micro': 0.4367704280155642, 'precision_macro': 0.4946996667358907, 'recall_per_class': array([0.04063205, 0.08829175, 0.90375587]), 'recall_micro': 0.32302158273381293, 'recall_macro': 0.34422655645391603, 'mcc_per_class': [0.01897587370057675, 0.034998592722854834, 0.007625671761305486], 'mcc_overall': -0.030928724432138798}\n",
      "            Val Loss:   0.743, Val F1:   {'f1_per_class': array([0.1       , 0.68817204, 0.12698413]), 'f1_micro': 0.459546925566343, 'f1_macro': 0.3050520566649599, 'precision_per_class': array([0.6       , 0.52892562, 0.4       ]), 'precision_micro': 0.5220588235294118, 'precision_macro': 0.509641873278237, 'recall_per_class': array([0.05454545, 0.98461538, 0.0754717 ]), 'recall_micro': 0.41040462427745666, 'recall_macro': 0.3715441790913489, 'mcc_per_class': [0.0670114504521317, 0.12848540365587144, -0.012272837629134187], 'mcc_overall': 0.09688373531372918}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Epoch 19, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.04700855, 0.1641541 , 0.582297  ]), 'f1_micro': 0.37664473684210525, 'f1_macro': 0.2644865505308907, 'precision_per_class': array([0.44      , 0.64473684, 0.4229543 ]), 'precision_micro': 0.43953934740882916, 'precision_macro': 0.5025637153457502, 'recall_per_class': array([0.0248307, 0.0940499, 0.9342723]), 'recall_micro': 0.3294964028776978, 'recall_macro': 0.35105096809148667, 'mcc_per_class': [0.0005840813495692008, 0.07382236191930226, 0.01180117435528076], 'mcc_overall': -0.02722763658658761}\n",
      "            Val Loss:   0.744, Val F1:   {'f1_per_class': array([0.06779661, 0.65909091, 0.30769231]), 'f1_micro': 0.46006389776357826, 'f1_macro': 0.3448599423175695, 'precision_per_class': array([0.5       , 0.52252252, 0.48      ]), 'precision_micro': 0.5142857142857142, 'precision_macro': 0.5008408408408408, 'recall_per_class': array([0.03636364, 0.89230769, 0.22641509]), 'recall_micro': 0.4161849710982659, 'recall_macro': 0.38502880767031716, 'mcc_per_class': [0.023180860221282643, 0.03619547112256257, 0.05982897333975739], 'mcc_overall': 0.08715440264114858}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Epoch 20, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.07112971, 0.10211268, 0.58152958]), 'f1_micro': 0.3692434210526316, 'f1_macro': 0.2515906548996301, 'precision_per_class': array([0.48571429, 0.61702128, 0.41979167]), 'precision_micro': 0.4309021113243762, 'precision_macro': 0.5075090763255656, 'recall_per_class': array([0.03837472, 0.05566219, 0.94600939]), 'recall_micro': 0.32302158273381293, 'recall_macro': 0.34668209853470894, 'mcc_per_class': [0.018142245081713543, 0.04492856814529418, -0.013824059675507755], 'mcc_overall': -0.03976826352115519}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.07017544, 0.10958904, 0.57471264]), 'f1_micro': 0.3684210526315789, 'f1_macro': 0.2514923744568475, 'precision_per_class': array([1.        , 0.5       , 0.41322314]), 'precision_micro': 0.42748091603053434, 'precision_macro': 0.6377410468319559, 'recall_per_class': array([0.03636364, 0.06153846, 0.94339623]), 'recall_micro': 0.3236994219653179, 'recall_macro': 0.34709944143906407, 'mcc_per_class': [0.14429523335682806, -0.008270131015031879, -0.0738586544237147], 'mcc_overall': -0.04413235328585039}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Epoch 21, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.06451613, 0.07815275, 0.5961401 ]), 'f1_micro': 0.37412443345694274, 'f1_macro': 0.24626966073736192, 'precision_per_class': array([0.68181818, 0.52380952, 0.42857143]), 'precision_micro': 0.437801350048216, 'precision_macro': 0.5447330447330447, 'recall_per_class': array([0.03386005, 0.04222649, 0.97887324]), 'recall_micro': 0.32661870503597124, 'recall_macro': 0.35165325736911296, 'mcc_per_class': [0.07323752071183794, 0.003531824097268658, 0.0738505995280294], 'mcc_overall': -0.029642417240591704}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.03508772, 0.27848101, 0.5497076 ]), 'f1_micro': 0.38436482084690554, 'f1_macro': 0.28775877809855155, 'precision_per_class': array([0.5       , 0.78571429, 0.39830508]), 'precision_micro': 0.44029850746268656, 'precision_macro': 0.5613397901533494, 'recall_per_class': array([0.01818182, 0.16923077, 0.88679245]), 'recall_micro': 0.34104046242774566, 'recall_macro': 0.358068346747592, 'mcc_per_class': [0.01625861784302288, 0.190902812930736, -0.17372116042029476], 'mcc_overall': -0.02584255074318725}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Epoch 22, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.12326044, 0.05347594, 0.59110473]), 'f1_micro': 0.3726606997558991, 'f1_macro': 0.2559470359271267, 'precision_per_class': array([0.51666667, 0.375     , 0.42561983]), 'precision_micro': 0.4288389513108614, 'precision_macro': 0.4390955004591368, 'recall_per_class': array([0.06997743, 0.02879079, 0.96713615]), 'recall_micro': 0.3294964028776978, 'recall_macro': 0.3553014546064957, 'mcc_per_class': [0.039733449173914845, -0.0569914036726189, 0.04088425438163153], 'mcc_overall': -0.0435795360503719}\n",
      "            Val Loss:   0.745, Val F1:   {'f1_per_class': array([0.03333333, 0.62820513, 0.51851852]), 'f1_micro': 0.48148148148148145, 'f1_macro': 0.39335232668566, 'precision_per_class': array([0.2       , 0.53846154, 0.50909091]), 'precision_micro': 0.5165562913907285, 'precision_macro': 0.41585081585081585, 'recall_per_class': array([0.01818182, 0.75384615, 0.52830189]), 'recall_micro': 0.4508670520231214, 'recall_macro': 0.43344328627347495, 'mcc_per_class': [-0.09693889434337498, 0.07288253165411002, 0.15770686206458148], 'mcc_overall': 0.09639780124947603}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Epoch 23, Train Loss: 0.748, Train metrics: {'f1_per_class': array([0.0746888 , 0.09859155, 0.58983536]), 'f1_micro': 0.3743359215365754, 'f1_macro': 0.2543719024883924, 'precision_per_class': array([0.46153846, 0.59574468, 0.42430484]), 'precision_micro': 0.43330179754020814, 'precision_macro': 0.49386266092009246, 'recall_per_class': array([0.04063205, 0.0537428 , 0.96713615]), 'recall_micro': 0.3294964028776978, 'recall_macro': 0.35383700223802567, 'mcc_per_class': [0.009430140497804626, 0.03552815788145055, 0.029334182092489138], 'mcc_overall': -0.03668276910871329}\n",
      "            Val Loss:   0.745, Val F1:   {'f1_per_class': array([0.4       , 0.3908046 , 0.42352941]), 'f1_micro': 0.4041095890410959, 'f1_macro': 0.40477800315528517, 'precision_per_class': array([0.36923077, 0.77272727, 0.5625    ]), 'precision_micro': 0.4957983193277311, 'precision_macro': 0.5681526806526807, 'recall_per_class': array([0.43636364, 0.26153846, 0.33962264]), 'recall_micro': 0.34104046242774566, 'recall_macro': 0.34584157980384395, 'mcc_per_class': [-0.14002948253353378, 0.23639070057991748, 0.16767092026386377], 'mcc_overall': 0.05187293001132469}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Epoch 24, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.0875    , 0.08288288, 0.58806614]), 'f1_micro': 0.3734542456718879, 'f1_macro': 0.2528163407836305, 'precision_per_class': array([0.56756757, 0.67647059, 0.4238342 ]), 'precision_micro': 0.4372586872586873, 'precision_macro': 0.5559574508980178, 'recall_per_class': array([0.04740406, 0.04414587, 0.9600939 ]), 'recall_micro': 0.3258992805755396, 'recall_macro': 0.35054794441319004, 'mcc_per_class': [0.05082635789197092, 0.06014890466861221, 0.022904008431800876], 'mcc_overall': -0.030405155657081028}\n",
      "            Val Loss:   0.745, Val F1:   {'f1_per_class': array([0.42975207, 0.54700855, 0.2972973 ]), 'f1_micro': 0.4423076923076923, 'f1_macro': 0.4246859701405156, 'precision_per_class': array([0.39393939, 0.61538462, 0.52380952]), 'precision_micro': 0.49640287769784175, 'precision_macro': 0.5110445110445111, 'recall_per_class': array([0.47272727, 0.49230769, 0.20754717]), 'recall_micro': 0.3988439306358382, 'recall_macro': 0.39086071161542857, 'mcc_per_class': [-0.09002127298091755, 0.16692003543013675, 0.09346714674859471], 'mcc_overall': 0.059286809678507225}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7496405690908432,\n",
       " {'f1_per_class': array([0.39316239, 0.6013986 , 0.18918919]),\n",
       "  'f1_micro': 0.437125748502994,\n",
       "  'f1_macro': 0.3945833945833946,\n",
       "  'precision_per_class': array([0.37704918, 0.55128205, 0.33333333]),\n",
       "  'precision_micro': 0.45625,\n",
       "  'precision_macro': 0.42055485498108447,\n",
       "  'recall_per_class': array([0.41071429, 0.66153846, 0.13207547]),\n",
       "  'recall_micro': 0.41954022988505746,\n",
       "  'recall_macro': 0.4014427396502868,\n",
       "  'mcc_per_class': [-0.12372148548351541,\n",
       "   0.09962742211111307,\n",
       "   -0.07581089846549775],\n",
       "  'mcc_overall': -0.0007565677749389509})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GIN no PCA\n",
    "trained_gin_nopca = train(gin_nopca, opt_gin_nopca, train_loader_nopca, val_loader_nopca, model_arch=\"GIN\", epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "val_loss, val_metrics = evaluate(trained_gin_nopca, test_loader_nopca, model_arch=\"GIN\")\n",
    "val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8792a8-f544-4624-a9f2-716101eab322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "torch.save(trained_gcn_pca.state_dict(), '../res/trained_models/trained_gcn_pca.pth')\n",
    "torch.save(trained_gat_pca.state_dict(), '../res/trained_models/trained_gat_pca.pth')\n",
    "torch.save(trained_gin_pca.state_dict(), '../res/trained_models/trained_gin_pca.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6901d5b-c65f-49ae-916d-1dc71e6ab332",
   "metadata": {},
   "source": [
    "### EGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "530d0b2c-a069-4619-a085-87e4b89d7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class E_GCL(nn.Module):\n",
    "    \"\"\"\n",
    "    E(n) Equivariant Convolutional Layer\n",
    "    re\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nf, output_nf, hidden_nf, edges_in_d=0, act_fn=nn.SiLU(), residual=True, attention=False, normalize=False, coords_agg='mean', tanh=False):\n",
    "        super(E_GCL, self).__init__()\n",
    "        input_edge = input_nf * 2\n",
    "        self.residual = residual\n",
    "        self.attention = attention\n",
    "        self.normalize = normalize\n",
    "        self.coords_agg = coords_agg\n",
    "        self.tanh = tanh\n",
    "        self.epsilon = 1e-8\n",
    "        edge_coords_nf = 1\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(input_edge + edge_coords_nf + edges_in_d, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn)\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf + input_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, output_nf))\n",
    "\n",
    "        layer = nn.Linear(hidden_nf, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        coord_mlp = []\n",
    "        coord_mlp.append(nn.Linear(hidden_nf, hidden_nf))\n",
    "        coord_mlp.append(act_fn)\n",
    "        coord_mlp.append(layer)\n",
    "        if self.tanh:\n",
    "            coord_mlp.append(nn.Tanh())\n",
    "        self.coord_mlp = nn.Sequential(*coord_mlp)\n",
    "\n",
    "        if self.attention:\n",
    "            self.att_mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_nf, 1),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def edge_model(self, source, target, radial, edge_attr):\n",
    "        if edge_attr is None:  # Unused.\n",
    "            out = torch.cat([source, target, radial], dim=1)\n",
    "        else:\n",
    "            out = torch.cat([source, target, radial, edge_attr], dim=1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.attention:\n",
    "            att_val = self.att_mlp(out)\n",
    "            out = out * att_val\n",
    "        return out\n",
    "\n",
    "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
    "        row, col = edge_index\n",
    "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0))\n",
    "        if node_attr is not None:\n",
    "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
    "        else:\n",
    "            agg = torch.cat([x, agg], dim=1)\n",
    "        out = self.node_mlp(agg)\n",
    "        if self.residual:\n",
    "            out = x + out\n",
    "        return out, agg\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat):\n",
    "        row, col = edge_index\n",
    "        trans = coord_diff * self.coord_mlp(edge_feat)\n",
    "        if self.coords_agg == 'sum':\n",
    "            agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
    "        elif self.coords_agg == 'mean':\n",
    "            agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))\n",
    "        else:\n",
    "            raise Exception('Wrong coords_agg parameter' % self.coords_agg)\n",
    "        coord += agg\n",
    "        return coord\n",
    "\n",
    "    def coord2radial(self, edge_index, coord):\n",
    "        row, col = edge_index\n",
    "        coord_diff = coord[row] - coord[col]\n",
    "        radial = torch.sum(coord_diff**2, 1).unsqueeze(1)\n",
    "\n",
    "        if self.normalize:\n",
    "            norm = torch.sqrt(radial).detach() + self.epsilon\n",
    "            coord_diff = coord_diff / norm\n",
    "\n",
    "        return radial, coord_diff\n",
    "\n",
    "    def forward(self, h, edge_index, coord, edge_attr=None, node_attr=None):\n",
    "        row, col = edge_index\n",
    "        radial, coord_diff = self.coord2radial(edge_index, coord)\n",
    "\n",
    "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr)\n",
    "        coord = self.coord_model(coord, edge_index, coord_diff, edge_feat)\n",
    "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
    "\n",
    "        return h, coord, edge_attr\n",
    "\n",
    "\n",
    "class EGNN(nn.Module):\n",
    "    def __init__(self, in_node_nf, hidden_nf, out_node_nf, in_edge_nf=0, device='cpu', act_fn=nn.SiLU(), n_layers=4, residual=True, attention=False, normalize=False, tanh=False):\n",
    "        '''\n",
    "\n",
    "        :param in_node_nf: Number of features for 'h' at the input\n",
    "        :param hidden_nf: Number of hidden features\n",
    "        :param out_node_nf: Number of features for 'h' at the output\n",
    "        :param in_edge_nf: Number of features for the edge features\n",
    "        :param device: Device (e.g. 'cpu', 'cuda:0',...)\n",
    "        :param act_fn: Non-linearity\n",
    "        :param n_layers: Number of layer for the EGNN\n",
    "        :param residual: Use residual connections, we recommend not changing this one\n",
    "        :param attention: Whether using attention or not\n",
    "        :param normalize: Normalizes the coordinates messages such that:\n",
    "                    instead of: x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)\n",
    "                    we get:     x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)/||x_i - x_j||\n",
    "                    We noticed it may help in the stability or generalization in some future works.\n",
    "                    We didn't use it in our paper.\n",
    "        :param tanh: Sets a tanh activation function at the output of phi_x(m_ij). I.e. it bounds the output of\n",
    "                        phi_x(m_ij) which definitely improves in stability but it may decrease in accuracy.\n",
    "                        We didn't use it in our paper.\n",
    "        '''\n",
    "\n",
    "        super(EGNN, self).__init__()\n",
    "        self.hidden_nf = hidden_nf\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_in = nn.Linear(in_node_nf, self.hidden_nf)\n",
    "        self.embedding_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
    "        for i in range(0, n_layers):\n",
    "            self.add_module(\"gcl_%d\" % i, E_GCL(self.hidden_nf, self.hidden_nf, self.hidden_nf, edges_in_d=in_edge_nf,\n",
    "                                                act_fn=act_fn, residual=residual, attention=attention,\n",
    "                                                normalize=normalize, tanh=tanh))\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, h, x, edges, edge_attr):\n",
    "        h = self.embedding_in(h)\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, x, _ = self._modules[\"gcl_%d\" % i](h, edges, x, edge_attr=edge_attr)\n",
    "        h = self.embedding_out(h)\n",
    "        return h, x\n",
    "\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    count = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "\n",
    "def get_edges(n_nodes):\n",
    "    rows, cols = [], []\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(n_nodes):\n",
    "            if i != j:\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "\n",
    "    edges = [rows, cols]\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_edges_batch(n_nodes, batch_size):\n",
    "    edges = get_edges(n_nodes)\n",
    "    edge_attr = torch.ones(len(edges[0]) * batch_size, 1)\n",
    "    edges = [torch.LongTensor(edges[0]), torch.LongTensor(edges[1])]\n",
    "    if batch_size == 1:\n",
    "        return edges, edge_attr\n",
    "    elif batch_size > 1:\n",
    "        rows, cols = [], []\n",
    "        for i in range(batch_size):\n",
    "            rows.append(edges[0] + n_nodes * i)\n",
    "            cols.append(edges[1] + n_nodes * i)\n",
    "        edges = [torch.cat(rows), torch.cat(cols)]\n",
    "    return edges, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8df9419d-fc8e-466d-82a0-cf1a9d5e85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "class EGNN_Graph_Classifier(torch.nn.Module):\n",
    "    \"\"\"EGNN model for graph classification\"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out, dim_edge_attr, n_layers=4, n_layers_egnn=1, use_graph_feat=False, dim_graph_feature=0, dropout_p=0.5):\n",
    "        super(EGNN_Graph_Classifier, self).__init__()\n",
    "        self.args = (dim_in, dim_h, dim_out, dim_edge_attr, n_layers, n_layers_egnn, use_graph_feat, dim_graph_feature, dropout_p)\n",
    "        self.use_graph_feat = use_graph_feat\n",
    "        self.dropout_p = dropout_p\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # EGNN layers\n",
    "        self.egnn_layers = torch.nn.ModuleList([EGNN(in_node_nf=dim_in if i == 0 else dim_h // (2**i),\n",
    "                                                     hidden_nf=dim_h // (2**i),\n",
    "                                                     out_node_nf=dim_h // (2**(i+1)),\n",
    "                                                     in_edge_nf=dim_edge_attr, \n",
    "                                                     n_layers=n_layers_egnn)\n",
    "                                                for i in range(n_layers)])\n",
    "        \n",
    "        # Calculate the total dimension after pooling all EGNN layers\n",
    "        total_dim = sum([dim_h // (2**i) for i in range(n_layers)])  # Sum of pooled dimensions\n",
    "\n",
    "        if use_graph_feat:\n",
    "            total_dim += dim_graph_feature  # Adjust for graph-level features\n",
    "\n",
    "        # Linear layers for classification\n",
    "        self.lin1 = torch.nn.Linear(total_dim//2, dim_h // 4)\n",
    "        self.lin2 = torch.nn.Linear(dim_h // 4, dim_out)\n",
    "\n",
    "    def forward(self, h, edge_index, x, batch, graph_features=None):\n",
    "        # h: node features\n",
    "        # x: coordinates\n",
    "        h_list = []\n",
    "        \n",
    "        # Apply EGNN layers\n",
    "        for layer in self.egnn_layers:\n",
    "            h, x = layer(h, x, edge_index, edge_attr=None)\n",
    "            h_add = global_add_pool(h, batch)\n",
    "            h_list.append(h_add)\n",
    "        \n",
    "        # Concatenate pooled representations from all layers\n",
    "        h = torch.cat(h_list, dim=1)\n",
    "\n",
    "        # Concatenate graph-level features if applicable\n",
    "        if self.use_graph_feat and graph_features is not None:\n",
    "            graph_features = graph_features.view(h.shape[0], -1)\n",
    "            h = torch.cat([h, graph_features], dim=1)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        h = self.lin1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_p, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h  # Return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "115064be-bf71-4b79-8533-64f19018176a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn.functional as F\\nfrom torch_geometric.nn import global_add_pool, global_max_pool\\n\\nclass EGNN_Graph_Classifier(torch.nn.Module):\\n    \"\"\"EGNN model for graph classification\"\"\"\\n    def __init__(self, dim_in, dim_h, dim_out, dim_edge_attr, n_layers=4, use_graph_feat=False, dropout_p=0.5):\\n        super(EGNN_Graph_Classifier, self).__init__()\\n        self.args = (dim_in, dim_h, dim_out, dim_edge_attr, n_layers, use_graph_feat, dropout_p)\\n        self.use_graph_feat = use_graph_feat\\n        self.dropout_p = dropout_p\\n\\n        # EGNN layer\\n        self.egnn = EGNN(in_node_nf=dim_in, hidden_nf=dim_h, out_node_nf=dim_h//2, in_edge_nf=dim_edge_attr, n_layers=n_layers)\\n        #self.egnn2 = EGNN(in_node_nf=dim_h//2, hidden_nf=dim_h//4, out_node_nf=dim_h//8, in_edge_nf=dim_edge_attr, n_layers=n_layers)\\n        \\n        # Linear layers for classification\\n        self.lin1 = torch.nn.Linear(dim_h , dim_h//2)\\n        self.lin2 = torch.nn.Linear(dim_h//2, dim_out)\\n\\n    def forward(self, h, edge_index, x, batch):\\n        #h, edge_index, x, batch = data.x, data.edge_index, data.coords.float(), data.batch\\n\\n        h1, x1 = self.egnn(h, x, edge_index, edge_attr=None)\\n        h_add = global_add_pool(h1, batch)\\n        h_max = global_max_pool(h1, batch)\\n        h = torch.cat([h_add, h_max], dim=1)\\n\\n\\n        # Fully connected layers for classification\\n        h = self.lin1(h)\\n        h = F.relu(h)\\n        h = F.dropout(h, p=self.dropout_p, training=self.training)\\n        h = self.lin2(h)\\n\\n        return h  # Return logits\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_add_pool, global_max_pool\n",
    "\n",
    "class EGNN_Graph_Classifier(torch.nn.Module):\n",
    "    \"\"\"EGNN model for graph classification\"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out, dim_edge_attr, n_layers=4, use_graph_feat=False, dropout_p=0.5):\n",
    "        super(EGNN_Graph_Classifier, self).__init__()\n",
    "        self.args = (dim_in, dim_h, dim_out, dim_edge_attr, n_layers, use_graph_feat, dropout_p)\n",
    "        self.use_graph_feat = use_graph_feat\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # EGNN layer\n",
    "        self.egnn = EGNN(in_node_nf=dim_in, hidden_nf=dim_h, out_node_nf=dim_h//2, in_edge_nf=dim_edge_attr, n_layers=n_layers)\n",
    "        #self.egnn2 = EGNN(in_node_nf=dim_h//2, hidden_nf=dim_h//4, out_node_nf=dim_h//8, in_edge_nf=dim_edge_attr, n_layers=n_layers)\n",
    "        \n",
    "        # Linear layers for classification\n",
    "        self.lin1 = torch.nn.Linear(dim_h , dim_h//2)\n",
    "        self.lin2 = torch.nn.Linear(dim_h//2, dim_out)\n",
    "\n",
    "    def forward(self, h, edge_index, x, batch):\n",
    "        #h, edge_index, x, batch = data.x, data.edge_index, data.coords.float(), data.batch\n",
    "\n",
    "        h1, x1 = self.egnn(h, x, edge_index, edge_attr=None)\n",
    "        h_add = global_add_pool(h1, batch)\n",
    "        h_max = global_max_pool(h1, batch)\n",
    "        h = torch.cat([h_add, h_max], dim=1)\n",
    "\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        h = self.lin1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_p, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h  # Return logits\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f378d0fc-fe0d-4604-a3e7-cda1774aafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, pos_weight=torch.tensor([1,1,1]), alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "        #self.mccloss = MCCLoss(reduction=\"none\").to(device)\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        loss = self.bce_loss(inputs, targets) #+ self.mccloss(inputs, targets)\n",
    "        pt = torch.exp(-loss)  # pt is the probability of the correct class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * loss\n",
    "        return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "179ad541-f0d7-463f-a833-06dd431811f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "input_dim_nopca = train_list_norm[0].x.shape[1]  # Adjust this based on your feature size\n",
    "input_dim_pca = train_list_norm_pca[0].x.shape[1]  # Adjust this based on your feature size\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "n_layers_egnn = 1\n",
    "output_dim = 3  # Number of labels\n",
    "graph_features_dim = train_list_norm[0].u.shape[0]\n",
    "\n",
    "# define potential models\n",
    "egnn_pca   = EGNN_Graph_Classifier(input_dim_pca, hidden_dim, output_dim, dim_edge_attr=0, n_layers=n_layers, n_layers_egnn=n_layers_egnn, use_graph_feat=False, dropout_p=0.5).to(device) \n",
    "egnn_nopca   = EGNN_Graph_Classifier(input_dim_nopca, hidden_dim, output_dim, dim_edge_attr=0, n_layers=n_layers, n_layers_egnn=n_layers_egnn, use_graph_feat=False, dropout_p=0.5).to(device)\n",
    "\n",
    "\n",
    "LR = 5e-4\n",
    "WD = 5e-4\n",
    "# define optimizers\n",
    "opt_egnn_pca = torch.optim.AdamW(egnn_pca.parameters(), lr=LR, weight_decay=WD)\n",
    "opt_egnn_nopca = torch.optim.AdamW(egnn_nopca.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "# define schedulers\n",
    "scheduler_egnn_pca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_egnn_pca, mode='min', factor=0.1, patience=1)\n",
    "scheduler_egnn_nopca = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_egnn_nopca, mode='min', factor=0.1, patience=1)\n",
    "\n",
    "# define loss\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = FocalLoss(pos_weight=pos_weight, alpha=0.5, gamma=2)\n",
    "#criterion = MCCLoss(reduction=\"product\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3c7e68e9-0a80-42f7-a51e-c638a0a66cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# EGNN PCA\\ntrained_egnn_pca = train(egnn_pca, opt_egnn_pca, train_loader_pca, val_loader_pca, model_arch=\"EGNN\", scheduler=scheduler_egnn_pca, epochs=100)\\nprint(\"\\nBest model:\")\\ntrained_egnn_pca.eval()\\nval_loss, val_metrics = evaluate(trained_egnn_pca, test_loader_pca, model_arch=\"EGNN\")\\nval_loss, val_metrics\\n'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# EGNN PCA\n",
    "trained_egnn_pca = train(egnn_pca, opt_egnn_pca, train_loader_pca, val_loader_pca, model_arch=\"EGNN\", scheduler=scheduler_egnn_pca, epochs=100)\n",
    "print(\"\\nBest model:\")\n",
    "trained_egnn_pca.eval()\n",
    "val_loss, val_metrics = evaluate(trained_egnn_pca, test_loader_pca, model_arch=\"EGNN\")\n",
    "val_loss, val_metrics\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "85599fd4-ecf7-47ce-ae18-8d04a1f92f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 155.228, Train metrics: {'f1_per_class': array([0.44191344, 0.55026455, 0.49038462]), 'f1_micro': 0.4986893840104849, 'f1_macro': 0.4941875350949003, 'precision_per_class': array([0.44597701, 0.50897227, 0.41530945]), 'precision_micro': 0.45788206979542717, 'precision_macro': 0.4567529084283431, 'recall_per_class': array([0.43792325, 0.59884837, 0.59859155]), 'recall_micro': 0.5474820143884892, 'recall_macro': 0.5451210561273938, 'mcc_per_class': [0.013656379719610914, -0.015791295478481546, -0.015251415545151246], 'mcc_overall': -0.0009061081937617749}\n",
      "            Val Loss:   14.293, Val F1:   {'f1_per_class': array([0.42718447, 0.58503401, 0.58333333]), 'f1_micro': 0.5454545454545454, 'f1_macro': 0.5318506043193977, 'precision_per_class': array([0.45833333, 0.52439024, 0.42608696]), 'precision_micro': 0.46530612244897956, 'precision_macro': 0.4696035112525038, 'recall_per_class': array([0.4       , 0.66153846, 0.9245283 ]), 'recall_micro': 0.6589595375722543, 'recall_macro': 0.6620222544750847, 'mcc_per_class': [0.034521963740470715, 0.023266326155312003, 0.03570939264633302], 'mcc_overall': 0.020797472994494356}\n",
      "*****\n",
      "Validation loss decreased (inf -> 14.2931). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 70.468, Train metrics: {'f1_per_class': array([0.31811487, 0.58629662, 0.53460208]), 'f1_micro': 0.5053547523427041, 'f1_macro': 0.47967118948666254, 'precision_per_class': array([0.45762712, 0.53481013, 0.42328767]), 'precision_micro': 0.47246558197747185, 'precision_macro': 0.47190830548640766, 'recall_per_class': array([0.24379233, 0.6487524 , 0.72535211]), 'recall_micro': 0.5431654676258992, 'recall_macro': 0.5392989456549119, 'mcc_per_class': [0.021628933076055148, 0.050330906314530806, 0.006276030895460201], 'mcc_overall': 0.030018197316919955}\n",
      "            Val Loss:   13.893, Val F1:   {'f1_per_class': array([0.13333333, 0.5511811 , 0.44444444]), 'f1_micro': 0.4271186440677966, 'f1_macro': 0.3763196267133275, 'precision_per_class': array([0.8       , 0.56451613, 0.43636364]), 'precision_micro': 0.5163934426229508, 'precision_macro': 0.6002932551319649, 'recall_per_class': array([0.07272727, 0.53846154, 0.45283019]), 'recall_micro': 0.36416184971098264, 'recall_macro': 0.3546729999560188, 'mcc_per_class': [0.14898662284988504, 0.09580235148669548, 0.02804249260202183], 'mcc_overall': 0.0813670891933906}\n",
      "*****\n",
      "Validation loss decreased (14.2931 -> 13.8934). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 49.936, Train metrics: {'f1_per_class': array([0.22108844, 0.65373134, 0.55357143]), 'f1_micro': 0.5341772151898734, 'f1_macro': 0.47613040240972015, 'precision_per_class': array([0.44827586, 0.53479853, 0.42307692]), 'precision_micro': 0.4768361581920904, 'precision_macro': 0.4687171066481411, 'recall_per_class': array([0.14672686, 0.84069098, 0.80046948]), 'recall_micro': 0.6071942446043166, 'recall_macro': 0.5959624415857715, 'mcc_per_class': [0.008326121369759808, 0.08045047792037509, 0.0068745935077865064], 'mcc_overall': 0.04405908758258863}\n",
      "            Val Loss:   8.568, Val F1:   {'f1_per_class': array([0.31428571, 0.6779661 , 0.55263158]), 'f1_micro': 0.5664160401002506, 'f1_macro': 0.514961131642666, 'precision_per_class': array([0.73333333, 0.53571429, 0.42424242]), 'precision_micro': 0.5, 'precision_macro': 0.5644300144300144, 'recall_per_class': array([0.2       , 0.92307692, 0.79245283]), 'recall_micro': 0.653179190751445, 'recall_macro': 0.6385099177552008, 'mcc_per_class': [0.220011419574196, 0.11229577231219764, 0.013993091357225214], 'mcc_overall': 0.10359825163954677}\n",
      "*****\n",
      "Validation loss decreased (13.8934 -> 8.5680). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 26.588, Train metrics: {'f1_per_class': array([0.14716981, 0.67073999, 0.58599696]), 'f1_micro': 0.553512209828158, 'f1_macro': 0.46796891786766387, 'precision_per_class': array([0.44827586, 0.51890756, 0.43355856]), 'precision_micro': 0.4763881681370005, 'precision_macro': 0.46691399455091137, 'recall_per_class': array([0.08803612, 0.94817658, 0.90375587]), 'recall_micro': 0.660431654676259, 'recall_macro': 0.6466561898064577, 'mcc_per_class': [0.006243689866910038, 0.02874415091113438, 0.06635189746300021], 'mcc_overall': 0.04793947402915012}\n",
      "            Val Loss:   3.070, Val F1:   {'f1_per_class': array([0.09090909, 0.68817204, 0.56804734]), 'f1_micro': 0.5463182897862233, 'f1_macro': 0.44904282373265003, 'precision_per_class': array([0.27272727, 0.52892562, 0.4137931 ]), 'precision_micro': 0.4637096774193548, 'precision_macro': 0.4051486653367531, 'recall_per_class': array([0.05454545, 0.98461538, 0.90566038]), 'recall_micro': 0.6647398843930635, 'recall_macro': 0.6482737388397766, 'mcc_per_class': [-0.10213393750970601, 0.12848540365587144, -0.04720322165051611], 'mcc_overall': 0.016738592342294505}\n",
      "*****\n",
      "Validation loss decreased (8.5680 -> 3.0696). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 73.717, Train metrics: {'f1_per_class': array([0.18518519, 0.39211137, 0.58388766]), 'f1_micro': 0.4457350272232305, 'f1_macro': 0.38706140371769554, 'precision_per_class': array([0.51546392, 0.49560117, 0.42610572]), 'precision_micro': 0.44981684981684983, 'precision_macro': 0.47905693597138477, 'recall_per_class': array([0.11286682, 0.3243762 , 0.92723005]), 'recall_micro': 0.441726618705036, 'recall_macro': 0.4548243545734119, 'mcc_per_class': [0.05074309012260139, -0.02816470035244984, 0.031894373118513106], 'mcc_overall': -0.01538750062749235}\n",
      "            Val Loss:   2.449, Val F1:   {'f1_per_class': array([0.13114754, 0.03030303, 0.59550562]), 'f1_micro': 0.380327868852459, 'f1_macro': 0.25231872975472164, 'precision_per_class': array([0.66666667, 1.        , 0.424     ]), 'precision_micro': 0.4393939393939394, 'precision_macro': 0.6968888888888888, 'recall_per_class': array([0.07272727, 0.01538462, 1.        ]), 'recall_micro': 0.3352601156069364, 'recall_macro': 0.3627039627039627, 'mcc_per_class': [0.10377015870985441, 0.08664694055586906, 0.0762116735027003], 'mcc_overall': -0.02687449654274126}\n",
      "*****\n",
      "Validation loss decreased (3.0696 -> 2.4493). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 61.048, Train metrics: {'f1_per_class': array([0.12648221, 0.39520958, 0.59042173]), 'f1_micro': 0.44525547445255476, 'f1_macro': 0.3707045080280212, 'precision_per_class': array([0.50793651, 0.52547771, 0.42446043]), 'precision_micro': 0.45185185185185184, 'precision_macro': 0.4859582155325179, 'recall_per_class': array([0.07223476, 0.31669866, 0.96948357]), 'recall_micro': 0.43884892086330934, 'recall_macro': 0.4528056624949146, 'mcc_per_class': [0.03624314185560027, 0.013626760095809428, 0.0317216921375158], 'mcc_overall': -0.011576439480852276}\n",
      "            Val Loss:   7.177, Val F1:   {'f1_per_class': array([0.07017544, 0.6746988 , 0.59428571]), 'f1_micro': 0.5527638190954773, 'f1_macro': 0.44638664935430944, 'precision_per_class': array([1.        , 0.55445545, 0.42622951]), 'precision_micro': 0.4888888888888889, 'precision_macro': 0.6602283179137586, 'recall_per_class': array([0.03636364, 0.86153846, 0.98113208]), 'recall_micro': 0.6358381502890174, 'recall_macro': 0.626344724457932, 'mcc_per_class': [0.14429523335682806, 0.15517766229633997, 0.06258772639834544], 'mcc_overall': 0.07598494769620573}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 52.742, Train metrics: {'f1_per_class': array([0.1010101 , 0.64604317, 0.58369099]), 'f1_micro': 0.5373134328358209, 'f1_macro': 0.44358141786739685, 'precision_per_class': array([0.48076923, 0.51668585, 0.41975309]), 'precision_micro': 0.4659270998415214, 'precision_macro': 0.4724027209962512, 'recall_per_class': array([0.05643341, 0.86180422, 0.95774648]), 'recall_micro': 0.6345323741007194, 'recall_macro': 0.6253280366999566, 'mcc_per_class': [0.01998789628220444, 0.00670464826822293, -0.016296975351083192], 'mcc_overall': 0.01974590320527075}\n",
      "            Val Loss:   4.222, Val F1:   {'f1_per_class': array([0.125     , 0.67759563, 0.59090909]), 'f1_micro': 0.557919621749409, 'f1_macro': 0.4645015731081305, 'precision_per_class': array([0.44444444, 0.52542373, 0.42276423]), 'precision_micro': 0.472, 'precision_macro': 0.4642108003000934, 'recall_per_class': array([0.07272727, 0.95384615, 0.98113208]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6692351673483748, 'mcc_per_class': [0.004438311756502355, 0.07339741275840792, 0.02761858293969059], 'mcc_overall': 0.04019237715946228}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 8, Train Loss: 108.619, Train metrics: {'f1_per_class': array([0.13017751, 0.67443454, 0.59048971]), 'f1_micro': 0.5576296296296296, 'f1_macro': 0.465033922671582, 'precision_per_class': array([0.515625  , 0.52452026, 0.4231943 ]), 'precision_micro': 0.47405541561712844, 'precision_macro': 0.48777985300571697, 'recall_per_class': array([0.0744921 , 0.94433781, 0.97652582]), 'recall_micro': 0.676978417266187, 'recall_macro': 0.6651185776064117, 'mcc_per_class': [0.040577323305204645, 0.06590795017080349, 0.02195105367547969], 'mcc_overall': 0.04354044491207749}\n",
      "            Val Loss:   3.969, Val F1:   {'f1_per_class': array([0.0952381 , 0.68478261, 0.59428571]), 'f1_micro': 0.5592417061611374, 'f1_macro': 0.45810213940648725, 'precision_per_class': array([0.375     , 0.52941176, 0.42622951]), 'precision_micro': 0.4738955823293173, 'precision_macro': 0.44354709096753453, 'recall_per_class': array([0.05454545, 0.96923077, 0.98113208]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.668302766415974, 'mcc_per_class': [-0.03229201705937779, 0.11169968320035786, 0.06258772639834544], 'mcc_overall': 0.045242284484121514}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.925203949213028,\n",
       " {'f1_per_class': array([0.12698413, 0.65921788, 0.58426966]),\n",
       "  'f1_micro': 0.5476190476190477,\n",
       "  'f1_macro': 0.4568238890001491,\n",
       "  'precision_per_class': array([0.57142857, 0.51754386, 0.416     ]),\n",
       "  'precision_micro': 0.46747967479674796,\n",
       "  'precision_macro': 0.501657477025898,\n",
       "  'recall_per_class': array([0.07142857, 0.90769231, 0.98113208]),\n",
       "  'recall_micro': 0.6609195402298851,\n",
       "  'recall_macro': 0.6534176515308591,\n",
       "  'mcc_per_class': [0.06347389635014633,\n",
       "   0.033962642018834095,\n",
       "   -0.02120779397396682],\n",
       "  'mcc_overall': 0.029231891693933854})"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EGNN no PCA\n",
    "trained_egnn_nopca = train(egnn_nopca, opt_egnn_nopca, train_loader_nopca, val_loader_nopca, model_arch=\"EGNN\", scheduler=scheduler_egnn_nopca, epochs=100, patience=3)\n",
    "print(\"\\nBest model:\")\n",
    "trained_egnn_nopca.eval()\n",
    "val_loss, val_metrics = evaluate(trained_egnn_nopca, test_loader_nopca, model_arch=\"EGNN\")\n",
    "val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9fa19ab7-b058-4ad0-b21e-881b9581f180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.952672839164734,\n",
       " {'f1_per_class': array([0.12698413, 0.08571429, 0.2       ]),\n",
       "  'f1_micro': 0.13793103448275862,\n",
       "  'f1_macro': 0.13756613756613756,\n",
       "  'precision_per_class': array([0.57142857, 0.6       , 0.41176471]),\n",
       "  'precision_micro': 0.4827586206896552,\n",
       "  'precision_macro': 0.5277310924369747,\n",
       "  'recall_per_class': array([0.07142857, 0.04615385, 0.13207547]),\n",
       "  'recall_micro': 0.08045977011494253,\n",
       "  'recall_macro': 0.0832192964268436,\n",
       "  'mcc_per_class': [0.06347389635014633,\n",
       "   0.03571663917955086,\n",
       "   -0.004431049974208991],\n",
       "  'mcc_overall': 0.01501973810980609})"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_egnn_nopca.eval()\n",
    "evaluate(trained_egnn_nopca, test_loader_nopca, model_arch=\"EGNN\", threshold=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776c67f-fe20-46df-b3c8-ee89dd04d170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "57cbd7c3-f4b8-4cc1-80a4-b0d05eea7205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with n_layers=1, n_layers_egnn=1, hidden_dim=32\n",
      "Epoch 1, Train Loss: 57.232, Train metrics: {'f1_per_class': array([0.42012579, 0.61742984, 0.34993614]), 'f1_micro': 0.4924965893587995, 'f1_macro': 0.46249725557385907, 'precision_per_class': array([0.47443182, 0.50180072, 0.3837535 ]), 'precision_micro': 0.4682230869001297, 'precision_macro': 0.45332867995683124, 'recall_per_class': array([0.37697517, 0.80230326, 0.32159624]), 'recall_micro': 0.5194244604316547, 'recall_macro': 0.5002915587958451, 'mcc_per_class': [0.053399022271381026, -0.0585686111988711, -0.05627741276482471], 'mcc_overall': 0.02026941093045445}\n",
      "            Val Loss:   3.920, Val F1:   {'f1_per_class': array([0.21621622, 0.68085106, 0.25      ]), 'f1_micro': 0.4742857142857143, 'f1_macro': 0.3823557600153345, 'precision_per_class': array([0.42105263, 0.5203252 , 0.31428571]), 'precision_micro': 0.4689265536723164, 'precision_macro': 0.41855451637223134, 'recall_per_class': array([0.14545455, 0.98461538, 0.20754717]), 'recall_micro': 0.4797687861271676, 'recall_macro': 0.4458723666270836, 'mcc_per_class': [-0.013131747264432156, 0.057044570137671634, -0.13360378426991812], 'mcc_overall': 0.02119875385757953}\n",
      "*****\n",
      "Validation loss decreased (inf -> 3.9202). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 22.544, Train metrics: {'f1_per_class': array([0.14606742, 0.66800805, 0.18532819]), 'f1_micro': 0.4600865119937082, 'f1_macro': 0.33313454978275364, 'precision_per_class': array([0.42857143, 0.51340206, 0.52173913]), 'precision_micro': 0.5073720728534259, 'precision_macro': 0.48790420695396036, 'recall_per_class': array([0.08803612, 0.95585413, 0.11267606]), 'recall_micro': 0.420863309352518, 'recall_macro': 0.38552210013299354, 'mcc_per_class': [-0.006090621682089572, -0.01877700988521807, 0.06431709805157586], 'mcc_overall': 0.07714114739019856}\n",
      "            Val Loss:   3.218, Val F1:   {'f1_per_class': array([0.06666667, 0.67741935, 0.13333333]), 'f1_micro': 0.45098039215686275, 'f1_macro': 0.29247311827956984, 'precision_per_class': array([0.4       , 0.52066116, 0.57142857]), 'precision_micro': 0.518796992481203, 'precision_macro': 0.49736324281778826, 'recall_per_class': array([0.03636364, 0.96923077, 0.0754717 ]), 'recall_micro': 0.3988439306358382, 'recall_macro': 0.3603553679025377, 'mcc_per_class': [-0.014963721945621641, 0.04713283651697796, 0.07408491303237447], 'mcc_overall': 0.09039693707507944}\n",
      "*****\n",
      "Validation loss decreased (3.9202 -> 3.2179). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 46.140, Train metrics: {'f1_per_class': array([0.01724138, 0.67558528, 0.09185804]), 'f1_micro': 0.43560295324036097, 'f1_macro': 0.26156156705652317, 'precision_per_class': array([0.19047619, 0.51848049, 0.41509434]), 'precision_micro': 0.5066793893129771, 'precision_macro': 0.37468367430399124, 'recall_per_class': array([0.00902935, 0.96928983, 0.05164319]), 'recall_micro': 0.38201438848920866, 'recall_macro': 0.3433207883720006, 'mcc_per_class': [-0.07271096276677996, 0.03232991211983551, -0.0029870043526573726], 'mcc_overall': 0.07056311232323798}\n",
      "            Val Loss:   1.432, Val F1:   {'f1_per_class': array([0.09677419, 0.68108108, 0.03636364]), 'f1_micro': 0.44370860927152317, 'f1_macro': 0.2714063036643682, 'precision_per_class': array([0.42857143, 0.525     , 0.5       ]), 'precision_micro': 0.5193798449612403, 'precision_macro': 0.4845238095238095, 'recall_per_class': array([0.05454545, 0.96923077, 0.01886792]), 'recall_micro': 0.3872832369942196, 'recall_macro': 0.3475480494348419, 'mcc_per_class': [-0.003881191040835057, 0.08167539478627002, 0.02041760465737363], 'mcc_overall': 0.0891512992879644}\n",
      "*****\n",
      "Validation loss decreased (3.2179 -> 1.4320). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 6.820, Train metrics: {'f1_per_class': array([0.04301075, 0.66710963, 0.05393258]), 'f1_micro': 0.4339544513457557, 'f1_macro': 0.2546843238364433, 'precision_per_class': array([0.45454545, 0.5101626 , 0.63157895]), 'precision_micro': 0.511219512195122, 'precision_macro': 0.5320956678466305, 'recall_per_class': array([0.02257336, 0.96353167, 0.02816901]), 'recall_micro': 0.376978417266187, 'recall_macro': 0.3380913491271004, 'mcc_per_class': [0.004919442247482747, -0.06243609594887732, 0.05891840012478763], 'mcc_overall': 0.07589388817510959}\n",
      "            Val Loss:   1.719, Val F1:   {'f1_per_class': array([0.03448276, 0.68062827, 0.10526316]), 'f1_micro': 0.45098039215686275, 'f1_macro': 0.2734580629222451, 'precision_per_class': array([0.33333333, 0.51587302, 0.75      ]), 'precision_micro': 0.518796992481203, 'precision_macro': 0.5330687830687831, 'recall_per_class': array([0.01818182, 1.        , 0.05660377]), 'recall_micro': 0.3988439306358382, 'recall_macro': 0.35826186392224124, 'mcc_per_class': [-0.032489341183349514, 0.0, 0.12080886723401561], 'mcc_overall': 0.09039693707507944}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 4.229, Train metrics: {'f1_per_class': array([0.0130719 , 0.67934426, 0.06521739]), 'f1_micro': 0.4386252045826514, 'f1_macro': 0.2525445163414221, 'precision_per_class': array([0.1875    , 0.51593625, 0.44117647]), 'precision_micro': 0.50853889943074, 'precision_macro': 0.3815375751894383, 'recall_per_class': array([0.00677201, 0.99424184, 0.03521127]), 'recall_micro': 0.3856115107913669, 'recall_macro': 0.34540837308178124, 'mcc_per_class': [-0.06406831407128485, 0.014495579864840553, 0.007484755859598778], 'mcc_overall': 0.0735956029224904}\n",
      "            Val Loss:   1.520, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.10526316]), 'f1_micro': 0.4459016393442623, 'f1_macro': 0.2619638100486819, 'precision_per_class': array([0.        , 0.51587302, 0.75      ]), 'precision_micro': 0.5151515151515151, 'precision_macro': 0.4219576719576719, 'recall_per_class': array([0.        , 1.        , 0.05660377]), 'recall_micro': 0.3930635838150289, 'recall_macro': 0.3522012578616352, 'mcc_per_class': [-0.11177799767078231, 0.0, 0.12080886723401561], 'mcc_overall': 0.08451321939098895}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 2.193, Train metrics: {'f1_per_class': array([0.01758242, 0.67889908, 0.02672606]), 'f1_micro': 0.4345679012345679, 'f1_macro': 0.24106918601922792, 'precision_per_class': array([0.33333333, 0.51542289, 0.26086957]), 'precision_micro': 0.5076923076923077, 'precision_macro': 0.369875261374288, 'recall_per_class': array([0.00902935, 0.99424184, 0.01408451]), 'recall_micro': 0.37985611510791367, 'recall_macro': 0.3391185650083595, 'mcc_per_class': [-0.02315995510614343, 0.0023703222369104377, -0.04959255825833376], 'mcc_overall': 0.07162058229858717}\n",
      "            Val Loss:   1.247, Val F1:   {'f1_per_class': array([0.07017544, 0.68062827, 0.07272727]), 'f1_micro': 0.45544554455445546, 'f1_macro': 0.27451032785835766, 'precision_per_class': array([1.        , 0.51587302, 1.        ]), 'precision_micro': 0.5307692307692308, 'precision_macro': 0.8386243386243386, 'recall_per_class': array([0.03636364, 1.        , 0.03773585]), 'recall_micro': 0.3988439306358382, 'recall_macro': 0.3580331618067467, 'mcc_per_class': [0.14429523335682806, 0.0, 0.1490485139988275], 'mcc_overall': 0.10622795705569232}\n",
      "*****\n",
      "Validation loss decreased (1.4320 -> 1.2469). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 2.019, Train metrics: {'f1_per_class': array([0.00440529, 0.68015666, 0.03160271]), 'f1_micro': 0.43557019349526555, 'f1_macro': 0.2387215510368902, 'precision_per_class': array([0.09090909, 0.51533136, 0.41176471]), 'precision_micro': 0.5091434071222329, 'precision_macro': 0.33933505062847, 'recall_per_class': array([0.00225734, 1.        , 0.01643192]), 'recall_micro': 0.38057553956834533, 'recall_macro': 0.339563087075248, 'mcc_per_class': [-0.0734074090819475, 0.0, -0.002542632294382551], 'mcc_overall': 0.07367045891062733}\n",
      "            Val Loss:   0.856, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.07017544]), 'f1_micro': 0.4473684210526316, 'f1_macro': 0.2621726655206953, 'precision_per_class': array([1.        , 0.51587302, 0.5       ]), 'precision_micro': 0.5190839694656488, 'precision_macro': 0.671957671957672, 'recall_per_class': array([0.01818182, 1.        , 0.03773585]), 'recall_micro': 0.3930635838150289, 'recall_macro': 0.3519725557461406, 'mcc_per_class': [0.10162318990896088, 0.0, 0.029110570417835088], 'mcc_overall': 0.08977022497810772}\n",
      "*****\n",
      "Validation loss decreased (1.2469 -> 0.8563). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 1.152, Train metrics: {'f1_per_class': array([0.00894855, 0.68015666, 0.004662  ]), 'f1_micro': 0.43521594684385384, 'f1_macro': 0.23125573616224956, 'precision_per_class': array([0.5       , 0.51533136, 0.33333333]), 'precision_micro': 0.5147347740667977, 'precision_macro': 0.4495548961424332, 'recall_per_class': array([0.00451467, 1.        , 0.00234742]), 'recall_micro': 0.376978417266187, 'recall_macro': 0.3356206968422019, 'mcc_per_class': [0.007852704349140757, 0.0, -0.009726075474057384], 'mcc_overall': 0.08051745129386302}\n",
      "            Val Loss:   0.755, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43333333333333335, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5118110236220472, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.0762116735027003], 'mcc_overall': 0.07729787879473653}\n",
      "*****\n",
      "Validation loss decreased (0.8563 -> 0.7552). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 0.783, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.0046729 ]), 'f1_micro': 0.4340956340956341, 'f1_macro': 0.22827651838656937, 'precision_per_class': array([0.        , 0.51533136, 0.5       ]), 'precision_micro': 0.5142857142857142, 'precision_macro': 0.33844378503132216, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [-0.039318515283926804, 0.0, 0.007090123675145835], 'mcc_overall': 0.07969976020651748}\n",
      "            Val Loss:   0.755, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7552 -> 0.7552). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 0.851, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.0046729 ]), 'f1_micro': 0.43427620632279534, 'f1_macro': 0.22827651838656937, 'precision_per_class': array([0.        , 0.51533136, 0.5       ]), 'precision_micro': 0.514792899408284, 'precision_macro': 0.33844378503132216, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [-0.027788621816138795, 0.0, 0.007090123675145835], 'mcc_overall': 0.08036213850775714}\n",
      "            Val Loss:   0.755, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7552 -> 0.7549). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.755, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7549 -> 0.7548). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 0.806, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.00932401]), 'f1_micro': 0.43492723492723495, 'f1_macro': 0.2298268890958186, 'precision_per_class': array([0.        , 0.51533136, 0.66666667]), 'precision_micro': 0.5152709359605911, 'precision_macro': 0.39399934058687763, 'recall_per_class': array([0.        , 1.        , 0.00469484]), 'recall_micro': 0.3762589928057554, 'recall_macro': 0.3348982785602504, 'mcc_per_class': [-0.027788621816138795, 0.0, 0.02710187323108125], 'mcc_overall': 0.08110209796065011}\n",
      "            Val Loss:   0.755, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7548 -> 0.7547). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.776, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.027788621816138795, 0.0, 0.0], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.754, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7547 -> 0.7542). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 14, Train Loss: 0.759, Train metrics: {'f1_per_class': array([0.        , 0.67929458, 0.        ]), 'f1_micro': 0.43297252289758537, 'f1_macro': 0.22643152623557586, 'precision_per_class': array([0.        , 0.51485149, 0.        ]), 'precision_micro': 0.5138339920948617, 'precision_macro': 0.1716171617161716, 'recall_per_class': array([0.        , 0.99808061, 0.        ]), 'recall_micro': 0.37410071942446044, 'recall_macro': 0.33269353806781826, 'mcc_per_class': [-0.027788621816138795, -0.030515359552541615, -0.02685135334467745], 'mcc_overall': 0.07888126049304595}\n",
      "            Val Loss:   0.754, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7542 -> 0.7538). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 15, Train Loss: 0.765, Train metrics: {'f1_per_class': array([0.        , 0.67929458, 0.00468384]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2279928064853807, 'precision_per_class': array([0.        , 0.51485149, 1.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.504950495049505, 'recall_per_class': array([0.        , 0.99808061, 0.00234742]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3334760106812768, 'mcc_per_class': [-0.027788621816138795, -0.030515359552541615, 0.036873337339521854], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7538 -> 0.7535). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 16, Train Loss: 0.759, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7535 -> 0.7532). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 17, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 18, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 19, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7532 -> 0.7531). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 20, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.        , 0.67929458, 0.        ]), 'f1_micro': 0.4331528529779259, 'f1_macro': 0.22643152623557586, 'precision_per_class': array([0.        , 0.51485149, 0.        ]), 'precision_micro': 0.5143422354104846, 'precision_macro': 0.1716171617161716, 'recall_per_class': array([0.        , 0.99808061, 0.        ]), 'recall_micro': 0.37410071942446044, 'recall_macro': 0.33269353806781826, 'mcc_per_class': [-0.027788621816138795, -0.030515359552541615, 0.0], 'mcc_overall': 0.07954405774845993}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7531 -> 0.7531). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 21, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.67929458, 0.        ]), 'f1_micro': 0.4331528529779259, 'f1_macro': 0.22643152623557586, 'precision_per_class': array([0.        , 0.51485149, 0.        ]), 'precision_micro': 0.5143422354104846, 'precision_macro': 0.1716171617161716, 'recall_per_class': array([0.        , 0.99808061, 0.        ]), 'recall_micro': 0.37410071942446044, 'recall_macro': 0.33269353806781826, 'mcc_per_class': [-0.027788621816138795, -0.030515359552541615, 0.0], 'mcc_overall': 0.07954405774845993}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7531 -> 0.7524). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 22, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 23, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7524 -> 0.7523). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 24, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 25, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7523 -> 0.7520). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 26, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.        , 0.67929458, 0.        ]), 'f1_micro': 0.4331528529779259, 'f1_macro': 0.22643152623557586, 'precision_per_class': array([0.        , 0.51485149, 0.        ]), 'precision_micro': 0.5143422354104846, 'precision_macro': 0.1716171617161716, 'recall_per_class': array([0.        , 0.99808061, 0.        ]), 'recall_micro': 0.37410071942446044, 'recall_macro': 0.33269353806781826, 'mcc_per_class': [-0.027788621816138795, -0.030515359552541615, 0.0], 'mcc_overall': 0.07954405774845993}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 27, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 28, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7520 -> 0.7520). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 29, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.027788621816138795, 0.0, 0.0], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7520 -> 0.7514). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 30, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7514 -> 0.7511). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 31, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 32, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 33, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 34, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43362463587182687, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5143139190523198, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.027788621816138795, 0.0, -0.02685135334467745], 'mcc_overall': 0.07962185982611598}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "Validation loss decreased (0.7511 -> 0.7467). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 35, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 36, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 37, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.02685135334467745], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 38, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.748, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 39, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43362463587182687, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5143139190523198, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, -0.03799236082530976], 'mcc_overall': 0.07962185982611598}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 40, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.43398583923365264, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5153313550939663, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08094777641460923}\n",
      "            Val Loss:   0.748, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 41, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68015666, 0.        ]), 'f1_micro': 0.4338051623646961, 'f1_macro': 0.2267188859878155, 'precision_per_class': array([0.        , 0.51533136, 0.        ]), 'precision_micro': 0.5148221343873518, 'precision_macro': 0.17177711836465545, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3748201438848921, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.027788621816138795, 0.0, 0.0], 'mcc_overall': 0.08028463251953238}\n",
      "            Val Loss:   0.748, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "[{'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 32, 'val_loss': 0.9719278365373611, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.429042904290429, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5038759689922481, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.07911877721292356, 0.0, -0.0753940029727543], 'mcc_overall': 0.06777130152946205}}]\n",
      "\n",
      "Training model with n_layers=1, n_layers_egnn=1, hidden_dim=64\n",
      "Epoch 1, Train Loss: 126.772, Train metrics: {'f1_per_class': array([0.46403242, 0.63810252, 0.51589464]), 'f1_micro': 0.5478645066273933, 'f1_macro': 0.5393431958601921, 'precision_per_class': array([0.42095588, 0.53053435, 0.42074074]), 'precision_micro': 0.46384039900249374, 'precision_macro': 0.45741032474624, 'recall_per_class': array([0.51693002, 0.80038388, 0.66666667]), 'recall_micro': 0.6690647482014388, 'recall_macro': 0.6613268554664463, 'mcc_per_class': [-0.03746740512023671, 0.05685693581375467, -0.001791865226114611], 'mcc_overall': 0.015551244539593804}\n",
      "            Val Loss:   51.592, Val F1:   {'f1_per_class': array([0.50420168, 0.64088398, 0.47482014]), 'f1_micro': 0.55125284738041, 'f1_macro': 0.5399686008192378, 'precision_per_class': array([0.46875   , 0.5       , 0.38372093]), 'precision_micro': 0.4548872180451128, 'precision_macro': 0.45082364341085274, 'recall_per_class': array([0.54545455, 0.89230769, 0.62264151]), 'recall_micro': 0.6994219653179191, 'recall_macro': 0.6868012490654, 'mcc_per_class': [0.06605063498728045, -0.10817761304766826, -0.10964312501714193], 'mcc_overall': -0.00861406648881949}\n",
      "*****\n",
      "Validation loss decreased (inf -> 51.5915). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 85.064, Train metrics: {'f1_per_class': array([0.2764977 , 0.64752333, 0.55615802]), 'f1_micro': 0.5397301349325337, 'f1_macro': 0.49339301461133483, 'precision_per_class': array([0.43269231, 0.51720183, 0.4150289 ]), 'precision_micro': 0.46272493573264784, 'precision_macro': 0.45497434809626575, 'recall_per_class': array([0.20316027, 0.86564299, 0.842723  ]), 'recall_micro': 0.6474820143884892, 'recall_macro': 0.6371754232723466, 'mcc_per_class': [-0.005629117806977432, 0.009374284102988987, -0.031233520302950988], 'mcc_overall': 0.011895186644780056}\n",
      "            Val Loss:   8.557, Val F1:   {'f1_per_class': array([0.24      , 0.64835165, 0.55294118]), 'f1_micro': 0.5386416861826698, 'f1_macro': 0.4804309416074122, 'precision_per_class': array([0.45     , 0.5042735, 0.4017094]), 'precision_micro': 0.452755905511811, 'precision_macro': 0.451994301994302, 'recall_per_class': array([0.16363636, 0.90769231, 0.88679245]), 'recall_micro': 0.6647398843930635, 'recall_macro': 0.6527070413862867, 'mcc_per_class': [0.01181681584497013, -0.08368744886125766, -0.13822629035882647], 'mcc_overall': -0.014122588848041446}\n",
      "*****\n",
      "Validation loss decreased (51.5915 -> 8.5571). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 63.230, Train metrics: {'f1_per_class': array([0.21686747, 0.67040673, 0.58942795]), 'f1_micro': 0.5596221959858324, 'f1_macro': 0.49223405091921624, 'precision_per_class': array([0.45652174, 0.5281768 , 0.42617801]), 'precision_micro': 0.4744744744744745, 'precision_macro': 0.47029218172724985, 'recall_per_class': array([0.14221219, 0.91746641, 0.95539906]), 'recall_micro': 0.6820143884892086, 'recall_macro': 0.6716925537992258, 'mcc_per_class': [0.014697639464741029, 0.07510253774771872, 0.04025262808251583], 'mcc_overall': 0.04512482236667239}\n",
      "            Val Loss:   3.628, Val F1:   {'f1_per_class': array([0.13333333, 0.58536585, 0.58757062]), 'f1_micro': 0.5187032418952618, 'f1_macro': 0.43542326948693216, 'precision_per_class': array([0.8       , 0.48484848, 0.41935484]), 'precision_micro': 0.45614035087719296, 'precision_macro': 0.5680677745193875, 'recall_per_class': array([0.07272727, 0.73846154, 0.98113208]), 'recall_micro': 0.6011560693641619, 'recall_macro': 0.5974402955535031, 'mcc_per_class': [0.14898662284988504, -0.1188748247834231, -0.02041760465737363], 'mcc_overall': -0.003790189255080576}\n",
      "*****\n",
      "Validation loss decreased (8.5571 -> 3.6276). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 36.780, Train metrics: {'f1_per_class': array([0.05944798, 0.63908702, 0.59447983]), 'f1_micro': 0.5368228849665246, 'f1_macro': 0.43100494390280586, 'precision_per_class': array([0.5       , 0.50851305, 0.42553191]), 'precision_micro': 0.4651898734177215, 'precision_macro': 0.4780149894140282, 'recall_per_class': array([0.03160271, 0.85988484, 0.98591549]), 'recall_micro': 0.6345323741007194, 'recall_macro': 0.6258010128711885, 'mcc_per_class': [0.021028400658673587, -0.035516220948732594, 0.0541174787408586], 'mcc_overall': 0.01787692535915909}\n",
      "            Val Loss:   10.210, Val F1:   {'f1_per_class': array([0.03571429, 0.63687151, 0.58426966]), 'f1_micro': 0.5326876513317191, 'f1_macro': 0.41895181900517403, 'precision_per_class': array([1.   , 0.5  , 0.416]), 'precision_micro': 0.4583333333333333, 'precision_macro': 0.6386666666666666, 'recall_per_class': array([0.01818182, 0.87692308, 0.98113208]), 'recall_micro': 0.6358381502890174, 'recall_macro': 0.6254123235255311, 'mcc_per_class': [0.10162318990896088, -0.09789718465914449, -0.10497079557919099], 'mcc_overall': 0.001750677314891145}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 24.278, Train metrics: {'f1_per_class': array([0.0173913 , 0.67167235, 0.58831809]), 'f1_micro': 0.5463239689181112, 'f1_macro': 0.42579391505055436, 'precision_per_class': array([0.23529412, 0.52118644, 0.4201005 ]), 'precision_micro': 0.467280163599182, 'precision_macro': 0.3921936869458626, 'recall_per_class': array([0.00902935, 0.94433781, 0.98122066]), 'recall_micro': 0.6575539568345323, 'recall_macro': 0.6448626048498826, 'mcc_per_class': [-0.053475919828700445, 0.04397603780193437, -0.020194488011077025], 'mcc_overall': 0.024310168655198257}\n",
      "            Val Loss:   4.170, Val F1:   {'f1_per_class': array([0.        , 0.66666667, 0.58757062]), 'f1_micro': 0.5454545454545454, 'f1_macro': 0.4180790960451977, 'precision_per_class': array([0.        , 0.51239669, 0.41935484]), 'precision_micro': 0.46530612244897956, 'precision_macro': 0.3105838443081845, 'recall_per_class': array([0.        , 0.95384615, 0.98113208]), 'recall_micro': 0.6589595375722543, 'recall_macro': 0.6449927431059507, 'mcc_per_class': [0.0, -0.0342197306219155, -0.02041760465737363], 'mcc_overall': 0.020797472994494356}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 12.193, Train metrics: {'f1_per_class': array([0.01333333, 0.67112299, 0.59508772]), 'f1_micro': 0.5511717591219223, 'f1_macro': 0.4265146824279951, 'precision_per_class': array([0.42857143, 0.51487179, 0.42442442]), 'precision_micro': 0.4689550731953559, 'precision_macro': 0.4559558826225493, 'recall_per_class': array([0.00677201, 0.96353167, 0.99530516]), 'recall_micro': 0.6683453237410072, 'recall_macro': 0.6552029477380791, 'mcc_per_class': [-0.0016170260088887412, -0.0047855046450734256, 0.056533045195841784], 'mcc_overall': 0.029366899615130036}\n",
      "            Val Loss:   1.049, Val F1:   {'f1_per_class': array([0.        , 0.68508287, 0.59550562]), 'f1_micro': 0.5555555555555556, 'f1_macro': 0.4268628303019016, 'precision_per_class': array([0.        , 0.53448276, 0.424     ]), 'precision_micro': 0.47717842323651455, 'precision_macro': 0.3194942528735632, 'recall_per_class': array([0.        , 0.95384615, 1.        ]), 'recall_micro': 0.6647398843930635, 'recall_macro': 0.6512820512820513, 'mcc_per_class': [0.0, 0.12682892564209383, 0.0762116735027003], 'mcc_overall': 0.051930038054477747}\n",
      "*****\n",
      "Validation loss decreased (3.6276 -> 1.0491). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 2.942, Train metrics: {'f1_per_class': array([0.00445434, 0.6675694 , 0.5931612 ]), 'f1_micro': 0.5471866626972314, 'f1_macro': 0.42172831356358725, 'precision_per_class': array([0.16666667, 0.51569038, 0.42204568]), 'precision_micro': 0.46673438293550024, 'precision_macro': 0.368134241158012, 'recall_per_class': array([0.00225734, 0.9462572 , 0.99765258]), 'recall_micro': 0.6611510791366907, 'recall_macro': 0.6487223720664922, 'mcc_per_class': [-0.042282330069086334, 0.002995037622799235, 0.021872869965615827], 'mcc_overall': 0.023049268449821365}\n",
      "            Val Loss:   0.803, Val F1:   {'f1_per_class': array([0.        , 0.68085106, 0.59217877]), 'f1_micro': 0.5545023696682464, 'f1_macro': 0.424343278259836, 'precision_per_class': array([0.        , 0.5203252 , 0.42063492]), 'precision_micro': 0.46987951807228917, 'precision_macro': 0.3136533746289844, 'recall_per_class': array([0.        , 0.98461538, 1.        ]), 'recall_micro': 0.6763005780346821, 'recall_macro': 0.6615384615384615, 'mcc_per_class': [0.0, 0.057044570137671634, 0.0], 'mcc_overall': 0.034042819169781025}\n",
      "*****\n",
      "Validation loss decreased (1.0491 -> 0.8026). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 1.237, Train metrics: {'f1_per_class': array([0.0044843 , 0.67722772, 0.59233449]), 'f1_micro': 0.5530035335689046, 'f1_macro': 0.4246821741595106, 'precision_per_class': array([0.33333333, 0.51609658, 0.42120912]), 'precision_micro': 0.46809571286141577, 'precision_macro': 0.4235463435829159, 'recall_per_class': array([0.00225734, 0.98464491, 0.99765258]), 'recall_micro': 0.6755395683453237, 'recall_macro': 0.6615182773767928, 'mcc_per_class': [-0.011528165313076929, 0.011708244557768026, -0.007090123675145835], 'mcc_overall': 0.02749867304389891}\n",
      "            Val Loss:   0.757, Val F1:   {'f1_per_class': array([0.        , 0.68421053, 0.59217877]), 'f1_micro': 0.5566037735849056, 'f1_macro': 0.4254630990885034, 'precision_per_class': array([0.        , 0.52      , 0.42063492]), 'precision_micro': 0.4701195219123506, 'precision_macro': 0.31354497354497357, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.09232870714969654, 0.0], 'mcc_overall': 0.03512458440037854}\n",
      "*****\n",
      "Validation loss decreased (0.8026 -> 0.7566). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 1.053, Train metrics: {'f1_per_class': array([0.        , 0.67587114, 0.59414226]), 'f1_micro': 0.5527785945310203, 'f1_macro': 0.423337798941275, 'precision_per_class': array([0.        , 0.514     , 0.42261905]), 'precision_micro': 0.4674291397314769, 'precision_macro': 0.3122063492063492, 'recall_per_class': array([0.       , 0.9865643, 1.       ]), 'recall_micro': 0.6762589928057554, 'recall_macro': 0.6621880998080614, 'mcc_per_class': [-0.04817903050653849, -0.025399888820456688, 0.04655402417919602], 'mcc_overall': 0.025723578102179453}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7566 -> 0.7526). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 1.379, Train metrics: {'f1_per_class': array([0.        , 0.67803279, 0.5931612 ]), 'f1_micro': 0.5534665099882491, 'f1_macro': 0.42373132905479355, 'precision_per_class': array([0.        , 0.51494024, 0.42204568]), 'precision_micro': 0.46772591857000995, 'precision_macro': 0.3123286397607188, 'recall_per_class': array([0.        , 0.99232246, 0.99765258]), 'recall_micro': 0.6776978417266187, 'recall_macro': 0.663325012991148, 'mcc_per_class': [-0.04817903050653849, -0.009372549195996253, 0.021872869965615827], 'mcc_overall': 0.02661800289374162}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7526 -> 0.7524). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.785, Train metrics: {'f1_per_class': array([0.        , 0.68062827, 0.59372822]), 'f1_micro': 0.5550014667057788, 'f1_macro': 0.42478549841594154, 'precision_per_class': array([0.       , 0.5163853, 0.4222002]), 'precision_micro': 0.46854878652798415, 'precision_macro': 0.3128618336986322, 'recall_per_class': array([0.        , 0.99808061, 1.        ]), 'recall_micro': 0.6805755395683454, 'recall_macro': 0.6660268714011516, 'mcc_per_class': [-0.04817903050653849, 0.03346093631444005, 0.03799236082530976], 'mcc_overall': 0.02904702413242643}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 0.775, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59372822]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.42492465821016756, 'precision_per_class': array([0.        , 0.51635282, 0.4222002 ]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.3128510075982821, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [-0.027788621816138795, 0.045908196972884785, 0.03799236082530976], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7524 -> 0.7521). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.        , 0.68149117, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.42493531131033385, 'precision_per_class': array([0.        , 0.51686508, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31288241919430043, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [-0.027788621816138795, 0.056253711679722825, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7521 -> 0.7520). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 14, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.        , 0.68149117, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.42493531131033385, 'precision_per_class': array([0.        , 0.51686508, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31288241919430043, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [-0.027788621816138795, 0.056253711679722825, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 15, Train Loss: 1.009, Train metrics: {'f1_per_class': array([0.        , 0.68018313, 0.59290188]), 'f1_micro': 0.5550014667057788, 'f1_macro': 0.42436166838023226, 'precision_per_class': array([0.        , 0.51587302, 0.42136499]), 'precision_micro': 0.46854878652798415, 'precision_macro': 0.31241266701207354, 'recall_per_class': array([0.        , 0.99808061, 1.        ]), 'recall_micro': 0.6805755395683454, 'recall_macro': 0.6660268714011516, 'mcc_per_class': [0.0, 0.019866953836984644, 0.0], 'mcc_overall': 0.02904702413242643}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 16, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246385592220692, 'precision_per_class': array([0.        , 0.51584158, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125412541254125, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7520 -> 0.7517). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 17, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 18, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [-0.027788621816138795, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 19, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.0045045 , 0.68104575, 0.59274756]), 'f1_micro': 0.5557511737089202, 'f1_macro': 0.4260992718044158, 'precision_per_class': array([1.        , 0.51635282, 0.42162698]), 'precision_micro': 0.4692765113974232, 'precision_macro': 0.6459932695685917, 'recall_per_class': array([0.00225734, 1.        , 0.99765258]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666366395009131, 'mcc_per_class': [0.03562965505997028, 0.045908196972884785, 0.009726075474057384], 'mcc_overall': 0.03108492236996088}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7517 -> 0.7516). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 20, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59290188]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246492101827973, 'precision_per_class': array([0.        , 0.51635282, 0.42136499]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125726032473319, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.0], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7516 -> 0.7516). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 21, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246385592220692, 'precision_per_class': array([0.        , 0.51584158, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125412541254125, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7516 -> 0.7516). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 22, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59372822]), 'f1_micro': 0.5557511737089202, 'f1_macro': 0.42492465821016756, 'precision_per_class': array([0.        , 0.51635282, 0.4222002 ]), 'precision_micro': 0.4692765113974232, 'precision_macro': 0.3128510075982821, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.03799236082530976], 'mcc_overall': 0.03108492236996088}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7516 -> 0.7513). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 23, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7513 -> 0.7513). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 24, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59290188]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246492101827973, 'precision_per_class': array([0.        , 0.51635282, 0.42136499]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125726032473319, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.0], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7513 -> 0.7513). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 25, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59372822]), 'f1_micro': 0.5557511737089202, 'f1_macro': 0.42492465821016756, 'precision_per_class': array([0.        , 0.51635282, 0.4222002 ]), 'precision_micro': 0.4692765113974232, 'precision_macro': 0.3128510075982821, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.03799236082530976], 'mcc_overall': 0.03108492236996088}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 26, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246385592220692, 'precision_per_class': array([0.        , 0.51584158, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125412541254125, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7513 -> 0.7512). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 27, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59372822]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.42477637914384186, 'precision_per_class': array([0.        , 0.51584158, 0.4222002 ]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.3126805941248238, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.03799236082530976], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7512 -> 0.7511). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 28, Train Loss: 0.748, Train metrics: {'f1_per_class': array([0.        , 0.68149117, 0.59372822]), 'f1_micro': 0.5559142941003815, 'f1_macro': 0.4250731312321065, 'precision_per_class': array([0.        , 0.51686508, 0.4222002 ]), 'precision_micro': 0.4695091720376797, 'precision_macro': 0.3130217591937116, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.056253711679722825, 0.03799236082530976], 'mcc_overall': 0.03171984509275142}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 29, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246385592220692, 'precision_per_class': array([0.        , 0.51584158, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125412541254125, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 30, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 31, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 32, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246385592220692, 'precision_per_class': array([0.        , 0.51584158, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125412541254125, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7511 -> 0.7511). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 33, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7511 -> 0.7511). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 34, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59331476]), 'f1_micro': 0.555425219941349, 'f1_macro': 0.4246385592220692, 'precision_per_class': array([0.        , 0.51584158, 0.42178218]), 'precision_micro': 0.4688118811881188, 'precision_macro': 0.3125412541254125, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.02685135334467745], 'mcc_overall': 0.02981419384773099}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "Validation loss decreased (0.7511 -> 0.7510). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 35, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 36, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 37, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59331476]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.424786838288395, 'precision_per_class': array([0.        , 0.51635282, 0.42178218]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.31271166759887087, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.02685135334467745], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 38, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.59372822]), 'f1_micro': 0.5555881490173071, 'f1_macro': 0.42477637914384186, 'precision_per_class': array([0.        , 0.51584158, 0.4222002 ]), 'precision_micro': 0.46904408122833086, 'precision_macro': 0.3126805941248238, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.03244592311606976, 0.03799236082530976], 'mcc_overall': 0.030449705704736475}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 39, Train Loss: 0.748, Train metrics: {'f1_per_class': array([0.        , 0.68149117, 0.59372822]), 'f1_micro': 0.5559142941003815, 'f1_macro': 0.4250731312321065, 'precision_per_class': array([0.        , 0.51686508, 0.4222002 ]), 'precision_micro': 0.4695091720376797, 'precision_macro': 0.3130217591937116, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.056253711679722825, 0.03799236082530976], 'mcc_overall': 0.03171984509275142}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 40, Train Loss: 0.749, Train metrics: {'f1_per_class': array([0.        , 0.68149117, 0.59331476]), 'f1_micro': 0.5557511737089202, 'f1_macro': 0.42493531131033385, 'precision_per_class': array([0.        , 0.51686508, 0.42178218]), 'precision_micro': 0.4692765113974232, 'precision_macro': 0.31288241919430043, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.056253711679722825, 0.02685135334467745], 'mcc_overall': 0.03108492236996088}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 41, Train Loss: 0.748, Train metrics: {'f1_per_class': array([0.        , 0.68104575, 0.59372822]), 'f1_micro': 0.5557511737089202, 'f1_macro': 0.42492465821016756, 'precision_per_class': array([0.        , 0.51635282, 0.4222002 ]), 'precision_micro': 0.4692765113974232, 'precision_macro': 0.3128510075982821, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.681294964028777, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.045908196972884785, 0.03799236082530976], 'mcc_overall': 0.03108492236996088}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.59217877]), 'f1_micro': 0.5552941176470588, 'f1_macro': 0.4242690144003432, 'precision_per_class': array([0.        , 0.51587302, 0.42063492]), 'precision_micro': 0.46825396825396826, 'precision_macro': 0.31216931216931215, 'recall_per_class': array([0., 1., 1.]), 'recall_micro': 0.6820809248554913, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.030038276254163516}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "[{'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 32, 'val_loss': 0.9719278365373611, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.429042904290429, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5038759689922481, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.07911877721292356, 0.0, -0.0753940029727543], 'mcc_overall': 0.06777130152946205}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 64, 'val_loss': 1.6538964211940765, 'val_metrics': {'f1_per_class': array([0.06896552, 0.67015707, 0.57303371]), 'f1_micro': 0.5480093676814989, 'f1_macro': 0.4373854310564584, 'precision_per_class': array([1.        , 0.50793651, 0.408     ]), 'precision_micro': 0.4624505928853755, 'precision_macro': 0.6386455026455026, 'recall_per_class': array([0.03571429, 0.98461538, 0.96226415]), 'recall_micro': 0.6724137931034483, 'recall_macro': 0.6608646070910222, 'mcc_per_class': [0.1424279266355945, -0.0870069397818793, -0.14946445276890902], 'mcc_overall': 0.016250545381728078}}]\n",
      "\n",
      "Training model with n_layers=1, n_layers_egnn=1, hidden_dim=128\n",
      "Epoch 1, Train Loss: 122.657, Train metrics: {'f1_per_class': array([0.44444444, 0.49689441, 0.48      ]), 'f1_micro': 0.473972602739726, 'f1_macro': 0.47377961812744424, 'precision_per_class': array([0.4148728 , 0.53932584, 0.41811847]), 'precision_micro': 0.4522875816993464, 'precision_macro': 0.4574390360100087, 'recall_per_class': array([0.4785553 , 0.46065259, 0.56338028]), 'recall_micro': 0.497841726618705, 'recall_macro': 0.5008627258671242, 'mcc_per_class': [-0.04748878662782357, 0.0425713599406885, -0.00753531654884782], 'mcc_overall': -0.012158838050043605}\n",
      "            Val Loss:   14.470, Val F1:   {'f1_per_class': array([0.28915663, 0.21686747, 0.4496124 ]), 'f1_micro': 0.3389830508474576, 'f1_macro': 0.31854549982877245, 'precision_per_class': array([0.42857143, 0.5       , 0.38157895]), 'precision_micro': 0.4098360655737705, 'precision_macro': 0.4367167919799499, 'recall_per_class': array([0.21818182, 0.13846154, 0.54716981]), 'recall_micro': 0.28901734104046245, 'recall_macro': 0.3012710559880371, 'mcc_per_class': [-0.008553728447254386, -0.012966798874660847, -0.09753948068937729], 'mcc_overall': -0.06628352982297625}\n",
      "*****\n",
      "Validation loss decreased (inf -> 14.4696). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 92.527, Train metrics: {'f1_per_class': array([0.42465753, 0.53914591, 0.52968897]), 'f1_micro': 0.5031035609278014, 'f1_macro': 0.49783080479572667, 'precision_per_class': array([0.4295612 , 0.50248756, 0.44251969]), 'precision_micro': 0.46080191502094553, 'precision_macro': 0.45818948271740406, 'recall_per_class': array([0.41986456, 0.5815739 , 0.65962441]), 'recall_micro': 0.5539568345323741, 'recall_macro': 0.55368762310604, 'mcc_per_class': [-0.015034981804434168, -0.03124324416130079, 0.055676026325241684], 'mcc_overall': 0.005579356996453436}\n",
      "            Val Loss:   10.409, Val F1:   {'f1_per_class': array([0.29545455, 0.4587156 , 0.56953642]), 'f1_micro': 0.46551724137931033, 'f1_macro': 0.44123552187529347, 'precision_per_class': array([0.39393939, 0.56818182, 0.43877551]), 'precision_micro': 0.46285714285714286, 'precision_macro': 0.46696557410843126, 'recall_per_class': array([0.23636364, 0.38461538, 0.81132075]), 'recall_micro': 0.4682080924855491, 'recall_macro': 0.47743325856533403, 'mcc_per_class': [-0.05112864732349767, 0.07667312028455092, 0.06874745097032403], 'mcc_overall': 0.009663336315263343}\n",
      "*****\n",
      "Validation loss decreased (14.4696 -> 10.4090). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 45.340, Train metrics: {'f1_per_class': array([0.5047259 , 0.58041958, 0.41500586]), 'f1_micro': 0.5080196399345336, 'f1_macro': 0.5000504466682995, 'precision_per_class': array([0.43414634, 0.5329053 , 0.41451991]), 'precision_micro': 0.46606606606606604, 'precision_macro': 0.4605238482456135, 'recall_per_class': array([0.6027088 , 0.63723608, 0.41549296]), 'recall_micro': 0.5582733812949641, 'recall_macro': 0.5518126152703974, 'mcc_per_class': [-0.010131327406282198, 0.04455860641288767, -0.011853712552018086], 'mcc_overall': 0.01721281183558769}\n",
      "            Val Loss:   9.419, Val F1:   {'f1_per_class': array([0.51851852, 0.66666667, 0.32098765]), 'f1_micro': 0.544529262086514, 'f1_macro': 0.5020576131687243, 'precision_per_class': array([0.4375    , 0.52678571, 0.46428571]), 'precision_micro': 0.4863636363636364, 'precision_macro': 0.4761904761904762, 'recall_per_class': array([0.63636364, 0.90769231, 0.24528302]), 'recall_micro': 0.6184971098265896, 'recall_macro': 0.5964463209746228, 'mcc_per_class': [0.002637943998393978, 0.06176267477170871, 0.047263872542097775], 'mcc_overall': 0.06795643275963152}\n",
      "*****\n",
      "Validation loss decreased (10.4090 -> 9.4191). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 56.591, Train metrics: {'f1_per_class': array([0.56504389, 0.66760168, 0.15849057]), 'f1_micro': 0.5434714864443752, 'f1_macro': 0.46371204790667403, 'precision_per_class': array([0.43703704, 0.52596685, 0.40384615]), 'precision_micro': 0.4793842770753161, 'precision_macro': 0.45561668057064003, 'recall_per_class': array([0.79909707, 0.91362764, 0.09859155]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6037720846379997, 'mcc_per_class': [-0.0046244413903908325, 0.0621818084593764, -0.012013971022153432], 'mcc_overall': 0.051817234548672805}\n",
      "            Val Loss:   2.423, Val F1:   {'f1_per_class': array([0.58479532, 0.67741935, 0.12698413]), 'f1_micro': 0.5571428571428572, 'f1_macro': 0.46306626782008786, 'precision_per_class': array([0.43103448, 0.52066116, 0.4       ]), 'precision_micro': 0.47368421052631576, 'precision_macro': 0.450565213261138, 'recall_per_class': array([0.90909091, 0.96923077, 0.0754717 ]), 'recall_micro': 0.6763005780346821, 'recall_macro': 0.6512644588116286, 'mcc_per_class': [-0.037588108627526214, 0.04713283651697796, -0.012272837629134187], 'mcc_overall': 0.04413235328585039}\n",
      "*****\n",
      "Validation loss decreased (9.4191 -> 2.4227). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 80.317, Train metrics: {'f1_per_class': array([0.60764873, 0.67663043, 0.14      ]), 'f1_micro': 0.5685579196217494, 'f1_macro': 0.4747597199983578, 'precision_per_class': array([0.44272446, 0.52365931, 0.47297297]), 'precision_micro': 0.48244734202607825, 'precision_macro': 0.47978557905699937, 'recall_per_class': array([0.96839729, 0.95585413, 0.08215962]), 'recall_micro': 0.6920863309352518, 'recall_macro': 0.6688036807629988, 'mcc_per_class': [0.04399391293260914, 0.06634176829789705, 0.029371857580954036], 'mcc_overall': 0.06716021304871794}\n",
      "            Val Loss:   1.881, Val F1:   {'f1_per_class': array([0.60227273, 0.67039106, 0.21538462]), 'f1_micro': 0.5714285714285714, 'f1_macro': 0.4960161347032856, 'precision_per_class': array([0.43801653, 0.52631579, 0.58333333]), 'precision_micro': 0.48582995951417, 'precision_macro': 0.5158885505775458, 'recall_per_class': array([0.96363636, 0.92307692, 0.13207547]), 'recall_micro': 0.6936416184971098, 'recall_macro': 0.6729295861371334, 'mcc_per_class': [0.014963721945621641, 0.06440604253891084, 0.10692830770760613], 'mcc_overall': 0.07760799785183992}\n",
      "*****\n",
      "Validation loss decreased (2.4227 -> 1.8814). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 18.978, Train metrics: {'f1_per_class': array([0.6081548 , 0.67337357, 0.08492569]), 'f1_micro': 0.5643883836902317, 'f1_macro': 0.4554846892813436, 'precision_per_class': array([0.43824701, 0.51752577, 0.44444444]), 'precision_micro': 0.4764735017335314, 'precision_macro': 0.4667390765308373, 'recall_per_class': array([0.99322799, 0.96353167, 0.04694836]), 'recall_micro': 0.6920863309352518, 'recall_macro': 0.6679026725479363, 'mcc_per_class': [0.0016170260088887412, 0.021357360080861146, 0.010088155000577477], 'mcc_overall': 0.05148992928938716}\n",
      "            Val Loss:   0.877, Val F1:   {'f1_per_class': array([0.60674157, 0.68062827, 0.        ]), 'f1_micro': 0.5626477541371159, 'f1_macro': 0.4291232817616723, 'precision_per_class': array([0.43902439, 0.51587302, 0.        ]), 'precision_micro': 0.476, 'precision_macro': 0.31829913537230614, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [0.032489341183349514, 0.0, -0.0762116735027003], 'mcc_overall': 0.051412996484629735}\n",
      "*****\n",
      "Validation loss decreased (1.8814 -> 0.8770). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 1.100, Train metrics: {'f1_per_class': array([0.6081548 , 0.68110236, 0.02309469]), 'f1_micro': 0.5663924794359577, 'f1_macro': 0.4374506178224024, 'precision_per_class': array([0.43824701, 0.51744766, 0.71428571]), 'precision_micro': 0.4786494538232373, 'precision_macro': 0.5566601277556064, 'recall_per_class': array([0.99322799, 0.99616123, 0.01173709]), 'recall_micro': 0.6935251798561151, 'recall_macro': 0.667042102859814, 'mcc_per_class': [0.0016170260088887412, 0.04741517479985358, 0.049533691282220495], 'mcc_overall': 0.057439383192353415}\n",
      "            Val Loss:   0.886, Val F1:   {'f1_per_class': array([0.60335196, 0.68062827, 0.        ]), 'f1_micro': 0.5613207547169812, 'f1_macro': 0.42799340918619055, 'precision_per_class': array([0.43548387, 0.51587302, 0.        ]), 'precision_micro': 0.47410358565737054, 'precision_macro': 0.3171189622802526, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.01625861784302288, 0.0, -0.0762116735027003], 'mcc_overall': 0.04636683071989005}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 0.879, Train metrics: {'f1_per_class': array([0.60953697, 0.6802097 , 0.01856148]), 'f1_micro': 0.5663924794359577, 'f1_macro': 0.4361027188416003, 'precision_per_class': array([0.43924303, 0.51641791, 0.8       ]), 'precision_micro': 0.4786494538232373, 'precision_macro': 0.5852203127787358, 'recall_per_class': array([0.99548533, 0.99616123, 0.00938967]), 'recall_micro': 0.6935251798561151, 'recall_macro': 0.6670120756940606, 'mcc_per_class': [0.025658397993984585, 0.028138018812356162, 0.054059896304868786], 'mcc_overall': 0.057439383192353415}\n",
      "            Val Loss:   0.860, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "Validation loss decreased (0.8770 -> 0.8599). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 0.840, Train metrics: {'f1_per_class': array([0.61145618, 0.67931937, 0.00465116]), 'f1_micro': 0.5653067214558263, 'f1_macro': 0.43180890373067154, 'precision_per_class': array([0.44035785, 0.51539225, 0.25      ]), 'precision_micro': 0.4774417451660882, 'precision_macro': 0.4019167023677202, 'recall_per_class': array([1.        , 0.99616123, 0.00234742]), 'recall_micro': 0.6928057553956835, 'recall_macro': 0.6661695487490951, 'mcc_per_class': [0.06226065812392125, 0.0019334371402565543, -0.021872869965615827], 'mcc_overall': 0.054151763057529556}\n",
      "            Val Loss:   0.865, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 0.791, Train metrics: {'f1_per_class': array([0.61145618, 0.67976424, 0.00928074]), 'f1_micro': 0.5658937481655415, 'f1_macro': 0.433500387582632, 'precision_per_class': array([0.44035785, 0.51590457, 0.4       ]), 'precision_micro': 0.4779375309866138, 'precision_macro': 0.45208747514910536, 'recall_per_class': array([1.        , 0.99616123, 0.00469484]), 'recall_micro': 0.6935251798561151, 'recall_macro': 0.6669520213625536, 'mcc_per_class': [0.06226065812392125, 0.016269254788033222, -0.003050401672375041], 'mcc_overall': 0.05555375793032819}\n",
      "            Val Loss:   0.844, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "Validation loss decreased (0.8599 -> 0.8438). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.851, Train metrics: {'f1_per_class': array([0.61230131, 0.68149117, 0.01856148]), 'f1_micro': 0.5682418550044027, 'f1_macro': 0.437451322893368, 'precision_per_class': array([0.44123506, 0.51686508, 0.8       ]), 'precision_micro': 0.47992067426871593, 'precision_macro': 0.5860333797086785, 'recall_per_class': array([1.        , 1.        , 0.00938967]), 'recall_micro': 0.6964028776978417, 'recall_macro': 0.6697965571205008, 'mcc_per_class': [0.07374114196417626, 0.056253711679722825, 0.054059896304868786], 'mcc_overall': 0.06116173742152273}\n",
      "            Val Loss:   0.850, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 1.110, Train metrics: {'f1_per_class': array([0.61103448, 0.68104575, 0.01395349]), 'f1_micro': 0.5671554252199413, 'f1_macro': 0.43534457425490025, 'precision_per_class': array([0.43992056, 0.51635282, 0.75      ]), 'precision_micro': 0.4787128712871287, 'precision_macro': 0.5687577935620133, 'recall_per_class': array([1.        , 1.        , 0.00704225]), 'recall_micro': 0.6956834532374101, 'recall_macro': 0.6690140845070424, 'mcc_per_class': [0.055659968426709686, 0.045908196972884785, 0.04194667270462256], 'mcc_overall': 0.057874720430768675}\n",
      "            Val Loss:   0.877, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.765, Train metrics: {'f1_per_class': array([0.61272476, 0.68062827, 0.01392111]), 'f1_micro': 0.5674008810572687, 'f1_macro': 0.4357580479644593, 'precision_per_class': array([0.44167498, 0.5163853 , 0.6       ]), 'precision_micro': 0.4794044665012407, 'precision_macro': 0.5193534259848723, 'recall_per_class': array([1.        , 0.99808061, 0.00704225]), 'recall_micro': 0.6949640287769784, 'recall_macro': 0.6683742892415272, 'mcc_per_class': [0.07887188533836266, 0.03346093631444005, 0.02550474731624687], 'mcc_overall': 0.05961374004742333}\n",
      "            Val Loss:   0.870, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 14, Train Loss: 0.826, Train metrics: {'f1_per_class': array([0.61187845, 0.67976424, 0.01392111]), 'f1_micro': 0.5666470933646506, 'f1_macro': 0.4351879367809002, 'precision_per_class': array([0.44079602, 0.51590457, 0.6       ]), 'precision_micro': 0.47867063492063494, 'precision_macro': 0.5189001974883699, 'recall_per_class': array([1.        , 0.99616123, 0.00704225]), 'recall_micro': 0.6942446043165468, 'recall_macro': 0.6677344939760123, 'mcc_per_class': [0.06823705726996447, 0.016269254788033222, 0.02550474731624687], 'mcc_overall': 0.057584227796180276}\n",
      "            Val Loss:   0.871, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 15, Train Loss: 1.341, Train metrics: {'f1_per_class': array([0.61145618, 0.68062827, 0.01856148]), 'f1_micro': 0.5674882629107981, 'f1_macro': 0.43688197794789013, 'precision_per_class': array([0.44035785, 0.5163853 , 0.8       ]), 'precision_micro': 0.479187314172448, 'precision_macro': 0.5855810519208483, 'recall_per_class': array([1.        , 0.99808061, 0.00938967]), 'recall_micro': 0.6956834532374101, 'recall_macro': 0.6691567618549857, 'mcc_per_class': [0.06226065812392125, 0.03346093631444005, 0.054059896304868786], 'mcc_overall': 0.05913167745261363}\n",
      "            Val Loss:   0.872, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 16, Train Loss: 2.590, Train metrics: {'f1_per_class': array([0.61103448, 0.68018313, 0.01856148]), 'f1_micro': 0.5671554252199413, 'f1_macro': 0.43659303130123533, 'precision_per_class': array([0.43992056, 0.51587302, 0.8       ]), 'precision_micro': 0.4787128712871287, 'precision_macro': 0.5852645239934217, 'recall_per_class': array([1.        , 0.99808061, 0.00938967]), 'recall_micro': 0.6956834532374101, 'recall_macro': 0.6691567618549857, 'mcc_per_class': [0.055659968426709686, 0.019866953836984644, 0.054059896304868786], 'mcc_overall': 0.057874720430768675}\n",
      "            Val Loss:   0.873, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 17, Train Loss: 1.502, Train metrics: {'f1_per_class': array([0.61145618, 0.67976424, 0.01395349]), 'f1_micro': 0.5666470933646506, 'f1_macro': 0.43505796955353077, 'precision_per_class': array([0.44035785, 0.51590457, 0.75      ]), 'precision_micro': 0.47867063492063494, 'precision_macro': 0.5687541418157721, 'recall_per_class': array([1.        , 0.99616123, 0.00704225]), 'recall_micro': 0.6942446043165468, 'recall_macro': 0.6677344939760123, 'mcc_per_class': [0.06226065812392125, 0.016269254788033222, 0.04194667270462256], 'mcc_overall': 0.057584227796180276}\n",
      "            Val Loss:   0.879, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.        ]), 'f1_micro': 0.56, 'f1_macro': 0.4268760907504363, 'precision_per_class': array([0.432     , 0.51587302, 0.        ]), 'precision_micro': 0.4722222222222222, 'precision_macro': 0.315957671957672, 'recall_per_class': array([0.98181818, 1.        , 0.        ]), 'recall_micro': 0.6878612716763006, 'recall_macro': 0.6606060606060606, 'mcc_per_class': [-0.10162318990896088, 0.0, -0.0762116735027003], 'mcc_overall': 0.04130262984947483}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "[{'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 32, 'val_loss': 0.9719278365373611, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.429042904290429, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5038759689922481, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.07911877721292356, 0.0, -0.0753940029727543], 'mcc_overall': 0.06777130152946205}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 64, 'val_loss': 1.6538964211940765, 'val_metrics': {'f1_per_class': array([0.06896552, 0.67015707, 0.57303371]), 'f1_micro': 0.5480093676814989, 'f1_macro': 0.4373854310564584, 'precision_per_class': array([1.        , 0.50793651, 0.408     ]), 'precision_micro': 0.4624505928853755, 'precision_macro': 0.6386455026455026, 'recall_per_class': array([0.03571429, 0.98461538, 0.96226415]), 'recall_micro': 0.6724137931034483, 'recall_macro': 0.6608646070910222, 'mcc_per_class': [0.1424279266355945, -0.0870069397818793, -0.14946445276890902], 'mcc_overall': 0.016250545381728078}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 128, 'val_loss': 1.0704892873764038, 'val_metrics': {'f1_per_class': array([0.61111111, 0.67708333, 0.07142857]), 'f1_micro': 0.5700934579439252, 'f1_macro': 0.45320767195767203, 'precision_per_class': array([0.44354839, 0.51181102, 0.66666667]), 'precision_micro': 0.48031496062992124, 'precision_macro': 0.5406753591284961, 'recall_per_class': array([0.98214286, 1.        , 0.03773585]), 'recall_micro': 0.7011494252873564, 'recall_macro': 0.673292902066487, 'mcc_per_class': [0.0337123633364026, 0.0, 0.0786499300458639], 'mcc_overall': 0.06706527510148438}}]\n",
      "\n",
      "Training model with n_layers=1, n_layers_egnn=1, hidden_dim=256\n",
      "Epoch 1, Train Loss: 185.593, Train metrics: {'f1_per_class': array([0.43046358, 0.49651047, 0.50493097]), 'f1_micro': 0.4789599726308587, 'f1_macro': 0.4773016704075286, 'precision_per_class': array([0.42116631, 0.51659751, 0.43537415]), 'precision_micro': 0.45662100456621, 'precision_macro': 0.45771265557625745, 'recall_per_class': array([0.44018059, 0.47792706, 0.60093897]), 'recall_micro': 0.5035971223021583, 'recall_macro': 0.5063488724611103, 'mcc_per_class': [-0.03151914626955026, 0.00241833748280032, 0.033450232226750025], 'mcc_overall': -0.0033906122753506445}\n",
      "            Val Loss:   67.702, Val F1:   {'f1_per_class': array([0.53211009, 0.56164384, 0.5034965 ]), 'f1_micro': 0.5326633165829145, 'f1_macro': 0.5324168102853537, 'precision_per_class': array([0.53703704, 0.50617284, 0.4       ]), 'precision_micro': 0.4711111111111111, 'precision_macro': 0.4810699588477367, 'recall_per_class': array([0.52727273, 0.63076923, 0.67924528]), 'recall_micro': 0.6127167630057804, 'recall_macro': 0.6124290803536087, 'mcc_per_class': [0.17554257089229347, -0.026041430221709284, -0.06609125328408949], 'mcc_overall': 0.032712163923451276}\n",
      "*****\n",
      "Validation loss decreased (inf -> 67.7019). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 219.361, Train metrics: {'f1_per_class': array([0.5122449 , 0.50348259, 0.48447205]), 'f1_micro': 0.5001694340901389, 'f1_macro': 0.5000665115711004, 'precision_per_class': array([0.46741155, 0.52272727, 0.43333333]), 'precision_micro': 0.4727738629083921, 'precision_macro': 0.4744907172281474, 'recall_per_class': array([0.56659142, 0.48560461, 0.54929577]), 'recall_micro': 0.5309352517985612, 'recall_macro': 0.5338306010985651, 'mcc_per_class': [0.06270823570797529, 0.01418220285519189, 0.02595308947931993], 'mcc_overall': 0.02993054382589771}\n",
      "            Val Loss:   38.320, Val F1:   {'f1_per_class': array([0.50406504, 0.39215686, 0.55172414]), 'f1_micro': 0.4918918918918919, 'f1_macro': 0.4826486804421797, 'precision_per_class': array([0.45588235, 0.54054054, 0.43478261]), 'precision_micro': 0.4619289340101523, 'precision_macro': 0.4770685007257897, 'recall_per_class': array([0.56363636, 0.30769231, 0.75471698]), 'recall_micro': 0.5260115606936416, 'recall_macro': 0.5420152174869156, 'mcc_per_class': [0.042298904117990706, 0.03182587693974931, 0.04714233731172713], 'mcc_overall': 0.00891429248664362}\n",
      "*****\n",
      "Validation loss decreased (67.7019 -> 38.3202). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 139.452, Train metrics: {'f1_per_class': array([0.507431  , 0.39012926, 0.47927199]), 'f1_micro': 0.46153846153846156, 'f1_macro': 0.45894408316078533, 'precision_per_class': array([0.47895792, 0.5030303 , 0.42095915]), 'precision_micro': 0.46120689655172414, 'precision_macro': 0.46764912209549264, 'recall_per_class': array([0.53950339, 0.31861804, 0.55633803]), 'recall_micro': 0.4618705035971223, 'recall_macro': 0.4714864854666721, 'mcc_per_class': [0.08113630604927095, -0.017134045843052424, -0.0009213741909858051], 'mcc_overall': 0.005387866431310291}\n",
      "            Val Loss:   97.309, Val F1:   {'f1_per_class': array([0.50793651, 0.44247788, 0.42622951]), 'f1_micro': 0.4598337950138504, 'f1_macro': 0.4588812974131413, 'precision_per_class': array([0.45070423, 0.52083333, 0.37681159]), 'precision_micro': 0.44148936170212766, 'precision_macro': 0.44944971762944813, 'recall_per_class': array([0.58181818, 0.38461538, 0.49056604]), 'recall_micro': 0.4797687861271676, 'recall_macro': 0.48566653472313853, 'mcc_per_class': [0.0325224071702945, 0.007786310867104961, -0.09767055096615906], 'mcc_overall': -0.03231038501438363}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 294.117, Train metrics: {'f1_per_class': array([0.49190283, 0.43155452, 0.44847112]), 'f1_micro': 0.4588364434687157, 'f1_macro': 0.4573094931826163, 'precision_per_class': array([0.44587156, 0.54545455, 0.43326039]), 'precision_micro': 0.46686522710349965, 'precision_macro': 0.47486216632021944, 'recall_per_class': array([0.54853273, 0.35700576, 0.46478873]), 'recall_micro': 0.4510791366906475, 'recall_macro': 0.4567757406429103, 'mcc_per_class': [0.016764619926914602, 0.043000648803321315, 0.021880185975521323], 'mcc_overall': 0.015338367223204507}\n",
      "            Val Loss:   64.600, Val F1:   {'f1_per_class': array([0.3655914 , 0.53913043, 0.31914894]), 'f1_micro': 0.41721854304635764, 'f1_macro': 0.40795692293409463, 'precision_per_class': array([0.44736842, 0.62      , 0.36585366]), 'precision_micro': 0.4883720930232558, 'precision_macro': 0.4777406931964056, 'recall_per_class': array([0.30909091, 0.47692308, 0.28301887]), 'recall_micro': 0.36416184971098264, 'recall_macro': 0.35634428464617146, 'mcc_per_class': [0.014389963426783647, 0.16900145562210203, -0.07707005686904175], 'mcc_overall': 0.04435343803060243}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 84.521, Train metrics: {'f1_per_class': array([0.53076923, 0.36456996, 0.51943128]), 'f1_micro': 0.4815588030619346, 'f1_macro': 0.4715901572930575, 'precision_per_class': array([0.46231156, 0.5503876 , 0.43561208]), 'precision_micro': 0.46630727762803237, 'precision_macro': 0.4827704124530252, 'recall_per_class': array([0.62302483, 0.27255278, 0.64319249]), 'recall_micro': 0.497841726618705, 'recall_macro': 0.5129233673573633, 'mcc_per_class': [0.058404609141959926, 0.04105934099541814, 0.03702443679705873], 'mcc_overall': 0.015745249143949314}\n",
      "            Val Loss:   13.164, Val F1:   {'f1_per_class': array([0.53061224, 0.24096386, 0.56716418]), 'f1_micro': 0.47802197802197804, 'f1_macro': 0.4462467598080411, 'precision_per_class': array([0.42391304, 0.55555556, 0.4691358 ]), 'precision_micro': 0.45549738219895286, 'precision_macro': 0.4828681338343174, 'recall_per_class': array([0.70909091, 0.15384615, 0.71698113]), 'recall_micro': 0.5028901734104047, 'recall_macro': 0.5266393983375116, 'mcc_per_class': [-0.04177427914682549, 0.03241699718665212, 0.13181264285058228], 'mcc_overall': -0.00441125520110679}\n",
      "*****\n",
      "Validation loss decreased (38.3202 -> 13.1642). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 57.411, Train metrics: {'f1_per_class': array([0.53900709, 0.16199377, 0.55399835]), 'f1_micro': 0.4639624539054643, 'f1_macro': 0.41833307095478994, 'precision_per_class': array([0.44379562, 0.42975207, 0.42693774]), 'precision_micro': 0.43440050219711235, 'precision_macro': 0.4334951416000548, 'recall_per_class': array([0.68623025, 0.09980806, 0.78873239]), 'recall_micro': 0.497841726618705, 'recall_macro': 0.5249235680311801, 'mcc_per_class': [0.01640619338456325, -0.06313942312914994, 0.021154442321346282], 'mcc_overall': -0.05043342111665672}\n",
      "            Val Loss:   7.941, Val F1:   {'f1_per_class': array([0.24390244, 0.18421053, 0.48979592]), 'f1_micro': 0.3203125, 'f1_macro': 0.30596962790250887, 'precision_per_class': array([0.37037037, 0.63636364, 0.53333333]), 'precision_micro': 0.4939759036144578, 'precision_macro': 0.5133557800224468, 'recall_per_class': array([0.18181818, 0.10769231, 0.45283019]), 'recall_micro': 0.23699421965317918, 'recall_macro': 0.2474468927299116, 'mcc_per_class': [-0.06964220602680632, 0.07456750509567203, 0.17015813895256987], 'mcc_overall': 0.038652152807349016}\n",
      "*****\n",
      "Validation loss decreased (13.1642 -> 7.9408). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 27.008, Train metrics: {'f1_per_class': array([0.54678899, 0.10122164, 0.54099746]), 'f1_micro': 0.4546732255797611, 'f1_macro': 0.3963360317962438, 'precision_per_class': array([0.46058733, 0.55769231, 0.42272127]), 'precision_micro': 0.44436813186813184, 'precision_macro': 0.4803336339922229, 'recall_per_class': array([0.67268623, 0.05566219, 0.75117371]), 'recall_micro': 0.4654676258992806, 'recall_macro': 0.4931740424227676, 'mcc_per_class': [0.06020966692408534, 0.01973750895088509, 0.0047418719586532175], 'mcc_overall': -0.02685188101832951}\n",
      "            Val Loss:   2.151, Val F1:   {'f1_per_class': array([0.58959538, 0.28915663, 0.54658385]), 'f1_micro': 0.513189448441247, 'f1_macro': 0.4751119510534149, 'precision_per_class': array([0.43220339, 0.66666667, 0.40740741]), 'precision_micro': 0.4385245901639344, 'precision_macro': 0.5020924879681942, 'recall_per_class': array([0.92727273, 0.18461538, 0.83018868]), 'recall_micro': 0.6184971098265896, 'recall_macro': 0.6473589303777983, 'mcc_per_class': [-0.03333369502903513, 0.12318458930927804, -0.06563341623057815], 'mcc_overall': -0.051861300695987135}\n",
      "*****\n",
      "Validation loss decreased (7.9408 -> 2.1512). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 2.723, Train metrics: {'f1_per_class': array([0.60056259, 0.05494505, 0.58773181]), 'f1_micro': 0.5068249258160238, 'f1_macro': 0.41441315151566327, 'precision_per_class': array([0.43615935, 0.6       , 0.42213115]), 'precision_micro': 0.4313131313131313, 'precision_macro': 0.4860968312708965, 'recall_per_class': array([0.96388262, 0.02879079, 0.96713615]), 'recall_micro': 0.6143884892086331, 'recall_macro': 0.6532698518976922, 'mcc_per_class': [-0.0225261841249078, 0.026976674381835156, 0.008193701377857633], 'mcc_overall': -0.07424894791913082}\n",
      "            Val Loss:   0.747, Val F1:   {'f1_per_class': array([0.61452514, 0.03030303, 0.59550562]), 'f1_micro': 0.5153664302600472, 'f1_macro': 0.4134445959817876, 'precision_per_class': array([0.44354839, 1.        , 0.424     ]), 'precision_micro': 0.436, 'precision_macro': 0.6225161290322581, 'recall_per_class': array([1.        , 0.01538462, 1.        ]), 'recall_micro': 0.630057803468208, 'recall_macro': 0.6717948717948717, 'mcc_per_class': [0.11177799767078231, 0.08664694055586906, 0.0762116735027003], 'mcc_overall': -0.060793196767044866}\n",
      "*****\n",
      "Validation loss decreased (2.1512 -> 0.7466). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 0.848, Train metrics: {'f1_per_class': array([0.60833333, 0.02255639, 0.59368421]), 'f1_micro': 0.510450397409479, 'f1_macro': 0.40819131161236427, 'precision_per_class': array([0.43931795, 0.54545455, 0.42342342]), 'precision_micro': 0.4319880418535127, 'precision_macro': 0.46939864091318456, 'recall_per_class': array([0.98871332, 0.01151631, 0.99295775]), 'recall_micro': 0.6237410071942446, 'recall_macro': 0.6643957931808561, 'mcc_per_class': [0.01935422519441065, 0.006321666236889295, 0.03803631099098869], 'mcc_overall': -0.07383619713221219}\n",
      "            Val Loss:   0.765, Val F1:   {'f1_per_class': array([0.61016949, 0.05882353, 0.57627119]), 'f1_micro': 0.5071090047393365, 'f1_macro': 0.4150880691259555, 'precision_per_class': array([0.44262295, 0.66666667, 0.41129032]), 'precision_micro': 0.42971887550200805, 'precision_macro': 0.506859980022328, 'recall_per_class': array([0.98181818, 0.03076923, 0.96226415]), 'recall_micro': 0.6184971098265896, 'recall_macro': 0.6582838545102696, 'mcc_per_class': [0.06809377690001776, 0.04712377533112005, -0.1490485139988275], 'mcc_overall': -0.07795183397362392}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 0.829, Train metrics: {'f1_per_class': array([0.60851361, 0.04074074, 0.59353024]), 'f1_micro': 0.5119293078055964, 'f1_macro': 0.4142615292187904, 'precision_per_class': array([0.44040404, 0.57894737, 0.42369478]), 'precision_micro': 0.43341645885286784, 'precision_macro': 0.4810153959805196, 'recall_per_class': array([0.98419865, 0.02111324, 0.99061033]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6653074059995626, 'mcc_per_class': [0.03077668817624115, 0.017616582730688023, 0.03844766562332838], 'mcc_overall': -0.06972387122815571}\n",
      "            Val Loss:   0.782, Val F1:   {'f1_per_class': array([0.60227273, 0.08571429, 0.58285714]), 'f1_micro': 0.5083135391923991, 'f1_macro': 0.4236147186147186, 'precision_per_class': array([0.43801653, 0.6       , 0.41803279]), 'precision_micro': 0.4314516129032258, 'precision_macro': 0.4853497719369552, 'recall_per_class': array([0.96363636, 0.04615385, 0.96226415]), 'recall_micro': 0.6184971098265896, 'recall_macro': 0.6573514535778687, 'mcc_per_class': [0.014963721945621641, 0.0342197306219155, -0.029110570417835088], 'mcc_overall': -0.07269162540169592}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.791, Train metrics: {'f1_per_class': array([0.6103352 , 0.05185185, 0.59337562]), 'f1_micro': 0.5143025656148629, 'f1_macro': 0.4185208880046695, 'precision_per_class': array([0.44186047, 0.73684211, 0.42396777]), 'precision_micro': 0.4357821089455272, 'precision_macro': 0.5342234482667946, 'recall_per_class': array([0.98645598, 0.0268714 , 0.98826291]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6671967646303543, 'mcc_per_class': [0.0497350205240014, 0.06134088340350044, 0.03915126512703402], 'mcc_overall': -0.06290796444731821}\n",
      "            Val Loss:   0.775, Val F1:   {'f1_per_class': array([0.61016949, 0.08695652, 0.57954545]), 'f1_micro': 0.5118483412322274, 'f1_macro': 0.4255571559366696, 'precision_per_class': array([0.44262295, 0.75      , 0.41463415]), 'precision_micro': 0.43373493975903615, 'precision_macro': 0.5357523657203785, 'recall_per_class': array([0.98181818, 0.04615385, 0.96226415]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6634120596384747, 'mcc_per_class': [0.06809377690001776, 0.08483020186740119, -0.07783418828458256], 'mcc_overall': -0.06675236865928341}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 0.759, Train metrics: {'f1_per_class': array([0.6092991 , 0.03377111, 0.59425368]), 'f1_micro': 0.5127903557777125, 'f1_macro': 0.4124412946125022, 'precision_per_class': array([0.43987976, 0.75      , 0.42357642]), 'precision_micro': 0.43361511685728493, 'precision_macro': 0.5378187276984873, 'recall_per_class': array([0.99097065, 0.01727447, 0.99530516]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6678500970385648, 'mcc_per_class': [0.03001588457335534, 0.05146324870849494, 0.04480849020404885], 'mcc_overall': -0.06947350860347098}\n",
      "            Val Loss:   0.762, Val F1:   {'f1_per_class': array([0.61452514, 0.03030303, 0.59550562]), 'f1_micro': 0.5153664302600472, 'f1_macro': 0.4134445959817876, 'precision_per_class': array([0.44354839, 1.        , 0.424     ]), 'precision_micro': 0.436, 'precision_macro': 0.6225161290322581, 'recall_per_class': array([1.        , 0.01538462, 1.        ]), 'recall_micro': 0.630057803468208, 'recall_macro': 0.6717948717948717, 'mcc_per_class': [0.11177799767078231, 0.08664694055586906, 0.0762116735027003], 'mcc_overall': -0.060793196767044866}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 13, Train Loss: 0.792, Train metrics: {'f1_per_class': array([0.61122661, 0.01886792, 0.5931612 ]), 'f1_micro': 0.5114503816793893, 'f1_macro': 0.4077519120113493, 'precision_per_class': array([0.441     , 0.55555556, 0.42204568]), 'precision_micro': 0.4320436507936508, 'precision_macro': 0.47286707859796245, 'recall_per_class': array([0.99548533, 0.00959693, 0.99765258]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6675782794853732, 'mcc_per_class': [0.054190710329526756, 0.0076279716141731905, 0.021872869965615827], 'mcc_overall': -0.07417114729427829}\n",
      "            Val Loss:   0.762, Val F1:   {'f1_per_class': array([0.61111111, 0.03030303, 0.59550562]), 'f1_micro': 0.5141509433962265, 'f1_macro': 0.4123065864638898, 'precision_per_class': array([0.44 , 1.   , 0.424]), 'precision_micro': 0.4342629482071713, 'precision_macro': 0.6213333333333333, 'recall_per_class': array([1.        , 0.01538462, 1.        ]), 'recall_micro': 0.630057803468208, 'recall_macro': 0.6717948717948717, 'mcc_per_class': [0.07872218936609646, 0.08664694055586906, 0.0762116735027003], 'mcc_overall': -0.06605563247522502}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 14, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.61314879, 0.01893939, 0.59357542]), 'f1_micro': 0.5127753303964758, 'f1_macro': 0.4085545339537144, 'precision_per_class': array([0.44211577, 0.71428571, 0.42246521]), 'precision_micro': 0.43325062034739453, 'precision_macro': 0.5262888971654344, 'recall_per_class': array([1.        , 0.00959693, 0.99765258]), 'recall_micro': 0.6280575539568345, 'recall_macro': 0.6690831703807834, 'mcc_per_class': [0.0836980016858866, 0.033240678256833064, 0.031605550660996955], 'mcc_overall': -0.07070826789997298}\n",
      "            Val Loss:   0.761, Val F1:   {'f1_per_class': array([0.61452514, 0.03030303, 0.59550562]), 'f1_micro': 0.5153664302600472, 'f1_macro': 0.4134445959817876, 'precision_per_class': array([0.44354839, 1.        , 0.424     ]), 'precision_micro': 0.436, 'precision_macro': 0.6225161290322581, 'recall_per_class': array([1.        , 0.01538462, 1.        ]), 'recall_micro': 0.630057803468208, 'recall_macro': 0.6717948717948717, 'mcc_per_class': [0.11177799767078231, 0.08664694055586906, 0.0762116735027003], 'mcc_overall': -0.060793196767044866}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 15, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61080332, 0.01886792, 0.59300699]), 'f1_micro': 0.5111633372502937, 'f1_macro': 0.4075594138783393, 'precision_per_class': array([0.44055944, 0.55555556, 0.42231076]), 'precision_micro': 0.43197616683217477, 'precision_macro': 0.4728085843623693, 'recall_per_class': array([0.99548533, 0.00959693, 0.99530516]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6667958068719146, 'mcc_per_class': [0.04798035013348144, 0.0076279716141731905, 0.02293890189625262], 'mcc_overall': -0.07425196899262426}\n",
      "            Val Loss:   0.761, Val F1:   {'f1_per_class': array([0.61111111, 0.03030303, 0.59550562]), 'f1_micro': 0.5141509433962265, 'f1_macro': 0.4123065864638898, 'precision_per_class': array([0.44 , 1.   , 0.424]), 'precision_micro': 0.4342629482071713, 'precision_macro': 0.6213333333333333, 'recall_per_class': array([1.        , 0.01538462, 1.        ]), 'recall_micro': 0.630057803468208, 'recall_macro': 0.6717948717948717, 'mcc_per_class': [0.07872218936609646, 0.08664694055586906, 0.0762116735027003], 'mcc_overall': -0.06605563247522502}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "[{'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 32, 'val_loss': 0.9719278365373611, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.429042904290429, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5038759689922481, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.07911877721292356, 0.0, -0.0753940029727543], 'mcc_overall': 0.06777130152946205}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 64, 'val_loss': 1.6538964211940765, 'val_metrics': {'f1_per_class': array([0.06896552, 0.67015707, 0.57303371]), 'f1_micro': 0.5480093676814989, 'f1_macro': 0.4373854310564584, 'precision_per_class': array([1.        , 0.50793651, 0.408     ]), 'precision_micro': 0.4624505928853755, 'precision_macro': 0.6386455026455026, 'recall_per_class': array([0.03571429, 0.98461538, 0.96226415]), 'recall_micro': 0.6724137931034483, 'recall_macro': 0.6608646070910222, 'mcc_per_class': [0.1424279266355945, -0.0870069397818793, -0.14946445276890902], 'mcc_overall': 0.016250545381728078}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 128, 'val_loss': 1.0704892873764038, 'val_metrics': {'f1_per_class': array([0.61111111, 0.67708333, 0.07142857]), 'f1_micro': 0.5700934579439252, 'f1_macro': 0.45320767195767203, 'precision_per_class': array([0.44354839, 0.51181102, 0.66666667]), 'precision_micro': 0.48031496062992124, 'precision_macro': 0.5406753591284961, 'recall_per_class': array([0.98214286, 1.        , 0.03773585]), 'recall_micro': 0.7011494252873564, 'recall_macro': 0.673292902066487, 'mcc_per_class': [0.0337123633364026, 0.0, 0.0786499300458639], 'mcc_overall': 0.06706527510148438}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 256, 'val_loss': 1.4745376259088516, 'val_metrics': {'f1_per_class': array([0.6043956 , 0.        , 0.58100559]), 'f1_micro': 0.5023474178403756, 'f1_macro': 0.395133730329261, 'precision_per_class': array([0.43650794, 0.        , 0.41269841]), 'precision_micro': 0.4246031746031746, 'precision_macro': 0.2830687830687831, 'recall_per_class': array([0.98214286, 0.        , 0.98113208]), 'recall_micro': 0.6149425287356322, 'recall_macro': 0.6544249775381851, 'mcc_per_class': [-0.10031130682352807, 0.0, -0.10526709849026072], 'mcc_overall': -0.09004026736191141}}]\n",
      "\n",
      "Training model with n_layers=1, n_layers_egnn=2, hidden_dim=32\n",
      "Epoch 1, Train Loss: 70.553, Train metrics: {'f1_per_class': array([0.30357143, 0.67248322, 0.29180328]), 'f1_micro': 0.49927849927849927, 'f1_macro': 0.42261930957882105, 'precision_per_class': array([0.44541485, 0.51702786, 0.48369565]), 'precision_micro': 0.5007235890014472, 'precision_macro': 0.4820461210375249, 'recall_per_class': array([0.23024831, 0.96161228, 0.20892019]), 'recall_micro': 0.497841726618705, 'recall_macro': 0.4669269262867559, 'mcc_per_class': [0.00789073871176953, 0.016305254582390304, 0.059542402488493205], 'mcc_overall': 0.07791387078467454}\n",
      "            Val Loss:   6.668, Val F1:   {'f1_per_class': array([0.2962963 , 0.67368421, 0.10169492]), 'f1_micro': 0.47878787878787876, 'f1_macro': 0.3572251406922831, 'precision_per_class': array([0.46153846, 0.512     , 0.5       ]), 'precision_micro': 0.5031847133757962, 'precision_macro': 0.49117948717948723, 'recall_per_class': array([0.21818182, 0.98461538, 0.05660377]), 'recall_micro': 0.45664739884393063, 'recall_macro': 0.41980032546070284, 'mcc_per_class': [0.025734554788350895, -0.08664694055586906, 0.035948902595613355], 'mcc_overall': 0.07699785717140263}\n",
      "*****\n",
      "Validation loss decreased (inf -> 6.6684). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 28.424, Train metrics: {'f1_per_class': array([0.16129032, 0.67986799, 0.13438735]), 'f1_micro': 0.4606436603334626, 'f1_macro': 0.3251818870526604, 'precision_per_class': array([0.39130435, 0.51810865, 0.425     ]), 'precision_micro': 0.49957947855340623, 'precision_macro': 0.44480433324585195, 'recall_per_class': array([0.10158014, 0.98848369, 0.07981221]), 'recall_micro': 0.4273381294964029, 'recall_macro': 0.38995867574455995, 'mcc_per_class': [-0.03384679463222156, 0.04249377186050788, 0.00215796682480047], 'mcc_overall': 0.06653862102616068}\n",
      "            Val Loss:   2.802, Val F1:   {'f1_per_class': array([0.21621622, 0.67368421, 0.12698413]), 'f1_micro': 0.4648318042813456, 'f1_macro': 0.3389615179088863, 'precision_per_class': array([0.42105263, 0.512     , 0.4       ]), 'precision_micro': 0.4935064935064935, 'precision_macro': 0.44435087719298244, 'recall_per_class': array([0.14545455, 0.98461538, 0.0754717 ]), 'recall_micro': 0.4393063583815029, 'recall_macro': 0.40184720939437923, 'mcc_per_class': [-0.013131747264432156, -0.08664694055586906, -0.012272837629134187], 'mcc_overall': 0.059638945019114514}\n",
      "*****\n",
      "Validation loss decreased (6.6684 -> 2.8022). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 15.957, Train metrics: {'f1_per_class': array([0.16574586, 0.67544437, 0.13414634]), 'f1_micro': 0.46280344557556774, 'f1_macro': 0.32511218970463723, 'precision_per_class': array([0.45      , 0.51402806, 0.5       ]), 'precision_micro': 0.5077319587628866, 'precision_macro': 0.4880093520374082, 'recall_per_class': array([0.10158014, 0.98464491, 0.07746479]), 'recall_micro': 0.4251798561151079, 'recall_macro': 0.38789661260007136, 'mcc_per_class': [0.007892828854892293, -0.02284924762902618, 0.04208627218542377], 'mcc_overall': 0.07830601325101538}\n",
      "            Val Loss:   6.479, Val F1:   {'f1_per_class': array([0.18666667, 0.67368421, 0.125     ]), 'f1_micro': 0.45592705167173253, 'f1_macro': 0.3284502923976608, 'precision_per_class': array([0.35      , 0.512     , 0.36363636]), 'precision_micro': 0.4807692307692308, 'precision_macro': 0.40854545454545454, 'recall_per_class': array([0.12727273, 0.98461538, 0.0754717 ]), 'recall_micro': 0.43352601156069365, 'recall_macro': 0.39578660333377313, 'mcc_per_class': [-0.07576664277069084, -0.08664694055586906, -0.03570939264633302], 'mcc_overall': 0.03886318044666048}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 31.430, Train metrics: {'f1_per_class': array([0.104     , 0.68062827, 0.10084034]), 'f1_micro': 0.45527156549520764, 'f1_macro': 0.29515620279525423, 'precision_per_class': array([0.45614035, 0.5163853 , 0.48      ]), 'precision_micro': 0.5116696588868941, 'precision_macro': 0.4841752179190113, 'recall_per_class': array([0.05869074, 0.99808061, 0.05633803]), 'recall_micro': 0.41007194244604317, 'recall_macro': 0.371036462431154, 'mcc_per_class': [0.008848154627911402, 0.03346093631444005, 0.02708624302250848], 'mcc_overall': 0.08162249801895383}\n",
      "            Val Loss:   1.548, Val F1:   {'f1_per_class': array([0.06451613, 0.68062827, 0.10344828]), 'f1_micro': 0.45016077170418006, 'f1_macro': 0.282864225715212, 'precision_per_class': array([0.28571429, 0.51587302, 0.6       ]), 'precision_micro': 0.5072463768115942, 'precision_macro': 0.46719576719576716, 'recall_per_class': array([0.03636364, 1.        , 0.05660377]), 'recall_micro': 0.4046242774566474, 'recall_macro': 0.36432246998284734, 'mcc_per_class': [-0.07374262977586608, 0.0, 0.0738586544237147], 'mcc_overall': 0.07545419227180836}\n",
      "*****\n",
      "Validation loss decreased (2.8022 -> 1.5478). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 22.194, Train metrics: {'f1_per_class': array([0.01310044, 0.67498359, 0.0794702 ]), 'f1_micro': 0.4396055875102712, 'f1_macro': 0.2558514067954221, 'precision_per_class': array([0.2       , 0.51297405, 0.66666667]), 'precision_micro': 0.5124521072796935, 'precision_macro': 0.4598802395209581, 'recall_per_class': array([0.00677201, 0.9865643 , 0.04225352]), 'recall_micro': 0.38489208633093525, 'recall_macro': 0.3451966098600967, 'mcc_per_class': [-0.05891100231851683, -0.04976938856460541, 0.082291178339924], 'mcc_overall': 0.07875128333552109}\n",
      "            Val Loss:   1.178, Val F1:   {'f1_per_class': array([0.06557377, 0.68062827, 0.10344828]), 'f1_micro': 0.45161290322580644, 'f1_macro': 0.2832167728683937, 'precision_per_class': array([0.33333333, 0.51587302, 0.6       ]), 'precision_micro': 0.5109489051094891, 'precision_macro': 0.48306878306878315, 'recall_per_class': array([0.03636364, 1.        , 0.05660377]), 'recall_micro': 0.4046242774566474, 'recall_macro': 0.36432246998284734, 'mcc_per_class': [-0.04651765735269336, 0.0, 0.0738586544237147], 'mcc_overall': 0.08062744794164553}\n",
      "*****\n",
      "Validation loss decreased (1.5478 -> 1.1778). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 8.368, Train metrics: {'f1_per_class': array([0.01327434, 0.68152031, 0.05393258]), 'f1_micro': 0.4416013206768469, 'f1_macro': 0.24957574503356209, 'precision_per_class': array([0.33333333, 0.51741294, 0.63157895]), 'precision_micro': 0.5179090029041626, 'precision_macro': 0.49410840534171246, 'recall_per_class': array([0.00677201, 0.99808061, 0.02816901]), 'recall_micro': 0.38489208633093525, 'recall_macro': 0.3443405457724357, 'mcc_per_class': [-0.020027061351476253, 0.05390571538780189, 0.05891840012478763], 'mcc_overall': 0.08599053605595983}\n",
      "            Val Loss:   1.155, Val F1:   {'f1_per_class': array([0.03508772, 0.68062827, 0.03571429]), 'f1_micro': 0.4407894736842105, 'f1_macro': 0.25047675908794675, 'precision_per_class': array([0.5       , 0.51587302, 0.33333333]), 'precision_micro': 0.5114503816793893, 'precision_macro': 0.4497354497354497, 'recall_per_class': array([0.01818182, 1.        , 0.01886792]), 'recall_micro': 0.3872832369942196, 'recall_macro': 0.34568324757004, 'mcc_per_class': [0.01625861784302288, 0.0, -0.02761858293969059], 'mcc_overall': 0.07861167678944454}\n",
      "*****\n",
      "Validation loss decreased (1.1778 -> 1.1548). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 16.102, Train metrics: {'f1_per_class': array([0.01333333, 0.67929458, 0.04977376]), 'f1_micro': 0.4407758976475444, 'f1_macro': 0.24746722256538986, 'precision_per_class': array([0.42857143, 0.51485149, 0.6875    ]), 'precision_micro': 0.5169409486931268, 'precision_macro': 0.5436409712399811, 'recall_per_class': array([0.00677201, 0.99808061, 0.0258216 ]), 'recall_micro': 0.3841726618705036, 'recall_macro': 0.3435580731589772, 'mcc_per_class': [-0.0016170260088887412, -0.030515359552541615, 0.06834691107522531], 'mcc_overall': 0.0845942285587961}\n",
      "            Val Loss:   0.939, Val F1:   {'f1_per_class': array([0.07017544, 0.68062827, 0.        ]), 'f1_micro': 0.44370860927152317, 'f1_macro': 0.2502679036159334, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5193798449612403, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.03636364, 1.        , 0.        ]), 'recall_micro': 0.3872832369942196, 'recall_macro': 0.34545454545454546, 'mcc_per_class': [0.14429523335682806, 0.0, -0.0762116735027003], 'mcc_overall': 0.0891512992879644}\n",
      "*****\n",
      "Validation loss decreased (1.1548 -> 0.9388). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 3.911, Train metrics: {'f1_per_class': array([0.00884956, 0.67892318, 0.04109589]), 'f1_micro': 0.43762950683796104, 'f1_macro': 0.24295620862378753, 'precision_per_class': array([0.22222222, 0.51596806, 0.75      ]), 'precision_micro': 0.5161290322580645, 'precision_macro': 0.4960634286981593, 'recall_per_class': array([0.00451467, 0.99232246, 0.02112676]), 'recall_micro': 0.37985611510791367, 'recall_macro': 0.3393212966878101, 'mcc_per_class': [-0.04125070812961304, 0.013442736861042918, 0.07294409423812984], 'mcc_overall': 0.08281158856046432}\n",
      "            Val Loss:   0.833, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.9388 -> 0.8326). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 1.217, Train metrics: {'f1_per_class': array([0.        , 0.67931937, 0.01395349]), 'f1_micro': 0.43463780183180684, 'f1_macro': 0.2310909533666139, 'precision_per_class': array([0.        , 0.51539225, 0.75      ]), 'precision_micro': 0.5158102766798419, 'precision_macro': 0.4217974180734856, 'recall_per_class': array([0.        , 0.99616123, 0.00704225]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.3344011606426789, 'mcc_per_class': [-0.027788621816138795, 0.0019334371402565543, 0.04194667270462256], 'mcc_overall': 0.08168800454601882}\n",
      "            Val Loss:   0.823, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.8326 -> 0.8233). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 1.217, Train metrics: {'f1_per_class': array([0.        , 0.681074  , 0.02314815]), 'f1_micro': 0.4369538077403246, 'f1_macro': 0.23474071648596861, 'precision_per_class': array([0.        , 0.51689861, 0.83333333]), 'precision_micro': 0.5182625863770978, 'precision_macro': 0.45007731389441136, 'recall_per_class': array([0.        , 0.99808061, 0.01173709]), 'recall_micro': 0.3776978417266187, 'recall_macro': 0.33660590113511096, 'mcc_per_class': [-0.027788621816138795, 0.04448231926608054, 0.06446513069280281], 'mcc_overall': 0.08523396514272352}\n",
      "            Val Loss:   0.798, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.8233 -> 0.7977). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 1.024, Train metrics: {'f1_per_class': array([0.        , 0.681074  , 0.01388889]), 'f1_micro': 0.43547044129891754, 'f1_macro': 0.23165429673288218, 'precision_per_class': array([0.        , 0.51689861, 0.5       ]), 'precision_micro': 0.516798418972332, 'precision_macro': 0.3389662027833002, 'recall_per_class': array([0.        , 0.99808061, 0.00704225]), 'recall_micro': 0.3762589928057554, 'recall_macro': 0.3350409559081939, 'mcc_per_class': [0.0, 0.04448231926608054, 0.012304868883740272], 'mcc_overall': 0.08309137657250526}\n",
      "            Val Loss:   0.795, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7977 -> 0.7946). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 0.765, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.793, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7946 -> 0.7934). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.816, Train metrics: {'f1_per_class': array([0.        , 0.67973856, 0.01398601]), 'f1_micro': 0.43528922180607577, 'f1_macro': 0.23124152535917242, 'precision_per_class': array([0.        , 0.51536174, 1.        ]), 'precision_micro': 0.5162882527147088, 'precision_macro': 0.5051205814337628, 'recall_per_class': array([0.        , 0.99808061, 0.00704225]), 'recall_micro': 0.3762589928057554, 'recall_macro': 0.3350409559081939, 'mcc_per_class': [-0.027788621816138795, 0.001365790888828626, 0.06392982193621988], 'mcc_overall': 0.08242791248441975}\n",
      "            Val Loss:   0.777, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7934 -> 0.7770). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 14, Train Loss: 0.779, Train metrics: {'f1_per_class': array([0.00449438, 0.68060091, 0.00468384]), 'f1_micro': 0.43528922180607577, 'f1_macro': 0.2299263790689654, 'precision_per_class': array([0.5       , 0.51584158, 1.        ]), 'precision_micro': 0.5162882527147088, 'precision_macro': 0.671947194719472, 'recall_per_class': array([0.00225734, 1.        , 0.00234742]), 'recall_micro': 0.3762589928057554, 'recall_macro': 0.3348682513944969, 'mcc_per_class': [0.005547194594233466, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08242791248441975}\n",
      "            Val Loss:   0.763, Val F1:   {'f1_per_class': array([0.03571429, 0.68062827, 0.        ]), 'f1_micro': 0.44, 'f1_macro': 0.23878085265519822, 'precision_per_class': array([1.        , 0.51587302, 0.        ]), 'precision_micro': 0.5196850393700787, 'precision_macro': 0.5052910052910052, 'recall_per_class': array([0.01818182, 1.        , 0.        ]), 'recall_micro': 0.3815028901734104, 'recall_macro': 0.33939393939393936, 'mcc_per_class': [0.10162318990896088, 0.0, 0.0], 'mcc_overall': 0.08854012511424803}\n",
      "*****\n",
      "Validation loss decreased (0.7770 -> 0.7629). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 15, Train Loss: 1.134, Train metrics: {'f1_per_class': array([0.00449438, 0.68060091, 0.00934579]), 'f1_micro': 0.43594009983361065, 'f1_macro': 0.23148036361666835, 'precision_per_class': array([0.5       , 0.51584158, 1.        ]), 'precision_micro': 0.5167652859960552, 'precision_macro': 0.671947194719472, 'recall_per_class': array([0.00225734, 1.        , 0.00469484]), 'recall_micro': 0.376978417266187, 'recall_macro': 0.3356507240079554, 'mcc_per_class': [0.005547194594233466, 0.03244592311606976, 0.05217260817560143], 'mcc_overall': 0.08316750165237724}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7629 -> 0.7527). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 16, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.0046729 ]), 'f1_micro': 0.43463780183180684, 'f1_macro': 0.2284246038770905, 'precision_per_class': array([0.        , 0.51584158, 0.5       ]), 'precision_micro': 0.5158102766798419, 'precision_macro': 0.33861386138613864, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.007090123675145835], 'mcc_overall': 0.08168800454601882}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7527 -> 0.7517). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 17, Train Loss: 0.856, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00934579]), 'f1_micro': 0.43528922180607577, 'f1_macro': 0.22998223627584438, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.5162882527147088, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00469484]), 'recall_micro': 0.3762589928057554, 'recall_macro': 0.3348982785602504, 'mcc_per_class': [-0.027788621816138795, 0.03244592311606976, 0.05217260817560143], 'mcc_overall': 0.08242791248441975}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7517 -> 0.7510). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 18, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 19, Train Loss: 0.771, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43463780183180684, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.5158102766798419, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [-0.027788621816138795, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08168800454601882}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 20, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 21, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 22, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 23, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 24, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.        , 0.68060091, 0.00468384]), 'f1_micro': 0.43481882548937945, 'f1_macro': 0.22842825172814143, 'precision_per_class': array([0.        , 0.51584158, 1.        ]), 'precision_micro': 0.516320474777448, 'precision_macro': 0.5052805280528053, 'recall_per_class': array([0.        , 1.        , 0.00234742]), 'recall_micro': 0.37553956834532376, 'recall_macro': 0.33411580594679186, 'mcc_per_class': [0.0, 0.03244592311606976, 0.036873337339521854], 'mcc_overall': 0.08235149508075852}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.        , 0.68062827, 0.        ]), 'f1_micro': 0.43478260869565216, 'f1_macro': 0.22687609075043633, 'precision_per_class': array([0.        , 0.51587302, 0.        ]), 'precision_micro': 0.5158730158730159, 'precision_macro': 0.17195767195767198, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.37572254335260113, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.08260525969894966}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "[{'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 32, 'val_loss': 0.9719278365373611, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.429042904290429, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5038759689922481, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.07911877721292356, 0.0, -0.0753940029727543], 'mcc_overall': 0.06777130152946205}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 64, 'val_loss': 1.6538964211940765, 'val_metrics': {'f1_per_class': array([0.06896552, 0.67015707, 0.57303371]), 'f1_micro': 0.5480093676814989, 'f1_macro': 0.4373854310564584, 'precision_per_class': array([1.        , 0.50793651, 0.408     ]), 'precision_micro': 0.4624505928853755, 'precision_macro': 0.6386455026455026, 'recall_per_class': array([0.03571429, 0.98461538, 0.96226415]), 'recall_micro': 0.6724137931034483, 'recall_macro': 0.6608646070910222, 'mcc_per_class': [0.1424279266355945, -0.0870069397818793, -0.14946445276890902], 'mcc_overall': 0.016250545381728078}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 128, 'val_loss': 1.0704892873764038, 'val_metrics': {'f1_per_class': array([0.61111111, 0.67708333, 0.07142857]), 'f1_micro': 0.5700934579439252, 'f1_macro': 0.45320767195767203, 'precision_per_class': array([0.44354839, 0.51181102, 0.66666667]), 'precision_micro': 0.48031496062992124, 'precision_macro': 0.5406753591284961, 'recall_per_class': array([0.98214286, 1.        , 0.03773585]), 'recall_micro': 0.7011494252873564, 'recall_macro': 0.673292902066487, 'mcc_per_class': [0.0337123633364026, 0.0, 0.0786499300458639], 'mcc_overall': 0.06706527510148438}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 256, 'val_loss': 1.4745376259088516, 'val_metrics': {'f1_per_class': array([0.6043956 , 0.        , 0.58100559]), 'f1_micro': 0.5023474178403756, 'f1_macro': 0.395133730329261, 'precision_per_class': array([0.43650794, 0.        , 0.41269841]), 'precision_micro': 0.4246031746031746, 'precision_macro': 0.2830687830687831, 'recall_per_class': array([0.98214286, 0.        , 0.98113208]), 'recall_micro': 0.6149425287356322, 'recall_macro': 0.6544249775381851, 'mcc_per_class': [-0.10031130682352807, 0.0, -0.10526709849026072], 'mcc_overall': -0.09004026736191141}}, {'n_layers': 1, 'n_layers_egnn': 2, 'hidden_dim': 32, 'val_loss': 0.7523949295282364, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.4318936877076412, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5118110236220472, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.07824282095173178}}]\n",
      "\n",
      "Training model with n_layers=1, n_layers_egnn=2, hidden_dim=64\n",
      "Epoch 1, Train Loss: 249.525, Train metrics: {'f1_per_class': array([0.51771654, 0.48955224, 0.50844278]), 'f1_micro': 0.5053449951409135, 'f1_macro': 0.5052371836581669, 'precision_per_class': array([0.45898778, 0.50826446, 0.4234375 ]), 'precision_micro': 0.45963464938126103, 'precision_macro': 0.46356324880167693, 'recall_per_class': array([0.59367946, 0.47216891, 0.63615023]), 'recall_micro': 0.5611510791366906, 'recall_macro': 0.5673328663103859, 'mcc_per_class': [0.0479668048046864, -0.013551273134361302, 0.005512760225135832], 'mcc_overall': 0.0030367407361021445}\n",
      "            Val Loss:   14.963, Val F1:   {'f1_per_class': array([0.59060403, 0.3960396 , 0.50746269]), 'f1_micro': 0.5104166666666666, 'f1_macro': 0.4980354391243993, 'precision_per_class': array([0.46808511, 0.55555556, 0.41975309]), 'precision_micro': 0.46445497630331756, 'precision_macro': 0.4811312494527624, 'recall_per_class': array([0.8       , 0.30769231, 0.64150943]), 'recall_micro': 0.5664739884393064, 'recall_macro': 0.5830672472181906, 'mcc_per_class': [0.1091244749534345, 0.05022019609518254, -0.002396593506374223], 'mcc_overall': 0.015303748495000621}\n",
      "*****\n",
      "Validation loss decreased (inf -> 14.9627). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 54.151, Train metrics: {'f1_per_class': array([0.56914468, 0.35459184, 0.52135231]), 'f1_micro': 0.49889205444761, 'f1_macro': 0.4816962780515172, 'precision_per_class': array([0.44059406, 0.52851711, 0.41977077]), 'precision_micro': 0.44544940644431885, 'precision_macro': 0.4629606477703562, 'recall_per_class': array([0.80361174, 0.26679463, 0.68779343]), 'recall_micro': 0.5669064748201439, 'recall_macro': 0.5860665970329336, 'mcc_per_class': [0.009706824559694611, 0.015644667482158748, -0.004821363877719748], 'mcc_overall': -0.030492526122065242}\n",
      "            Val Loss:   22.608, Val F1:   {'f1_per_class': array([0.60115607, 0.4       , 0.52229299]), 'f1_micro': 0.5270588235294118, 'f1_macro': 0.5078163543315783, 'precision_per_class': array([0.44067797, 0.63333333, 0.39423077]), 'precision_micro': 0.4444444444444444, 'precision_macro': 0.4894140228885991, 'recall_per_class': array([0.94545455, 0.29230769, 0.77358491]), 'recall_micro': 0.6473988439306358, 'recall_macro': 0.6704490478075383, 'mcc_per_class': [0.03229201705937779, 0.13139085248226046, -0.11629170123453067], 'mcc_overall': -0.037547845317704394}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 63.813, Train metrics: {'f1_per_class': array([0.59183673, 0.22556391, 0.57925926]), 'f1_micro': 0.5149099498080898, 'f1_macro': 0.4655533012425243, 'precision_per_class': array([0.43702906, 0.52083333, 0.42316017]), 'precision_micro': 0.43665498247371054, 'precision_macro': 0.46034085666755203, 'recall_per_class': array([0.91647856, 0.14395393, 0.91784038]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6594242885441592, 'mcc_per_class': [-0.007807911458471979, 0.004486677147059945, 0.011848263880896721], 'mcc_overall': -0.06029137506594086}\n",
      "            Val Loss:   7.468, Val F1:   {'f1_per_class': array([0.58285714, 0.20253165, 0.58757062]), 'f1_micro': 0.5150812064965197, 'f1_macro': 0.45765313663189655, 'precision_per_class': array([0.425     , 0.57142857, 0.41935484]), 'precision_micro': 0.43023255813953487, 'precision_macro': 0.4719278033794163, 'recall_per_class': array([0.92727273, 0.12307692, 0.98113208]), 'recall_micro': 0.6416184971098265, 'recall_macro': 0.6771605752737827, 'mcc_per_class': [-0.10377015870985441, 0.039303520309269174, -0.02041760465737363], 'mcc_overall': -0.08075801786022827}\n",
      "*****\n",
      "Validation loss decreased (14.9627 -> 7.4680). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 10.375, Train metrics: {'f1_per_class': array([0.60691602, 0.10327022, 0.31786217]), 'f1_micro': 0.4230343300110742, 'f1_macro': 0.34268280315854655, 'precision_per_class': array([0.44147844, 0.5       , 0.39649123]), 'precision_micro': 0.43442001516300227, 'precision_macro': 0.4459898891650756, 'recall_per_class': array([0.97065463, 0.05758157, 0.26525822]), 'recall_micro': 0.41223021582733815, 'recall_macro': 0.4311648057994326, 'mcc_per_class': [0.03410826661841355, -0.007705488262400932, -0.03156195756027293], 'mcc_overall': -0.042029471667928246}\n",
      "            Val Loss:   11.857, Val F1:   {'f1_per_class': array([0.59649123, 0.16216216, 0.03508772]), 'f1_micro': 0.3841059602649007, 'f1_macro': 0.26458036984352773, 'precision_per_class': array([0.43965517, 0.66666667, 0.25      ]), 'precision_micro': 0.4496124031007752, 'precision_macro': 0.4521072796934866, 'recall_per_class': array([0.92727273, 0.09230769, 0.01886792]), 'recall_micro': 0.3352601156069364, 'recall_macro': 0.34614944803624054, 'mcc_per_class': [0.021613162460827576, 0.08368744886125766, -0.06258772639834544], 'mcc_overall': -0.011643888541100036}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 13.620, Train metrics: {'f1_per_class': array([0.5968661 , 0.11359725, 0.18285714]), 'f1_micro': 0.398406374501992, 'f1_macro': 0.2977734952835354, 'precision_per_class': array([0.43600416, 0.55      , 0.48484848]), 'precision_micro': 0.44642857142857145, 'precision_macro': 0.49028421572646347, 'recall_per_class': array([0.94582393, 0.06333973, 0.11267606]), 'recall_micro': 0.3597122302158273, 'recall_macro': 0.37394657179641794, 'mcc_per_class': [-0.019225704194622044, 0.017424346038525978, 0.0423593669863496], 'mcc_overall': -0.018218489554271623}\n",
      "            Val Loss:   4.662, Val F1:   {'f1_per_class': array([0.59770115, 0.11267606, 0.56140351]), 'f1_micro': 0.5, 'f1_macro': 0.4239269048450818, 'precision_per_class': array([0.43697479, 0.66666667, 0.40677966]), 'precision_micro': 0.4279835390946502, 'precision_macro': 0.5034737058665274, 'recall_per_class': array([0.94545455, 0.06153846, 0.90566038]), 'recall_micro': 0.6011560693641619, 'recall_macro': 0.6375511281171659, 'mcc_per_class': [0.003881191040835057, 0.06747097830170132, -0.10779084050174915], 'mcc_overall': -0.07994938637332721}\n",
      "*****\n",
      "Validation loss decreased (7.4680 -> 4.6622). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 28.330, Train metrics: {'f1_per_class': array([0.60335196, 0.08586762, 0.5884017 ]), 'f1_micro': 0.5121879588839942, 'f1_macro': 0.42587375779039754, 'precision_per_class': array([0.43680485, 0.63157895, 0.42105263]), 'precision_micro': 0.4327543424317618, 'precision_macro': 0.49647881077820943, 'recall_per_class': array([0.9751693 , 0.04606526, 0.97652582]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6659201269796867, 'mcc_per_class': [-0.018583057940353784, 0.045967787249560216, -0.004146001655037549], 'mcc_overall': -0.07210957981338584}\n",
      "            Val Loss:   3.034, Val F1:   {'f1_per_class': array([0.61016949, 0.02898551, 0.57954545]), 'f1_micro': 0.5023696682464455, 'f1_macro': 0.4062334844390851, 'precision_per_class': array([0.44262295, 0.25      , 0.41463415]), 'precision_micro': 0.42570281124497994, 'precision_macro': 0.36908569905371186, 'recall_per_class': array([0.98181818, 0.01538462, 0.96226415]), 'recall_micro': 0.6127167630057804, 'recall_macro': 0.6531556493820645, 'mcc_per_class': [0.06809377690001776, -0.09633260212060814, -0.07783418828458256], 'mcc_overall': -0.0891512992879644}\n",
      "*****\n",
      "Validation loss decreased (4.6622 -> 3.0341). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 5.424, Train metrics: {'f1_per_class': array([0.61099513, 0.04797048, 0.58815233]), 'f1_micro': 0.5116279069767442, 'f1_macro': 0.4157059785555557, 'precision_per_class': array([0.4416499 , 0.61904762, 0.4203629 ]), 'precision_micro': 0.43298455406078723, 'precision_macro': 0.4936868072232679, 'recall_per_class': array([0.99097065, 0.02495202, 0.97887324]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6649319698064152, 'mcc_per_class': [0.053475919828700445, 0.030225477344439523, -0.0146639405956938], 'mcc_overall': -0.07103896275150784}\n",
      "            Val Loss:   1.513, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.58426966]), 'f1_micro': 0.5023474178403756, 'f1_macro': 0.39733482318372754, 'precision_per_class': array([0.43650794, 0.        , 0.416     ]), 'precision_micro': 0.42292490118577075, 'precision_macro': 0.2841693121693121, 'recall_per_class': array([1.        , 0.        , 0.98113208]), 'recall_micro': 0.6184971098265896, 'recall_macro': 0.660377358490566, 'mcc_per_class': [0.0, -0.13109795466600435, -0.10497079557919099], 'mcc_overall': -0.09922362562763135}\n",
      "*****\n",
      "Validation loss decreased (3.0341 -> 1.5129). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 0.824, Train metrics: {'f1_per_class': array([0.61061337, 0.01140684, 0.59357542]), 'f1_micro': 0.5110002933411557, 'f1_macro': 0.40519854439682357, 'precision_per_class': array([0.43948413, 0.6       , 0.42246521]), 'precision_micro': 0.4314016840019812, 'precision_macro': 0.4873164452438807, 'recall_per_class': array([1.        , 0.00575816, 0.99765258]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6678035798497532, 'mcc_per_class': [0.04817903050653849, 0.011943809690014098, 0.031605550660996955], 'mcc_overall': -0.07615409379082701}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (1.5129 -> 0.7532). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.60977288, 0.00382409, 0.59331476]), 'f1_micro': 0.5099648300117233, 'f1_macro': 0.40230391289943995, 'precision_per_class': array([0.43861386, 0.5       , 0.42178218]), 'precision_micro': 0.43026706231454004, 'precision_macro': 0.4534653465346534, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, -0.001365790888828626, 0.02685135334467745], 'mcc_overall': -0.07954405774845993}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.61061337, 0.01145038, 0.59414226]), 'f1_micro': 0.5115869756526841, 'f1_macro': 0.4054020037277362, 'precision_per_class': array([0.43948413, 1.        , 0.42261905]), 'precision_micro': 0.4318969787023279, 'precision_macro': 0.6207010582010583, 'recall_per_class': array([1.        , 0.00575816, 1.        ]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6685860524632118, 'mcc_per_class': [0.04817903050653849, 0.05290656184849171, 0.04655402417919602], 'mcc_overall': -0.07475141221851696}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7532 -> 0.7531). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.61061337, 0.01145038, 0.59414226]), 'f1_micro': 0.5115869756526841, 'f1_macro': 0.4054020037277362, 'precision_per_class': array([0.43948413, 1.        , 0.42261905]), 'precision_micro': 0.4318969787023279, 'precision_macro': 0.6207010582010583, 'recall_per_class': array([1.        , 0.00575816, 1.        ]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6685860524632118, 'mcc_per_class': [0.04817903050653849, 0.05290656184849171, 0.04655402417919602], 'mcc_overall': -0.07475141221851696}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7531 -> 0.7530). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 0.753, Train metrics: {'f1_per_class': array([0.61061337, 0.01145038, 0.59372822]), 'f1_micro': 0.5114369501466276, 'f1_macro': 0.40526399158849946, 'precision_per_class': array([0.43948413, 1.        , 0.4222002 ]), 'precision_micro': 0.4316831683168317, 'precision_macro': 0.6205614417333941, 'recall_per_class': array([1.        , 0.00575816, 1.        ]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6685860524632118, 'mcc_per_class': [0.04817903050653849, 0.05290656184849171, 0.03799236082530976], 'mcc_overall': -0.07541278083866033}\n",
      "            Val Loss:   0.753, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7530 -> 0.7526). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.854, Train metrics: {'f1_per_class': array([0.61145618, 0.01520913, 0.59414226]), 'f1_micro': 0.5121736579642123, 'f1_macro': 0.4069358538543597, 'precision_per_class': array([0.44035785, 0.8       , 0.42261905]), 'precision_micro': 0.4323922734026746, 'precision_macro': 0.5543256335005838, 'recall_per_class': array([1.        , 0.00767754, 1.        ]), 'recall_micro': 0.6280575539568345, 'recall_macro': 0.6692258477287268, 'mcc_per_class': [0.06226065812392125, 0.040156874168061414, 0.04655402417919602], 'mcc_overall': -0.07334873064620691}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7526 -> 0.7524). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 14, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7524 -> 0.7523). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 15, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 16, Train Loss: 0.871, Train metrics: {'f1_per_class': array([0.60935351, 0.00382409, 0.59331476]), 'f1_micro': 0.5098154116612951, 'f1_macro': 0.4021641208582458, 'precision_per_class': array([0.43818002, 0.5       , 0.42178218]), 'precision_micro': 0.4300543746910529, 'precision_macro': 0.4533207326667385, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.0, -0.001365790888828626, 0.02685135334467745], 'mcc_overall': -0.08020722666406664}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7523 -> 0.7521). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 17, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61061337, 0.01145038, 0.59414226]), 'f1_micro': 0.5115869756526841, 'f1_macro': 0.4054020037277362, 'precision_per_class': array([0.43948413, 1.        , 0.42261905]), 'precision_micro': 0.4318969787023279, 'precision_macro': 0.6207010582010583, 'recall_per_class': array([1.        , 0.00575816, 1.        ]), 'recall_micro': 0.6273381294964029, 'recall_macro': 0.6685860524632118, 'mcc_per_class': [0.04817903050653849, 0.05290656184849171, 0.04655402417919602], 'mcc_overall': -0.07475141221851696}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7521 -> 0.7520). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 18, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7520 -> 0.7519). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 19, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7519 -> 0.7517). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 20, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 21, Train Loss: 0.771, Train metrics: {'f1_per_class': array([0.60977288, 0.00764818, 0.59331476]), 'f1_micro': 0.5105509964830012, 'f1_macro': 0.40357861015884094, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4307616221562809, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.027788621816138795, 0.04317661519522753, 0.02685135334467745], 'mcc_overall': -0.07814033908231065}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7517 -> 0.7517). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 22, Train Loss: 0.762, Train metrics: {'f1_per_class': array([0.60977288, 0.00382409, 0.59331476]), 'f1_micro': 0.5099648300117233, 'f1_macro': 0.40230391289943995, 'precision_per_class': array([0.43861386, 0.5       , 0.42178218]), 'precision_micro': 0.43026706231454004, 'precision_macro': 0.4534653465346534, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, -0.001365790888828626, 0.02685135334467745], 'mcc_overall': -0.07954405774845993}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7517 -> 0.7517). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 23, Train Loss: 0.759, Train metrics: {'f1_per_class': array([0.60977288, 0.00382409, 0.59331476]), 'f1_micro': 0.5099648300117233, 'f1_macro': 0.40230391289943995, 'precision_per_class': array([0.43861386, 0.5       , 0.42178218]), 'precision_micro': 0.43026706231454004, 'precision_macro': 0.4534653465346534, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, -0.001365790888828626, 0.02685135334467745], 'mcc_overall': -0.07954405774845993}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7517 -> 0.7514). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 24, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.59290188]), 'f1_micro': 0.5093786635404455, 'f1_macro': 0.40075179549324735, 'precision_per_class': array([0.43818002, 0.        , 0.42136499]), 'precision_micro': 0.4297725024727992, 'precision_macro': 0.2865150016485328, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08094777641460923}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 25, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 26, Train Loss: 0.754, Train metrics: {'f1_per_class': array([0.60977288, 0.00382409, 0.59331476]), 'f1_micro': 0.5099648300117233, 'f1_macro': 0.40230391289943995, 'precision_per_class': array([0.43861386, 0.5       , 0.42178218]), 'precision_micro': 0.43026706231454004, 'precision_macro': 0.4534653465346534, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, -0.001365790888828626, 0.02685135334467745], 'mcc_overall': -0.07954405774845993}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 27, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.59290188]), 'f1_micro': 0.5093786635404455, 'f1_macro': 0.40075179549324735, 'precision_per_class': array([0.43818002, 0.        , 0.42136499]), 'precision_micro': 0.4297725024727992, 'precision_macro': 0.2865150016485328, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08094777641460923}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7514 -> 0.7513). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 28, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 29, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7513 -> 0.7512). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 30, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.50946644]), 'f1_micro': 0.4711507810009563, 'f1_macro': 0.37293998158087255, 'precision_per_class': array([0.43818002, 0.        , 0.40217391]), 'precision_micro': 0.42301087578706353, 'precision_macro': 0.280117977608624, 'recall_per_class': array([1.        , 0.        , 0.69483568]), 'recall_micro': 0.5316546762589928, 'recall_macro': 0.564945226917058, 'mcc_per_class': [0.0, 0.0, -0.06358288370098904], 'mcc_overall': -0.08253080651750118}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 31, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7512 -> 0.7512). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 32, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 33, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7512 -> 0.7510). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 34, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 35, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 36, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 37, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7510 -> 0.7509). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 38, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 39, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.59290188]), 'f1_micro': 0.5093786635404455, 'f1_macro': 0.40075179549324735, 'precision_per_class': array([0.43818002, 0.        , 0.42136499]), 'precision_micro': 0.4297725024727992, 'precision_macro': 0.2865150016485328, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08094777641460923}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7509 -> 0.7509). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 40, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 41, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 42, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 43, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.59290188]), 'f1_micro': 0.5093786635404455, 'f1_macro': 0.40075179549324735, 'precision_per_class': array([0.43818002, 0.        , 0.42136499]), 'precision_micro': 0.4297725024727992, 'precision_macro': 0.2865150016485328, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08094777641460923}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 44, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.59290188]), 'f1_micro': 0.5093786635404455, 'f1_macro': 0.40075179549324735, 'precision_per_class': array([0.43818002, 0.        , 0.42136499]), 'precision_micro': 0.4297725024727992, 'precision_macro': 0.2865150016485328, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08094777641460923}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 45, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "Validation loss decreased (0.7509 -> 0.7509). Saving model...\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 46, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 47, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 48, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 49, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5.000000000000001e-07]\n",
      "Epoch 50, Train Loss: 0.750, Train metrics: {'f1_per_class': array([0.61019284, 0.00764818, 0.59372822]), 'f1_micro': 0.510850439882698, 'f1_macro': 0.40385641467282857, 'precision_per_class': array([0.43904856, 1.        , 0.4222002 ]), 'precision_micro': 0.4311881188118812, 'precision_macro': 0.6204162537165511, 'recall_per_class': array([1.        , 0.00383877, 1.        ]), 'recall_micro': 0.6266187050359712, 'recall_macro': 0.6679462571976967, 'mcc_per_class': [0.039318515283926804, 0.04317661519522753, 0.03799236082530976], 'mcc_overall': -0.07681580716781221}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5.000000000000001e-07]\n",
      "Epoch 51, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60935351, 0.        , 0.59290188]), 'f1_micro': 0.5093786635404455, 'f1_macro': 0.40075179549324735, 'precision_per_class': array([0.43818002, 0.        , 0.42136499]), 'precision_micro': 0.4297725024727992, 'precision_macro': 0.2865150016485328, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.625179856115108, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08094777641460923}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5.000000000000001e-07]\n",
      "Epoch 52, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.60977288, 0.00383142, 0.59331476]), 'f1_micro': 0.5101143359718557, 'f1_macro': 0.4023063548482128, 'precision_per_class': array([0.43861386, 1.        , 0.42178218]), 'precision_micro': 0.4304799604156358, 'precision_macro': 0.62013201320132, 'recall_per_class': array([1.        , 0.00191939, 1.        ]), 'recall_micro': 0.6258992805755396, 'recall_macro': 0.6673064619321817, 'mcc_per_class': [0.027788621816138795, 0.030515359552541615, 0.02685135334467745], 'mcc_overall': -0.07888126049304595}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.        , 0.59217877]), 'f1_micro': 0.508235294117647, 'f1_macro': 0.3999711925265183, 'precision_per_class': array([0.43650794, 0.        , 0.42063492]), 'precision_micro': 0.42857142857142855, 'precision_macro': 0.28571428571428575, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6242774566473989, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.08260525969894966}\n",
      "*****\n",
      "No improvement for 7 epochs.\n",
      "Early stopping triggered after 7 epochs without improvement.\n",
      "[{'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 32, 'val_loss': 0.9719278365373611, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.429042904290429, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5038759689922481, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [-0.07911877721292356, 0.0, -0.0753940029727543], 'mcc_overall': 0.06777130152946205}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 64, 'val_loss': 1.6538964211940765, 'val_metrics': {'f1_per_class': array([0.06896552, 0.67015707, 0.57303371]), 'f1_micro': 0.5480093676814989, 'f1_macro': 0.4373854310564584, 'precision_per_class': array([1.        , 0.50793651, 0.408     ]), 'precision_micro': 0.4624505928853755, 'precision_macro': 0.6386455026455026, 'recall_per_class': array([0.03571429, 0.98461538, 0.96226415]), 'recall_micro': 0.6724137931034483, 'recall_macro': 0.6608646070910222, 'mcc_per_class': [0.1424279266355945, -0.0870069397818793, -0.14946445276890902], 'mcc_overall': 0.016250545381728078}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 128, 'val_loss': 1.0704892873764038, 'val_metrics': {'f1_per_class': array([0.61111111, 0.67708333, 0.07142857]), 'f1_micro': 0.5700934579439252, 'f1_macro': 0.45320767195767203, 'precision_per_class': array([0.44354839, 0.51181102, 0.66666667]), 'precision_micro': 0.48031496062992124, 'precision_macro': 0.5406753591284961, 'recall_per_class': array([0.98214286, 1.        , 0.03773585]), 'recall_micro': 0.7011494252873564, 'recall_macro': 0.673292902066487, 'mcc_per_class': [0.0337123633364026, 0.0, 0.0786499300458639], 'mcc_overall': 0.06706527510148438}}, {'n_layers': 1, 'n_layers_egnn': 1, 'hidden_dim': 256, 'val_loss': 1.4745376259088516, 'val_metrics': {'f1_per_class': array([0.6043956 , 0.        , 0.58100559]), 'f1_micro': 0.5023474178403756, 'f1_macro': 0.395133730329261, 'precision_per_class': array([0.43650794, 0.        , 0.41269841]), 'precision_micro': 0.4246031746031746, 'precision_macro': 0.2830687830687831, 'recall_per_class': array([0.98214286, 0.        , 0.98113208]), 'recall_micro': 0.6149425287356322, 'recall_macro': 0.6544249775381851, 'mcc_per_class': [-0.10031130682352807, 0.0, -0.10526709849026072], 'mcc_overall': -0.09004026736191141}}, {'n_layers': 1, 'n_layers_egnn': 2, 'hidden_dim': 32, 'val_loss': 0.7523949295282364, 'val_metrics': {'f1_per_class': array([0.        , 0.67708333, 0.        ]), 'f1_micro': 0.4318936877076412, 'f1_macro': 0.22569444444444445, 'precision_per_class': array([0.        , 0.51181102, 0.        ]), 'precision_micro': 0.5118110236220472, 'precision_macro': 0.1706036745406824, 'recall_per_class': array([0., 1., 0.]), 'recall_micro': 0.3735632183908046, 'recall_macro': 0.3333333333333333, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.07824282095173178}}, {'n_layers': 1, 'n_layers_egnn': 2, 'hidden_dim': 64, 'val_loss': 0.7509341090917587, 'val_metrics': {'f1_per_class': array([0.61202186, 0.        , 0.58888889]), 'f1_micro': 0.5093457943925234, 'f1_macro': 0.4003035822707954, 'precision_per_class': array([0.44094488, 0.        , 0.41732283]), 'precision_micro': 0.42913385826771655, 'precision_macro': 0.28608923884514437, 'recall_per_class': array([1., 0., 1.]), 'recall_micro': 0.6264367816091954, 'recall_macro': 0.6666666666666666, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': -0.07824282095173178}}]\n",
      "\n",
      "Training model with n_layers=1, n_layers_egnn=2, hidden_dim=128\n",
      "Epoch 1, Train Loss: 180.058, Train metrics: {'f1_per_class': array([0.46569179, 0.52517275, 0.35897436]), 'f1_micro': 0.4571215510812826, 'f1_macro': 0.4499463005654174, 'precision_per_class': array([0.46412556, 0.54065041, 0.39548023]), 'precision_micro': 0.4744582043343653, 'precision_macro': 0.46675206434362737, 'recall_per_class': array([0.46726862, 0.51055662, 0.3286385 ]), 'recall_micro': 0.44100719424460433, 'recall_macro': 0.43548791418613697, 'mcc_per_class': [0.04646019804644346, 0.049326529091993886, -0.03847968409733563], 'mcc_overall': 0.027950072545708287}\n",
      "            Val Loss:   36.834, Val F1:   {'f1_per_class': array([0.44859813, 0.4       , 0.23529412]), 'f1_micro': 0.37857142857142856, 'f1_macro': 0.36129741616272676, 'precision_per_class': array([0.46153846, 0.525     , 0.53333333]), 'precision_micro': 0.4953271028037383, 'precision_macro': 0.5066239316239316, 'recall_per_class': array([0.43636364, 0.32307692, 0.1509434 ]), 'recall_micro': 0.3063583815028902, 'recall_macro': 0.30346131855565817, 'mcc_per_class': [0.04230734745959121, 0.01245538093625725, 0.08392147717670026], 'mcc_overall': 0.047492320172427216}\n",
      "*****\n",
      "Validation loss decreased (inf -> 36.8340). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 2, Train Loss: 81.615, Train metrics: {'f1_per_class': array([0.46205357, 0.46439628, 0.42645382]), 'f1_micro': 0.45149525893508385, 'f1_macro': 0.4509678920328859, 'precision_per_class': array([0.45695364, 0.50223214, 0.41463415]), 'precision_micro': 0.4578402366863905, 'precision_macro': 0.4579399771942374, 'recall_per_class': array([0.46726862, 0.4318618 , 0.43896714]), 'recall_micro': 0.44532374100719424, 'recall_macro': 0.44603252113257136, 'mcc_per_class': [0.03409221427104104, -0.02338106246956835, -0.012232968893606812], 'mcc_overall': -0.0008133491833566728}\n",
      "            Val Loss:   26.299, Val F1:   {'f1_per_class': array([0.20779221, 0.55      , 0.5       ]), 'f1_micro': 0.4459016393442623, 'f1_macro': 0.4192640692640693, 'precision_per_class': array([0.36363636, 0.6       , 0.49090909]), 'precision_micro': 0.5151515151515151, 'precision_macro': 0.48484848484848486, 'recall_per_class': array([0.14545455, 0.50769231, 0.50943396]), 'recall_micro': 0.3930635838150289, 'recall_macro': 0.38752693847033476, 'mcc_per_class': [-0.06757916794059521, 0.1481618662741383, 0.12529076969894157], 'mcc_overall': 0.08451321939098895}\n",
      "*****\n",
      "Validation loss decreased (36.8340 -> 26.2995). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 3, Train Loss: 59.240, Train metrics: {'f1_per_class': array([0.49224806, 0.60202361, 0.55802469]), 'f1_micro': 0.5534517914360617, 'f1_macro': 0.5507654540474999, 'precision_per_class': array([0.43123939, 0.53684211, 0.42965779]), 'precision_micro': 0.46500244738130203, 'precision_macro': 0.46591309624484367, 'recall_per_class': array([0.57336343, 0.68522073, 0.79577465]), 'recall_micro': 0.6834532374100719, 'recall_macro': 0.6847862694683894, 'mcc_per_class': [-0.016526295024585975, 0.059670899346885416, 0.03166154672276773], 'mcc_overall': 0.019346683304654933}\n",
      "            Val Loss:   3.517, Val F1:   {'f1_per_class': array([0.33707865, 0.27906977, 0.59302326]), 'f1_micro': 0.4495677233429395, 'f1_macro': 0.4030572249804025, 'precision_per_class': array([0.44117647, 0.57142857, 0.42857143]), 'precision_micro': 0.4482758620689655, 'precision_macro': 0.4803921568627451, 'recall_per_class': array([0.27272727, 0.18461538, 0.96226415]), 'recall_micro': 0.4508670520231214, 'recall_macro': 0.47320226942868454, 'mcc_per_class': [0.0057225039927158204, 0.04971545769599045, 0.06628650113422979], 'mcc_overall': -0.017418020010189845}\n",
      "*****\n",
      "Validation loss decreased (26.2995 -> 3.5169). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 4, Train Loss: 7.657, Train metrics: {'f1_per_class': array([0.58125   , 0.62909368, 0.57412791]), 'f1_micro': 0.5946082136558327, 'f1_macro': 0.5948238618584578, 'precision_per_class': array([0.44444444, 0.52146465, 0.41578947]), 'precision_micro': 0.45754168282279956, 'precision_macro': 0.4605661881977672, 'recall_per_class': array([0.83972912, 0.79270633, 0.92723005]), 'recall_micro': 0.8489208633093526, 'recall_macro': 0.8532218335201037, 'mcc_per_class': [0.027691368571972425, 0.023338255188258602, -0.0445604966288867], 'mcc_overall': -0.003589701379210174}\n",
      "            Val Loss:   1.550, Val F1:   {'f1_per_class': array([0.59090909, 0.6631016 , 0.59770115]), 'f1_micro': 0.6182495344506518, 'f1_macro': 0.6172372815374844, 'precision_per_class': array([0.42975207, 0.50819672, 0.42975207]), 'precision_micro': 0.45604395604395603, 'precision_macro': 0.4559002845142935, 'recall_per_class': array([0.94545455, 0.95384615, 0.98113208]), 'recall_micro': 0.9595375722543352, 'recall_macro': 0.9601442582574657, 'mcc_per_class': [-0.0670114504521317, -0.08483020186740119, 0.09085268110527737], 'mcc_overall': -0.01666223773245214}\n",
      "*****\n",
      "Validation loss decreased (3.5169 -> 1.5498). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 5, Train Loss: 2.338, Train metrics: {'f1_per_class': array([0.60839161, 0.67722772, 0.59190916]), 'f1_micro': 0.6270096463022508, 'f1_macro': 0.6258428288644228, 'precision_per_class': array([0.44072948, 0.51609658, 0.4242116 ]), 'precision_micro': 0.4605263157894737, 'precision_macro': 0.46034588663703757, 'recall_per_class': array([0.98194131, 0.98464491, 0.97887324]), 'recall_micro': 0.9820143884892086, 'recall_macro': 0.9818198207731127, 'mcc_per_class': [0.032951628205818755, 0.011708244557768026, 0.03415815778214084], 'mcc_overall': 0.029388810544850482}\n",
      "            Val Loss:   0.926, Val F1:   {'f1_per_class': array([0.60335196, 0.68062827, 0.60571429]), 'f1_micro': 0.6311926605504588, 'f1_macro': 0.6298981710909525, 'precision_per_class': array([0.43548387, 0.51587302, 0.43442623]), 'precision_micro': 0.46236559139784944, 'precision_macro': 0.4619277054496515, 'recall_per_class': array([0.98181818, 1.        , 1.        ]), 'recall_micro': 0.9942196531791907, 'recall_macro': 0.993939393939394, 'mcc_per_class': [-0.01625861784302288, 0.0, 0.15428602321452597], 'mcc_overall': 0.07418171308615017}\n",
      "*****\n",
      "Validation loss decreased (1.5498 -> 0.9265). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 6, Train Loss: 2.454, Train metrics: {'f1_per_class': array([0.6092991 , 0.68062827, 0.59119497]), 'f1_micro': 0.6281818181818182, 'f1_macro': 0.6270407795511614, 'precision_per_class': array([0.43987976, 0.5163853 , 0.42089552]), 'precision_micro': 0.4591362126245847, 'precision_macro': 0.45905352826231294, 'recall_per_class': array([0.99097065, 0.99808061, 0.99295775]), 'recall_micro': 0.9942446043165467, 'recall_macro': 0.9940030051032892, 'mcc_per_class': [0.03001588457335534, 0.03346093631444005, -0.012304868883740272], 'mcc_overall': 0.019380084085905122}\n",
      "            Val Loss:   0.768, Val F1:   {'f1_per_class': array([0.61452514, 0.68062827, 0.58757062]), 'f1_micro': 0.6288848263254113, 'f1_macro': 0.6275746777950134, 'precision_per_class': array([0.44354839, 0.51587302, 0.41935484]), 'precision_micro': 0.45989304812834225, 'precision_macro': 0.45959208055982254, 'recall_per_class': array([1.        , 1.        , 0.98113208]), 'recall_micro': 0.9942196531791907, 'recall_macro': 0.9937106918238993, 'mcc_per_class': [0.11177799767078231, 0.0, -0.02041760465737363], 'mcc_overall': 0.043108595069978314}\n",
      "*****\n",
      "Validation loss decreased (0.9265 -> 0.7677). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 7, Train Loss: 1.207, Train metrics: {'f1_per_class': array([0.61122661, 0.68065574, 0.59004905]), 'f1_micro': 0.6284414106939704, 'f1_macro': 0.6273104676302949, 'precision_per_class': array([0.441     , 0.51693227, 0.42057942]), 'precision_micro': 0.459567387687188, 'precision_macro': 0.4595038971652518, 'recall_per_class': array([0.99548533, 0.99616123, 0.98826291]), 'recall_micro': 0.9935251798561151, 'recall_macro': 0.9933031555062671, 'mcc_per_class': [0.054190710329526756, 0.038363708925677364, -0.01591722507248384], 'mcc_overall': 0.026514949366566662}\n",
      "            Val Loss:   0.778, Val F1:   {'f1_per_class': array([0.61452514, 0.68062827, 0.59217877]), 'f1_micro': 0.6302367941712204, 'f1_macro': 0.6291107276219448, 'precision_per_class': array([0.44354839, 0.51587302, 0.42063492]), 'precision_micro': 0.4601063829787234, 'precision_macro': 0.4600187745349036, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.11177799767078231, 0.0, 0.0], 'mcc_overall': 0.06699882656261998}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 8, Train Loss: 1.607, Train metrics: {'f1_per_class': array([0.61346287, 0.6802097 , 0.59160839]), 'f1_micro': 0.6295201273595633, 'f1_macro': 0.6284269877238572, 'precision_per_class': array([0.44288577, 0.51641791, 0.42131474]), 'precision_micro': 0.46025939474559363, 'precision_macro': 0.46020614100890134, 'recall_per_class': array([0.99774266, 0.99616123, 0.99295775]), 'recall_micro': 0.99568345323741, 'recall_macro': 0.9956205461808892, 'mcc_per_class': [0.08309937023573828, 0.028138018812356162, -0.0012186291632384203], 'mcc_overall': 0.04246110406947499}\n",
      "            Val Loss:   0.755, Val F1:   {'f1_per_class': array([0.61111111, 0.67368421, 0.59217877]), 'f1_micro': 0.6265938069216758, 'f1_macro': 0.6256580308623825, 'precision_per_class': array([0.44      , 0.512     , 0.42063492]), 'precision_micro': 0.4574468085106383, 'precision_macro': 0.45754497354497353, 'recall_per_class': array([1.        , 0.98461538, 1.        ]), 'recall_micro': 0.9942196531791907, 'recall_macro': 0.9948717948717949, 'mcc_per_class': [0.07872218936609646, -0.08664694055586906, 0.0], 'mcc_overall': -0.00619642326590705}\n",
      "*****\n",
      "Validation loss decreased (0.7677 -> 0.7546). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 9, Train Loss: 0.826, Train metrics: {'f1_per_class': array([0.61145618, 0.68193717, 0.59160839]), 'f1_micro': 0.629453142727479, 'f1_macro': 0.6283339136856095, 'precision_per_class': array([0.44035785, 0.51737835, 0.42131474]), 'precision_micro': 0.45972820682797483, 'precision_macro': 0.4596836484859286, 'recall_per_class': array([1.        , 1.        , 0.99295775]), 'recall_micro': 0.9978417266187051, 'recall_macro': 0.9976525821596244, 'mcc_per_class': [0.06226065812392125, 0.06498843548862354, -0.0012186291632384203], 'mcc_overall': 0.0395780676989646}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.61111111, 0.68062827, 0.59217877]), 'f1_micro': 0.6290909090909091, 'f1_macro': 0.627972718104047, 'precision_per_class': array([0.44      , 0.51587302, 0.42063492]), 'precision_micro': 0.4588859416445623, 'precision_macro': 0.4588359788359789, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.07872218936609646, 0.0, 0.0], 'mcc_overall': 0.047312450877356325}\n",
      "*****\n",
      "Validation loss decreased (0.7546 -> 0.7518). Saving model...\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 10, Train Loss: 0.918, Train metrics: {'f1_per_class': array([0.61134163, 0.67887508, 0.59020979]), 'f1_micro': 0.6279228149829739, 'f1_macro': 0.6268088346836966, 'precision_per_class': array([0.44067797, 0.51488095, 0.42031873]), 'precision_micro': 0.45870646766169154, 'precision_macro': 0.45862588119408293, 'recall_per_class': array([0.99774266, 0.99616123, 0.99061033]), 'recall_micro': 0.9949640287769784, 'recall_macro': 0.9948380735674308, 'mcc_per_class': [0.056372033172142386, -0.016519804005753536, -0.025376160222729458], 'mcc_overall': 0.010762632537098513}\n",
      "            Val Loss:   0.768, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.59550562]), 'f1_micro': 0.6265938069216758, 'f1_macro': 0.6253779634096124, 'precision_per_class': array([0.432     , 0.51587302, 0.424     ]), 'precision_micro': 0.4574468085106383, 'precision_macro': 0.4572910052910053, 'recall_per_class': array([0.98181818, 1.        , 1.        ]), 'recall_micro': 0.9942196531791907, 'recall_macro': 0.993939393939394, 'mcc_per_class': [-0.10162318990896088, 0.0, 0.0762116735027003], 'mcc_overall': -0.00619642326590705}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 11, Train Loss: 0.927, Train metrics: {'f1_per_class': array([0.60857538, 0.68104575, 0.59425368]), 'f1_micro': 0.6291165114694527, 'f1_macro': 0.6279582703468504, 'precision_per_class': array([0.43868395, 0.51635282, 0.42357642]), 'precision_micro': 0.4596747427812811, 'precision_macro': 0.45953773210358256, 'recall_per_class': array([0.99322799, 1.        , 0.99530516]), 'recall_micro': 0.9964028776978417, 'recall_macro': 0.9961777184299678, 'mcc_per_class': [0.011372328839701839, 0.045908196972884785, 0.04480849020404885], 'mcc_overall': 0.03405920454298691}\n",
      "            Val Loss:   0.763, Val F1:   {'f1_per_class': array([0.6       , 0.68062827, 0.59217877]), 'f1_micro': 0.6254545454545455, 'f1_macro': 0.6242690144003432, 'precision_per_class': array([0.432     , 0.51587302, 0.42063492]), 'precision_micro': 0.4562334217506631, 'precision_macro': 0.4561693121693122, 'recall_per_class': array([0.98181818, 1.        , 1.        ]), 'recall_micro': 0.9942196531791907, 'recall_macro': 0.993939393939394, 'mcc_per_class': [-0.10162318990896088, 0.0, 0.0], 'mcc_overall': -0.05606388687779217}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 12, Train Loss: 1.026, Train metrics: {'f1_per_class': array([0.60953697, 0.67845449, 0.59300699]), 'f1_micro': 0.6280653950953679, 'f1_macro': 0.6269994839915942, 'precision_per_class': array([0.43924303, 0.51491054, 0.42231076]), 'precision_micro': 0.45885865958858657, 'precision_macro': 0.4588214405466273, 'recall_per_class': array([0.99548533, 0.99424184, 0.99530516]), 'recall_micro': 0.9949640287769784, 'recall_macro': 0.9950107780811277, 'mcc_per_class': [0.025658397993984585, -0.011943809690014098, 0.02293890189625262], 'mcc_overall': 0.01432093272943757}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [0.0005]\n",
      "Epoch 13, Train Loss: 0.819, Train metrics: {'f1_per_class': array([0.61061337, 0.68104575, 0.59233449]), 'f1_micro': 0.6290760869565217, 'f1_macro': 0.6279978721656998, 'precision_per_class': array([0.43948413, 0.51635282, 0.42120912]), 'precision_micro': 0.4590218109715796, 'precision_macro': 0.45901535650049025, 'recall_per_class': array([1.        , 1.        , 0.99765258]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9992175273865415, 'mcc_per_class': [0.04817903050653849, 0.045908196972884785, -0.007090123675145835], 'mcc_overall': 0.030448831806742235}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 14, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.61061337, 0.68015666, 0.59372822]), 'f1_micro': 0.6292440018107741, 'f1_macro': 0.6281660836831852, 'precision_per_class': array([0.43948413, 0.51533136, 0.4222002 ]), 'precision_micro': 0.45904887714663145, 'precision_macro': 0.4590052267647162, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.04817903050653849, 0.0, 0.03799236082530976], 'mcc_overall': 0.037376246486506094}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "Validation loss decreased (0.7518 -> 0.7517). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 15, Train Loss: 0.752, Train metrics: {'f1_per_class': array([0.60977288, 0.68060091, 0.59372822]), 'f1_micro': 0.629101606698348, 'f1_macro': 0.6280340070401483, 'precision_per_class': array([0.43861386, 0.51584158, 0.4222002 ]), 'precision_micro': 0.45889732585011556, 'precision_macro': 0.45888521458687, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.027788621816138795, 0.03244592311606976, 0.03799236082530976], 'mcc_overall': 0.033424812321966}\n",
      "            Val Loss:   0.751, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "Validation loss decreased (0.7517 -> 0.7515). Saving model...\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 16, Train Loss: 0.756, Train metrics: {'f1_per_class': array([0.61103448, 0.68149117, 0.5931612 ]), 'f1_micro': 0.6296464188576609, 'f1_macro': 0.6285622845791864, 'precision_per_class': array([0.43992056, 0.51686508, 0.42204568]), 'precision_micro': 0.45962938451356716, 'precision_macro': 0.45961043857022005, 'recall_per_class': array([1.        , 1.        , 0.99765258]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9992175273865415, 'mcc_per_class': [0.055659968426709686, 0.056253711679722825, 0.021872869965615827], 'mcc_overall': 0.04448508571066845}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 1 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 17, Train Loss: 0.847, Train metrics: {'f1_per_class': array([0.61103448, 0.68062827, 0.59414226]), 'f1_micro': 0.6296464188576609, 'f1_macro': 0.6286016714747186, 'precision_per_class': array([0.43992056, 0.5163853 , 0.42261905]), 'precision_micro': 0.45962938451356716, 'precision_macro': 0.45964163553537934, 'recall_per_class': array([1.        , 0.99808061, 1.        ]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9993602047344851, 'mcc_per_class': [0.055659968426709686, 0.03346093631444005, 0.04655402417919602], 'mcc_overall': 0.04448508571066845}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 2 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 18, Train Loss: 0.751, Train metrics: {'f1_per_class': array([0.61103448, 0.68149117, 0.5931612 ]), 'f1_micro': 0.6296464188576609, 'f1_macro': 0.6285622845791864, 'precision_per_class': array([0.43992056, 0.51686508, 0.42204568]), 'precision_micro': 0.45962938451356716, 'precision_macro': 0.45961043857022005, 'recall_per_class': array([1.        , 1.        , 0.99765258]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9992175273865415, 'mcc_per_class': [0.055659968426709686, 0.056253711679722825, 0.021872869965615827], 'mcc_overall': 0.04448508571066845}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 3 epochs.\n",
      "Learning rate adjusted: [5e-05]\n",
      "Epoch 19, Train Loss: 0.841, Train metrics: {'f1_per_class': array([0.61019284, 0.67973856, 0.59372822]), 'f1_micro': 0.6289336653837446, 'f1_macro': 0.6278865408511946, 'precision_per_class': array([0.43904856, 0.51536174, 0.4222002 ]), 'precision_micro': 0.4588701684836472, 'precision_macro': 0.45887016848364715, 'recall_per_class': array([1.        , 0.99808061, 1.        ]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9993602047344851, 'mcc_per_class': [0.039318515283926804, 0.001365790888828626, 0.03799236082530976], 'mcc_overall': 0.02605799942212223}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 4 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 20, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.61061337, 0.68104575, 0.59233449]), 'f1_micro': 0.6290760869565217, 'f1_macro': 0.6279978721656998, 'precision_per_class': array([0.43948413, 0.51635282, 0.42120912]), 'precision_micro': 0.4590218109715796, 'precision_macro': 0.45901535650049025, 'recall_per_class': array([1.        , 1.        , 0.99765258]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9992175273865415, 'mcc_per_class': [0.04817903050653849, 0.045908196972884785, -0.007090123675145835], 'mcc_overall': 0.030448831806742235}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 5 epochs.\n",
      "Learning rate adjusted: [5e-06]\n",
      "Epoch 21, Train Loss: 0.755, Train metrics: {'f1_per_class': array([0.61061337, 0.68104575, 0.5931612 ]), 'f1_micro': 0.6293611236973267, 'f1_macro': 0.6282734406675716, 'precision_per_class': array([0.43948413, 0.51635282, 0.42204568]), 'precision_micro': 0.4593253968253968, 'precision_macro': 0.45929421060041653, 'recall_per_class': array([1.        , 1.        , 0.99765258]), 'recall_micro': 0.9992805755395684, 'recall_macro': 0.9992175273865415, 'mcc_per_class': [0.04817903050653849, 0.045908196972884785, 0.021872869965615827], 'mcc_overall': 0.03801303544974774}\n",
      "            Val Loss:   0.752, Val F1:   {'f1_per_class': array([0.60773481, 0.68062827, 0.59217877]), 'f1_micro': 0.6279491833030852, 'f1_macro': 0.6268472832769546, 'precision_per_class': array([0.43650794, 0.51587302, 0.42063492]), 'precision_micro': 0.4576719576719577, 'precision_macro': 0.4576719576719577, 'recall_per_class': array([1., 1., 1.]), 'recall_micro': 1.0, 'recall_macro': 1.0, 'mcc_per_class': [0.0, 0.0, 0.0], 'mcc_overall': 0.0}\n",
      "*****\n",
      "No improvement for 6 epochs.\n",
      "Learning rate adjusted: [5e-06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Example call to the function\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim_nopca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_edge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_nopca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader_nopca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader_nopca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[118], line 29\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(input_dim, output_dim, dim_edge_attr, device, train_loader, val_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_arch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEGNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     32\u001b[0m trained_model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[115], line 112\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, model_arch, scheduler, epochs, patience)\u001b[0m\n\u001b[1;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_arch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEGNN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch, data\u001b[38;5;241m.\u001b[39mu)\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[117], line 39\u001b[0m, in \u001b[0;36mEGNN_Graph_Classifier.forward\u001b[0;34m(self, h, edge_index, x, batch, graph_features)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Apply EGNN layers\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39megnn_layers:\n\u001b[0;32m---> 39\u001b[0m     h, x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     h_add \u001b[38;5;241m=\u001b[39m global_add_pool(h, batch)\n\u001b[1;32m     41\u001b[0m     h_list\u001b[38;5;241m.\u001b[39mappend(h_add)\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 144\u001b[0m, in \u001b[0;36mEGNN.forward\u001b[0;34m(self, h, x, edges, edge_attr)\u001b[0m\n\u001b[1;32m    142\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_in(h)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 144\u001b[0m     h, x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcl_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_out(h)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h, x\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 99\u001b[0m, in \u001b[0;36mE_GCL.forward\u001b[0;34m(self, h, edge_index, coord, edge_attr, node_attr)\u001b[0m\n\u001b[1;32m     96\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[1;32m     97\u001b[0m radial, coord_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord2radial(edge_index, coord)\n\u001b[0;32m---> 99\u001b[0m edge_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_model(coord, edge_index, coord_diff, edge_feat)\n\u001b[1;32m    101\u001b[0m h, agg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_model(h, edge_index, edge_feat, node_attr)\n",
      "Cell \u001b[0;32mIn[50], line 54\u001b[0m, in \u001b[0;36mE_GCL.edge_model\u001b[0;34m(self, source, target, radial, edge_attr)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([source, target, radial, edge_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention:\n\u001b[1;32m     56\u001b[0m     att_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_mlp(out)\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/gr-fe/saadat/tools/python3_ven_jed/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "n_layers_list = [1, 2, 3]  # Example: different numbers of layers\n",
    "n_layers_egnn_list = [1, 2, 3]  # Example: different numbers of EGNN layers\n",
    "hidden_dim_list = [32, 64, 128, 256]  # Example: different hidden dimensions\n",
    "\n",
    "# Example training function (assuming train and evaluate are already defined)\n",
    "def train_and_evaluate_models(input_dim, output_dim, dim_edge_attr, device, train_loader, val_loader, test_loader, epochs=100):\n",
    "    results = []\n",
    "    \n",
    "    for n_layers in n_layers_list:\n",
    "        for n_layers_egnn in n_layers_egnn_list:\n",
    "            for hidden_dim in hidden_dim_list:\n",
    "                print(f\"\\nTraining model with n_layers={n_layers}, n_layers_egnn={n_layers_egnn}, hidden_dim={hidden_dim}\")\n",
    "                \n",
    "                # Instantiate the model\n",
    "                model = EGNN_Graph_Classifier(input_dim, hidden_dim, output_dim, dim_edge_attr=dim_edge_attr, \n",
    "                                              n_layers=n_layers, n_layers_egnn=n_layers_egnn, \n",
    "                                              use_graph_feat=False, dropout_p=0.5).to(device)\n",
    "                \n",
    "               # define optimizers\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "                \n",
    "                # define schedulers\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "                \n",
    "                # Train the model\n",
    "                trained_model = train(model, optimizer, train_loader, val_loader, model_arch=\"EGNN\", scheduler=scheduler, epochs=epochs)\n",
    "                \n",
    "                # Evaluate the model on the validation set\n",
    "                trained_model.eval()\n",
    "                val_loss, val_metrics = evaluate(trained_model, test_loader, model_arch=\"EGNN\")\n",
    "                \n",
    "                # Log results\n",
    "                results.append({\n",
    "                    'n_layers': n_layers,\n",
    "                    'n_layers_egnn': n_layers_egnn,\n",
    "                    'hidden_dim': hidden_dim,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_metrics': val_metrics\n",
    "                })\n",
    "\n",
    "                print(results)\n",
    "    \n",
    "    # Return the results for all combinations\n",
    "    return results\n",
    "\n",
    "# Example call to the function\n",
    "results = train_and_evaluate_models(input_dim_nopca, output_dim, dim_edge_attr=0, device=device, \n",
    "                                    train_loader=train_loader_nopca, val_loader=val_loader_nopca, \n",
    "                                    test_loader=test_loader_nopca, epochs=100)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Model with n_layers={result['n_layers']}, n_layers_egnn={result['n_layers_egnn']}, hidden_dim={result['hidden_dim']}:\")\n",
    "    print(f\"Validation Loss: {result['val_loss']}, Validation Metrics: {result['val_metrics']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45284bde-ee8a-4f76-bc98-707a148e5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers=1, n_layers_egnn=2, hidden_dim=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79972c-48b1-47a1-8ad1-a1e18906046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = next(iter(val_loader_nopca)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9f0e2d01-88f0-4441-981b-32c2a5a63cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-648.9427,  412.3538,  634.8348]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 8\n",
    "trained_egnn_nopca.eval()\n",
    "trained_egnn_nopca(train_list_norm[idx].x.float(), train_list_norm[idx].edge_index, train_list_norm[idx].coords.float(), train_list_norm[idx].batch)#, train_list_norm[idx].u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dcecc0-c93b-46cd-b631-b481e2fdcc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_norm[idx].x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a805a-f546-4df5-b743-955b6ab00179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905b1fe-91c0-4835-bdb5-ad19ba769379",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data.x, data.edge_index, data.coords.float(), data.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99716f-da9d-4f94-a8ff-4447bb3f02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_egnn_pca.state_dict(), '../res/trained_models/trained_egnn_pca.pth')\n",
    "torch.save(trained_egnn_nopca.state_dict(), '../res/trained_models/trained_egnn_nopca.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b80721-82fd-4a9a-b6a4-c27fbfd9e30d",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cb69a-d406-4d95-97cc-7013db8794ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read models\n",
    "\n",
    "trained_gcn_pca = GNN(\"GCN\", input_dim_pca, hidden_dim, output_dim, graph_features_dim, n_layer=3, use_graph_feat=False).to(device)\n",
    "trained_gcn_pca.load_state_dict(torch.load('../res/trained_models/trained_gcn_pca.pth'))\n",
    "trained_gcn_pca.eval()\n",
    "\n",
    "trained_gat_pca = GNN(\"GAT\", input_dim_pca, hidden_dim, output_dim, graph_features_dim, n_layer=3, heads=2, use_graph_feat=False).to(device)\n",
    "trained_gat_pca.load_state_dict(torch.load('../res/trained_models/trained_gat_pca.pth'))\n",
    "trained_gat_pca.eval()\n",
    "\n",
    "trained_gin_pca = GNN(\"GIN\", input_dim_pca, hidden_dim, output_dim, graph_features_dim, n_layer=3, use_graph_feat=False).to(device)\n",
    "trained_gin_pca.load_state_dict(torch.load('../res/trained_models/trained_gin_pca.pth'))\n",
    "trained_gin_pca.eval()\n",
    "\n",
    "trained_egnn_pca = EGNN_Graph_Classifier(input_dim_pca, hidden_dim, output_dim, dim_edge_attr=0, n_layers=2, use_graph_feat=False, dropout_p=0.5).to(device) \n",
    "trained_egnn_pca.load_state_dict(torch.load('../res/trained_models/trained_egnn_pca.pth'))\n",
    "trained_egnn_pca.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04cef7-93f3-4548-ae76-860012e31ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN\n",
    "\n",
    "gcn_loss, gcn_test_metrics = evaluate(trained_gcn_pca, test_loader_pca)\n",
    "gcn_loss, gcn_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2ef46-6214-472b-98de-bafbae08cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "gat_loss, gat_test_metrics = evaluate(trained_gat_pca, test_loader_pca)\n",
    "gat_loss, gat_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c135c-1e4f-4b93-b2c8-7ff29145f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIN\n",
    "gin_loss, gin_test_metrics = evaluate(trained_gin_pca, test_loader_pca)\n",
    "gin_loss, gin_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b400b41-9138-4e3c-ab20-ced7d5623328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGNN\n",
    "\n",
    "egnn_loss, egnn_test_metrics = evaluate(trained_egnn_pca, test_loader_pca)\n",
    "egnn_loss, egnn_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847049ca-1a1c-4615-982e-664e10518ec1",
   "metadata": {},
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007206c-d7da-4eee-a83b-f3b6a8c98f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "class EGNN_Graph_Classifier(torch.nn.Module):\n",
    "    \"\"\"EGNN model for graph classification\"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out, dim_edge_attr, n_layers=4, use_graph_feat=False, dropout_p=0.5):\n",
    "        super(EGNN_Graph_Classifier, self).__init__()\n",
    "        self.args = (dim_in, dim_h, dim_out, dim_edge_attr, n_layers, use_graph_feat, dropout_p)\n",
    "        self.use_graph_feat = use_graph_feat\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # EGNN layer\n",
    "        self.egnn = EGNN(in_node_nf=dim_in, hidden_nf=dim_h, out_node_nf=dim_h, in_edge_nf=dim_edge_attr, n_layers=n_layers)\n",
    "\n",
    "        # Linear layers for classification\n",
    "        self.lin1 = torch.nn.Linear(dim_h, dim_h)\n",
    "        self.lin2 = torch.nn.Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, h): # this line changed due to GNNExplainer requirements\n",
    "\n",
    "        '''\n",
    "        # Pass through the EGNN model\n",
    "        h1, x1 = self.egnn(h, x, edge_index, edge_attr=None)\n",
    "        h1 = F.dropout(h1, p=self.dropout_p, training=self.training)\n",
    "        \n",
    "        h2, x2 = self.egnn(h1, x1, edge_index, edge_attr=None)\n",
    "        h2 = F.dropout(h2, p=self.dropout_p, training=self.training)\n",
    "        \n",
    "        h3, x3 = self.egnn(h2, x2, edge_index, edge_attr=None)\n",
    "        h3 = F.dropout(h3, p=self.dropout_p, training=self.training)\n",
    "\n",
    "        # Global pooling (sum/mean) over the nodes for graph classification\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled embeddings\n",
    "        h = torch.cat([h1, h2, h3], dim=1)\n",
    "         '''\n",
    "        h1, x1 = self.egnn(h, x, edge_index, edge_attr=None)\n",
    "        h = global_add_pool(h1, batch)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        h = self.lin1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_p, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h  # Return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56708626-1abc-475a-894c-a6255cdb7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_egnn_pca = EGNN_Graph_Classifier(input_dim_pca, hidden_dim, output_dim, dim_edge_attr=0, n_layers=2, use_graph_feat=False, dropout_p=0.5).to(device) \n",
    "trained_egnn_pca.load_state_dict(torch.load('../res/trained_models/trained_egnn_pca.pth'))\n",
    "trained_egnn_pca.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204cffa-808e-4d1d-a7c3-ee55f0e75590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "explainer = Explainer(\n",
    "            model=trained_egnn_pca,\n",
    "            algorithm=GNNExplainer(epochs=250),\n",
    "            explanation_type='model',\n",
    "            node_mask_type='object',\n",
    "            edge_mask_type=None,\n",
    "            model_config=dict(\n",
    "                mode='multiclass_classification',\n",
    "                task_level='graph',\n",
    "                return_type='raw',\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e838648-3259-4b31-9949-ed8950e347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "explanation = explainer(x=test_list_norm_pca[idx].coords.float(), edge_index=test_list_norm_pca[idx].edge_index, batch=test_list_norm_pca[idx].batch, h=test_list_norm_pca[idx].x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f9c7b-d32c-4701-8528-569efab2ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.node_mask[1:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
